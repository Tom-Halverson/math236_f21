[["index.html", "MATH 236: Linear Algebra Preface", " MATH 236: Linear Algebra Preface This is the class handbook for Math 236 Linear Algebra at Macalester College. The content here was made by Andrew Beveridge and Tom Halverson and other faculty in the Department of Mathematics, Statistics and Computer Science at Macalester College. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["problem-set-1.html", "Section 1 Problem Set 1 1.1 Characterize the Solution Set 1.2 Find the General Solution 1.3 Elementary row operations are reversible 1.4 Designer Parabolas 1.5 Traffic Flow", " Section 1 Problem Set 1 Due: Friday September 10 by midnight CST. These will typically be due on Wednesday but this one is moved to Friday because we do not have class on Monday due to Labor Day. Upload your solutions to problems 1–4 by writing them out by hand, scanning them to pdf using a scanning software such as AdobeScan, assembling them into a single PDF, and uploading it to Moodle. Problem 1.5 is to be done using RStudio. To solve it, create an Rmarkdown file, knit it to .html, and upload the .html on Moodle along with the PDF for questions 1-4. There will be time to work on this problem in class on May 23-24. 1.1 Characterize the Solution Set The following augmented matrices are in row echelon form. Decide whether the set of solutions is a point, line, plane, or the empty set in 3-space. Briefly justify your answer. \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 1 &amp; -3 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 1 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; -1 &amp; 0 &amp; -2 \\\\ 0 &amp; 1 &amp; 1 &amp; 7\\\\ 0 &amp; 0 &amp; 1 &amp; 1\\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; -1 &amp; 0 &amp; 6 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.2 Find the General Solution Each of the following matrices is the reduced row echelon form of the augmented matrix of a system of linear equations. Give the general solution to each system. \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 3 &amp; 0 &amp; -2 &amp; 5\\\\ 0 &amp; 0 &amp; 1 &amp; 4 &amp; -2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; 4 &amp; 0 &amp; 3 &amp; 6\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -2&amp; -8 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 4 &amp; 0 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7 &amp; 6\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.3 Elementary row operations are reversible In each case below, an elementary row operation turns the matrix \\(A\\) into the matrix \\(B\\). For each of them, Describe the row operation that turns \\(A\\) into \\(B\\), and Describe the row operation that turns \\(B\\) into \\(A\\). Give your answers in the form: “scale \\(R_2\\) by 3” or “swap \\(R_1\\) and \\(R_4\\)” or “replace \\(R_3\\) with \\(R_3 + \\frac{1}{5} R_1\\).” \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 0 &amp; 7 &amp; 0 &amp; -4 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 4 &amp; 1 &amp; -2 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] 1.4 Designer Parabolas In each part below, set up and solve a linear system of equations to find all possible parabolas of the form \\[ f(x) = a + b x + c x^2 \\] that satisfy the given conditions. For full credit, please solve these by hand, doing all row reductions that bring the system of equations to Reduced Row Echelon Form. On future assignments, you can solve problems like this using either RStudio or WolframAlpha. You are welcome (and, in fact, encouraged) to check your answers using software. \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(2,4)\\). \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(3,10)\\). \\(f(x)\\) passes through the two points: \\((1,3)\\) and \\((3,11)\\). 1.5 Traffic Flow Below you find a section of one-way streets in downtown St Paul, where the arrows indicate traffic direction. The traffic control center has installed electronic sensors that count the numbers of vehicles passing through the 6 streets that lead into and out of this area. Assume that the total flow that enters each intersection equals the the total flow that leaves each intersection (we will ignore parking and staying). Create a system of linear equations to find the possible flow values for the inner streets \\(x_1, x_2, x_3, x_4\\). Using RStudio, enter the augmented matrix of this system, and solve it using the rref command. Type out the general solution to this system of equations. There will be time to do this in class on Wednesday. Your answer to part b should be an infinite solution set. Give two distinct solutions that are realistic in terms of traffic flow. Is it possible to close down the street labeled by \\(x_2\\) for road construction? That is, is it possible to have \\(x_2 = 0\\) and to meet the other conditions? "],["problem-set-2.html", "Section 2 Problem Set 2 2.1 Vector Equation 2.2 Matrix Equation 2.3 RREF for a linear system 2.4 A square matrix 2.5 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) 2.6 A Balanced Diet", " Section 2 Problem Set 2 Due: Friday September 17 by 11:59PM CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. In problems where you use RStudio for row reduction and are not asked to turn in an R markdown file, you can write something like this: ## Loading required package: pracma The Problem Set covers sections 1.3 (vector equations), 1.4 (matrix equations), and 1.5 (parametric solutions and the relationship between homogeneous and nonhomogeneous equations). 2.1 Vector Equation Decide if the vectors \\(\\mathsf{b}\\) and \\(\\mathsf{d}\\) are in the span of the vectors \\(\\mathsf{v}_1\\), \\(\\mathsf{v}_2\\), \\(\\mathsf{v}_3\\), \\(\\mathsf{v}_4\\). If the vector is in the span, then give a linear combination of the vectors that does it. If it is not in the span, demonstrate that it is not with an appropriate matrix computation. \\[ \\mathsf{v}_1 = \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\\\ 1 \\end{bmatrix},\\quad \\mathsf{v}_2 = \\begin{bmatrix} -1 \\\\ -1 \\\\ -1 \\\\ 3 \\end{bmatrix},\\quad \\mathsf{v}_3 = \\begin{bmatrix} -4 \\\\ -4 \\\\ -3 \\\\ 5 \\end{bmatrix},\\quad \\mathsf{v}_4 = \\begin{bmatrix} -1 \\\\ 2 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] \\[ \\mathsf{b} = \\begin{bmatrix} 36 \\\\ 48 \\\\ 19 \\\\ -25 \\end{bmatrix},\\qquad \\mathsf{d} = \\begin{bmatrix} 30 \\\\ 24 \\\\ 10 \\\\ 10 \\end{bmatrix}. \\] Here are the vectors for you: v1 = c(2,2,1,1) v2 = c(-1,-1,-1,3) v3 = c(-4,-4,-3,5) v4 = c(-1,2,-2,1) b = c(36,48,19,-25) d = c(30,24,10,10) 2.2 Matrix Equation Solve the matrix equation, A x = b, below and give your answer in parametric vector form (we will disccuss parametric form on Monday Sept 13). Describe the solution set geometrically: e.g., “a line in \\(\\mathbb{R}^4\\)” or “a plane in \\(\\mathbb{R}^6\\).” \\[ \\begin{bmatrix} 1 &amp; 1 &amp; -1 &amp; -1 &amp; 2 \\\\ 1 &amp; 0 &amp; -2 &amp; 1 &amp; 1 \\\\ -2 &amp; 1 &amp; 5 &amp; 1 &amp; -6 \\\\ -3 &amp; 0 &amp; 6 &amp; 2 &amp; -8 \\\\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; -3 \\\\ 1 &amp; 0 &amp; -2 &amp; -1 &amp; 3 \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 2\\\\ 1 \\\\ 6 \\\\ -1 \\end{bmatrix} \\] Here is the matrix A and the vector b for you: A = cbind(c(1,1,-2,-3,0,1),c(1,0,1,0,1,0),c(-1,-2,5,6,1,-2),c(-1,1,1,2,2,-1),c(2,1,-6,-8,-3,3)) b = c(1,3,2,1,6,-1) 2.3 RREF for a linear system Here is the reduced row echelon form of a matrix \\(\\mathsf{A}\\) (you are not given the matrix \\(\\mathsf{A}\\)). \\[ \\mathsf{A} \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 5 &amp; -2\\\\ 0 &amp; 1 &amp; 0 &amp; -7 &amp; 4\\\\ 0 &amp; 0 &amp; 1 &amp; 2 &amp; -3\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{array} \\right] \\] Give the solution to the homogeneous matrix equation \\(A x = \\mathbf{0}\\) in parametric vector form and describe the geometry of the solution. For example, you answer should be something like: “it is a plane in \\(\\mathbb{R}^3\\)” or “it is a line in \\(\\mathbb{R}^7\\)” or “it is a point in \\(\\mathbb{R}^4\\).” Suppose that we also know that \\(\\mathsf{A}\\begin{bmatrix} 4 \\\\ 1 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 1 \\\\ 2 \\\\3 \\end{bmatrix}\\). Then give the general solution to \\(\\mathsf{A} \\mathsf{x}= \\begin{bmatrix} 5 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\) in parametric form. 2.4 A square matrix Suppose that \\(A\\) is a \\(5\\times 5\\) matrix and \\(\\mathsf{b}\\) is a vector in \\(\\mathbb{R}^5\\) with the property that \\(A\\mathsf{x}=\\mathsf{b}\\) has a unique solution. Explain why the columns of \\(A\\) must span \\(\\mathbb{R}^5\\). Use the reduced row echelon form of \\(A\\) in your explanation. 2.5 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) Suppose that \\(\\mathsf{x}_1\\) and \\(\\mathsf{x}_2\\) are solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) (where \\(\\mathsf{b} \\not= \\mathsf{0}\\)). Decide if any of the following are also solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\). Justify your answers. \\(\\mathsf{x}_1+ \\mathsf{x}_2\\) \\(\\mathsf{x}_1 - \\mathsf{x}_2\\) \\(\\frac{1}{2} ( \\mathsf{x}_1 + \\mathsf{x}_2)\\) \\(\\frac{5}{2} \\mathsf{x}_1 - \\frac{3}{2} \\mathsf{x}_2\\). Under what conditions on \\(c\\) and \\(d\\) is the linear combination \\(\\mathsf{x} = c \\mathsf{x}_1 + d \\mathsf{x}_2\\) a solution to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\)? Justify your answer. Let \\(\\mathsf{u}\\) be the vector that points to \\(1/3\\) of the way from the tip of \\(\\mathsf{v}\\) to the tip of \\(\\mathsf{w}\\) as depicted below. Write \\(\\mathsf{u}\\) as a linear combination of \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) (hint: think about \\(\\mathsf{w} - \\mathsf{v}\\)) If \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) are solutions to \\(A x = \\mathsf{b}\\) then show that \\(\\mathsf{u}\\) is also a solution to \\(A \\mathsf{x} = \\mathsf{b}\\). 2.6 A Balanced Diet An athlete wants to consume a daily diet of 200 grams of carbohydrates, 60 grams of fats and 160 grams of proteins. Here are some of their favorite foods. Table 2.1: Food Carb/Fat/Protein (grams) food carbs fats proteins almonds 3 8 5 avocado 15 31 4 beans 20 1 8 bread 12 1 2 cheese 1 5 3 chicken 0 13 50 egg 1 5 6 milk 12 8 8 zucchini 6 0 2 Explain why they cannot achieve their daily goal by eating only almonds, milk and zucchini.Use the word span in your answer. Explain why they cannot achieve their daily goal by eating only almonds, beans and cheese. Find a valid one-day diet consisting of almonds, chicken, and zucchini. "],["problem-set-3.html", "Section 3 Problem Set 3 3.1 RREF for a set of vectors 3.2 Linear independence and unique expressions. 3.3 Is the transformation linear? 3.4 Partial Information about a Linear Transformation 3.5 Square, Wide, and Tall Matrices", " Section 3 Problem Set 3 Due: Wednesday September 22 by midnight. Free Extension: anything turned in by 5PM Friday will be considered on time. After 5PM Friday, late assignments are subject to a 15% late penalty. After I post the solution on Monday, there is a 50% late penalty. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 1.7 Linear Independence and 1.8 Linear Transformations. 3.1 RREF for a set of vectors Suppose that we have five vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4,\\mathsf{v}_5\\) in \\(\\mathbb{R}^4\\) and that the matrix \\(A\\) containing those vectors row reduces as follows \\[ A = \\left[ \\begin{array}{ccc} \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\\\ \\mathsf{v}_1 &amp; \\mathsf{v}_2 &amp; \\mathsf{v}_3 &amp;\\mathsf{v}_4 &amp;\\mathsf{v}_5 \\\\ \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\end{array} \\right] \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; -3 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 4 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}. \\] Do the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\) span \\(\\mathbb{R}^4\\)? Justify your answer. Give the solution, in parametric form, to the homogeneous system of equations \\(A x = 0\\) for this problem. Give a dependence relation among the vectors \\(\\mathsf{v_1}, \\mathsf{v_2},\\mathsf{v_3},\\mathsf{v_4}, \\mathsf{v_5}\\). Is the vector \\(\\mathsf{v}_3\\) in \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2)\\)? Justify your answer. Suppose that \\(\\mathsf{b} = 5 \\mathsf{v}_1 + \\mathsf{v}_2 - 3 \\mathsf{v}_3 +4 \\mathsf{v}_4 - \\mathsf{v}_5\\). Use what you have done above to write \\(\\mathsf{b}\\) as a different linear combination of \\(\\mathsf{v_1}, \\mathsf{v_2},\\mathsf{v_3},\\mathsf{v_4}, \\mathsf{v_5}\\) (i.e., with different weights). 3.2 Linear independence and unique expressions. It is an important fact that if a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\ldots, \\mathsf{v}_n\\) is linearly independent then any vector in the span of these vectors can be written as a unique linear combination of those vectors. This property has a fairly simple proof. Let’s suppose that n = 4 for simplicity and that a vector v can be written in two ways as a combination of those vectors: \\[ \\begin{array}{ccccccc} v &amp;=&amp; c_1 \\mathsf{v}_1 &amp;+&amp; c_2 \\mathsf{v}_2 &amp;+&amp; c_3 \\mathsf{v}_3 &amp;+&amp; c_4 \\mathsf{v}_4 \\\\ &amp;=&amp; d_1 \\mathsf{v}_1 &amp;+&amp; d_2 \\mathsf{v}_2 &amp;+&amp; d_3 \\mathsf{v}_3 &amp;+&amp; d_4 \\mathsf{v}_4 \\end{array} \\] Then use the definition of linear independence to prove that \\(c_1 = d_1\\), \\(c_2 = d_2\\), \\(c_3 = d_3\\), and \\(c_4 = d_4\\). 3.3 Is the transformation linear? There are three transformations below. If you believe that the transformation is linear, then show it is by showing that the three linear transformation rules apply for arbitrary vectors. If you believe that the transformation is not linear. Then show that one of the rules fails for specific vectors. \\[ T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right) = \\begin{bmatrix} x_1 + x_2 + x_3 -1 \\\\ x_1 - x_2 + x_3 + 1 \\end{bmatrix} \\] \\[ T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\right) = \\begin{bmatrix} 3 x_1 - 5 x_2 \\\\ 2 x_1 + x_2 \\\\ 2 x_1 + 3 x_2 \\end{bmatrix} \\] \\[ T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right) = \\begin{bmatrix} x_1 + x_2^2 + x_3 \\\\ 2 x_2 + x_1 x_3 \\\\ 2 x_1 + 3 x_2 + x_3 \\end{bmatrix} \\] 3.4 Partial Information about a Linear Transformation We are given that \\(T: \\mathbb{R}^4 \\rightarrow \\mathbb{R}^3\\) is a linear transformation such that: \\[ T\\left(\\begin{bmatrix} 3 \\\\ ~2~ \\\\ 1 \\\\ 2 \\end{bmatrix} \\right)=\\begin{bmatrix} ~2~ \\\\ 3 \\\\ 6 \\end{bmatrix} \\qquad\\hbox{and}\\qquad T\\left(\\begin{bmatrix}~~2 \\\\ -1 \\\\ 0 \\\\ -1 \\end{bmatrix} \\right)=\\begin{bmatrix} 2 \\\\ ~0~ \\\\ 1 \\end{bmatrix}. \\] Use this information to compute the value of \\(T\\) below? \\[T\\left(\\begin{bmatrix} 5 \\\\ 8 \\\\ ~3~ \\\\ 8 \\end{bmatrix} \\right) = \\hskip5in\\] Hint: express the third input vector as a linear combination of the first two. 3.5 Square, Wide, and Tall Matrices In each question below, choose all of the answers that apply, and give a succinct explanation of how you know that it is true. [Wide] You are given a wide 5 x 8 matrix and you are studying the matrix equation \\(A x = b\\). These problems are said to be under constrained because there are fewer equations (constraints) than variables. \\[ A=\\begin{bmatrix} \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\end{bmatrix} \\] The vector that you are trying to get to is \\(b \\in \\mathbb{R}^m\\) and the solution vectors are \\(x \\in \\mathbb{R}^n\\). What are \\(m\\) and \\(n\\)? Decide which of the following statements are true and give a short and succinct justification of your answer using complete sentences and good punctuation. The columns of \\(A\\) must span \\(\\mathbb{R}^m\\). The columns of \\(A\\) must be linearly dependent. There is a unique solution to \\(A x = 0\\). If the columns span \\(\\mathbb{R}^m\\) then there is a unique solution to \\(A x = b\\) for all \\(b\\). [Tall] You are given a wide 7 x 4 matrix and you are studying the matrix equation \\(A x = b\\). These problems are said to be over constrained because there are more equations (constraints) than variables. \\[ A=\\begin{bmatrix} \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot \\ \\\\ \\end{bmatrix} \\] The vector that you are trying to get to is \\(b \\in \\mathbb{R}^m\\) and the solution vectors are \\(x \\in \\mathbb{R}^n\\). What are \\(m\\) and \\(n\\)? Decide which of the following statements are true and give a short and succinct justification of your answer using complete sentences and good punctuation. The columns of \\(A\\) might span \\(\\mathbb{R}^m\\). The columns of \\(A\\) must be linearly dependent. There is a unique solution to \\(A x = 0\\). [Square] You are given a square 6 x 6 matrix and you are studying the matrix equation \\(A x = b\\). \\[ A=\\begin{bmatrix} \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\ \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot &amp; \\cdot\\ \\\\ \\end{bmatrix} \\] The vector that you are trying to get to is \\(b \\in \\mathbb{R}^m\\) and the solution vectors are \\(x \\in \\mathbb{R}^n\\). What are \\(m\\) and \\(n\\)? Decide which of the following statements are true and give a short and succinct justification of your answer using complete sentences and good punctuation. The columns of \\(A\\) must span \\(\\mathbb{R}^m\\). If the columns span \\(\\mathbb{R}^m\\) then there is a unique solution to \\(A x = b\\) for all \\(b\\). If \\({\\bf 0}\\) is the only solution to \\(A x = {\\bf 0}\\), then there is a unique solution to \\(A x = b\\) for all \\(b \\in \\mathbb{R}^m\\). The columns of \\(A\\) span \\(\\mathbb{R}^m\\) if and only if the columns of \\(A\\) are linearly independent. "],["problem-set-4.html", "Section 4 Problem Set 4 4.1 Matrix of a Linear Transformation 4.2 Matrix of a Nonlinear Transformation? 4.3 Inner and Outer Products 4.4 Archaeological Seriation 4.5 Rental Car 4.6 Rainy Day in LA", " Section 4 Problem Set 4 Due: Wednesday September 29 by midnight CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 1.9. The Matrix of a Linear Transformation and One-to-One and Onto, 2.1. Matrix Multiplication, and 2.2 Matrix Inverses. 4.1 Matrix of a Linear Transformation In each example below, find the matrix of the given linear transformation (all of these transformations are linear). Do this by finding where the transformation sends the standard basis vectors \\(\\mathsf{e}_1,\\mathsf{e}_2,\\mathsf{e}_3, \\ldots\\). The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\right) = \\begin{bmatrix} x_3 \\\\ x_1 \\\\ x_2 \\\\\\end{bmatrix}. \\] The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)= \\begin{bmatrix} x_1 \\\\ x_1 + x_2 \\\\ x_2 + x_3 \\end{bmatrix} + \\begin{bmatrix} x_3 \\\\ x_3 \\\\ x_1 \\\\\\end{bmatrix} \\] The transformation \\(L: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) The transformation \\(R: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) The transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends my house on the left to the house on the right. \\(\\qquad \\qquad\\) 4.2 Matrix of a Nonlinear Transformation? This problem illustrates what happens if you try to make the matrix of a transformation that is not linear. Consider the transformation \\(T\\) defined below. We can see that it is nonlinear by the fact that one of the coordinates is squared and by the fact that it does not send 0 to 0. \\[ T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right) = \\begin{bmatrix} x_1 + x_2^2 + x_3 \\\\ 2 x_2 + x_1 x_3 + 1 \\\\ 2 x_1 + 3 x_2 + x_3 \\end{bmatrix} \\] Now, let’s see what happens if we compute a matrix for it anyway. Compute \\(T(\\mathbf{e}_1)\\), \\(T(\\mathbf{e}_2)\\), and \\(T(\\mathbf{e}_3)\\), and put the vectors you get in the columns of a matrix \\(A\\). Then compute the product below: \\[ \\underbrace{\\begin{bmatrix} \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\end{bmatrix}}_{A} \\begin{bmatrix} x_1 \\\\ x_ 2 \\\\ x_3 \\end{bmatrix} = \\] Explain how the result of this computation demonstrates that \\(T\\) is not linear. 4.3 Inner and Outer Products I hope to give you some time to discuss this in class on Friday Sep 24. We can also think of vectors as matrix. A column vector is an \\(n \\times 1\\) matrix and a row vector is a \\(1 \\times n\\) matrix. Compute the following products. These matrix products are called inner products (or dot products) of the vectors. \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} = \\qquad\\qquad \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\1 \\\\1 \\\\\\end{bmatrix} = \\qquad\\qquad \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 2 \\\\ 5 \\\\ 0 \\\\ -1 \\\\\\end{bmatrix} = \\qquad\\qquad \\] b. Now compute the following products. These are called outer products. \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} \\begin{bmatrix} 1 &amp; -5 &amp; 2 &amp; 3\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1\\end{bmatrix} =\\hskip3in \\] Row reduce both of the matrices that you get in part b (this should be easy to do by hand,but you can use R if you want to). How many pivots do you get? Explain why you always get this number of pivots when you row reduce an outer product. 4.4 Archaeological Seriation There may be time to discuss this in class on Friday Sep 24. The matrix \\(A\\) below is used in archaeological dating. Its rows correspond to four different grave sites \\(G_1, G_2, G_3, G_4\\) and its columns correspond to five types of pottery\\(P_1, P_2, P_3, P_4, P_5\\). There is a 1 in position \\(i\\)-\\(j\\) if pottery type \\(P_j\\) is found in grave \\(G_i\\) (and a 0 otherwise). \\[ A=\\begin{array}{c|ccccc} &amp; P_1 &amp; P_2 &amp; P_3 &amp; P_4 &amp; P_5 \\\\ \\hline G_1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ G_2 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ G_3 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ G_4 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{array} \\] Compute the matrix \\(\\mathbf{G} = A A^T\\), where \\(A^T\\) is the transpose of \\(A\\), meaning that the rows and columns have been interchanged. Give the meaning of the \\(i\\)-\\(j\\) entry of \\(\\mathbf{G}\\) (the entry in row \\(i\\) and column \\(j\\)). State clearly the meaning of this entry using complete sentences (or sentence) and explain why it has this meaning. 4.5 Rental Car I hope to give you some time to work on this in class on Friday Sep 24. Solve this problem using R and turn in a markdown file knitted to html. A group of Macalester alumni open a rental car company specializing in renting electric cars. As a start, they have opened offices in St. Paul, Rochester, and Duluth. Through market research they find that of the cars rented in St. Paul, 85% will get returned in St. Paul, 9% will get returned in Rochester, and 6% will get returned in Duluth. Of the cars rented in Rochester, 30% will get returned in St. Paul, 60% will get returned in Rochester, and 10% in Duluth. Of the cars rented in Duluth, 35% will get returned in St. Paul, 5% in Rochester, and 60% in Duluth. This information is represented in the matrix below. StP = c(.85,.09,.06) Roch = c(.30,.60,.10) Dul = c(.35,.05,.60) M = cbind(StP,Roch,Dul) M ## StP Roch Dul ## [1,] 0.85 0.3 0.35 ## [2,] 0.09 0.6 0.05 ## [3,] 0.06 0.1 0.60 Such a matrix is called a probability matrix or a stochastic matrix because it contains numbers between 0 and 1 and each of its columns sum to 1. The owners are trying to use this data to estimate how much of their fleet will be at each location on average in the long run. Assume that initially they locate 20 cars in each city. This can be recorded by the vector v0 = c(20,20,20). Apply, M to v0, call this vector v1, and explain, using how the matrix-vector product works, why v1 represents the number of cars at each location one day later (for simplicity, we assume that each rental is for 1 day). Now apply M to v1 and call it v2. This should represent the number of cars at each location 2 days later. Also compute the square of the matrix M and call it M2. Confirm that M2 times v0 is the same as M times v1. Write a for loop that applies M over and over again to see what happens to the distribution of cars in the long-run. Does this sequence stabilize or does it keep changing after each application? If it does stabilize, how long does it take to stabilize (to within 0.1 cars at each location). See the MatrixMultiplication.Rmd activity we did in class as well as Writing Loops. Does the starting distribution matter? Try 4 different starting distributions (with a total of 60 cars) and see what the final distribution looks like in each case. For one of your 4 starting distributions, try all 60 cars at one of the locations. 4.6 Rainy Day in LA I hope to give you some time to discuss this in class on Monday Sep 24. In Los Angeles if it rains today, there is a 50% chance it will rain tomorrow, but it if is sunny today, there is a 90% chance it will be sunny tomorrow. This is modeled in the rain-sunshine probability matrix P. \\[ P = \\begin{array}{c|cc|} &amp;\\text{rain}&amp;\\text{sun}\\\\ \\hline \\text{rain}&amp;1/2&amp;1/10\\\\ \\text{sun}&amp;1/2&amp;9/10\\\\ \\hline \\end{array} \\] This matrix works as follows: if the rain-sunshine probability today is (40, 60) (that is, 40% chance rain and 60% chance sunshine), then the rain-sunshine probability tomorrow is (26, 74) as seen by the calculation below. \\[ \\begin{bmatrix} 1/2 &amp; 1/10 \\\\ 1/2 &amp; 9/10 \\\\ \\end{bmatrix} \\begin{bmatrix} 40 \\\\ 60 \\end{bmatrix} = \\begin{bmatrix} 26 \\\\ 74 \\end{bmatrix} \\] Find the rain-sunshine probability the day after tomorrow. Compute \\(P^2\\) and explain the meaning of each of the four entries in the matrix. Find \\(P^{-1}\\) and and use it find the rain-sunshine probability yesterday if the rain-sunshine probability today is (40, 60). "],["problem-set-5.html", "Section 5 Problem Set 5 5.1 Column and Null Space 5.2 Vectors Rescaled 5.3 Fibonacci Vectors 5.4 A Vector in Both Col(A) and Nul(A) 5.5 Getting Into a Subspace 5.6 Shrink to a basis 5.7 Extend to a basis", " Section 5 Problem Set 5 Due: Wednesday October 13 by 11:59pm CST. You will get a chance to discuss some of these in class with your classmates. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. 5.1 Column and Null Space Find a basis for the column space \\(Col(A)\\) and the null space \\(Nul(A)\\) of the matrix \\(A\\) below (A = rbind(c(1, 2, 0, 2, 0, -1),c(1, 2, 1, 1, 0, -2), c(2, 4, -2, 6, 1, 2),c(1, 2, 0, 2, -1, -3 ))) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 0 2 0 -1 ## [2,] 1 2 1 1 0 -2 ## [3,] 2 4 -2 6 1 2 ## [4,] 1 2 0 2 -1 -3 5.2 Vectors Rescaled If the function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^n\\) is a linear transformation, then show that the set below is a subspace of \\(\\mathbb{R}^n\\) \\[ E_{2021} = \\{\\ \\vec{x} \\in \\mathbb{R}^n \\mid T(\\vec{x}) = 2021 \\vec{x} \\}. \\] Is there anything special about 2021 in the definition? If it were replaced by another scalar, would it still be a subspace? 5.3 Fibonacci Vectors The Fibonacci vectors \\(F\\) in \\(\\mathbb{R}^5\\) are defined below: \\[ F = \\left\\{ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} ~\\Bigg\\vert~ \\ x_3 = x_1 + x_2, x_4 = x_2 + x_3, x_5 = x_3 + x_4 \\right\\} \\subseteq \\mathbb{R}^5. \\hskip5in \\] Show that \\(F\\) is a subspace by showing that it is closed under addition and scalar multiplication Write \\(F\\) as the span of a set of vectors (hint: “bake” the subspace conditions into the vectors as we do in class for the zero-sum subspace). Find a basis for \\(F\\) from among your spanning vectors in the previous part. The previous part gives you spanning. Now you need to argue that the vectors are linearly independent. The dimension of a vector space is the number of vectors in a basis for the vector space. What is the dimension of \\(F\\)? 5.4 A Vector in Both Col(A) and Nul(A) Give a \\(3 \\times 3\\) matrix \\(A\\) for which the vector \\(\\mathsf{v} = \\begin{bmatrix}3 \\\\ -2 \\\\ 5 \\end{bmatrix}\\) is in both \\(\\mathrm{Col}(A)\\) and \\(\\mathrm{Nul}(A)\\). Be sure to demonstrate that \\(\\mathsf{v} \\in \\mathrm{Col}(A)\\) and \\(\\mathsf{v} \\in \\mathrm{Nul}(A)\\). 5.5 Getting Into a Subspace Let \\(S \\subseteq \\mathbb{R}^n\\) be a subspace and let \\(\\mathsf{v}, \\mathsf{w} \\in \\mathbb{R}^n\\). For each of the following statements, either give a specific example or explain why it cannot happen. If \\(\\mathsf{v}\\) is not in \\(S\\) and \\(c\\) is a nonzero constant, can \\(c\\mathsf{v}\\) be in \\(S\\)? If \\(\\mathsf{v}\\) is not in \\(S\\) and \\(\\mathsf{w}\\) is not in \\(S\\), can \\(\\mathsf{v} + \\mathsf{w}\\) be in \\(S\\)? If \\(\\mathsf{v}\\) is in \\(S\\) and \\(\\mathsf{w}\\) is not in \\(S\\) can \\(\\mathsf{v} + \\mathsf{w}\\) be in \\(S\\)? 5.6 Shrink to a basis Let \\(S\\) be the subspace of \\(\\mathbb{R}^5\\) spanned by the vectors below. Find a basis for \\(S\\) and find the dimension of \\(S\\) (the number of vectors in a basis). \\[ \\mathsf{v}_1 = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 1\\\\ 3 \\end{bmatrix}, \\mathsf{v}_2 = \\begin{bmatrix} 1\\\\ -1\\\\ 5\\\\ 1\\\\ 1 \\end{bmatrix}, \\mathsf{v}_3 = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 0\\\\ 2 \\end{bmatrix}, \\mathsf{v}_4 = \\begin{bmatrix} 2\\\\ -1\\\\ 8\\\\ 1\\\\ 2 \\end{bmatrix}, \\mathsf{v}_5 = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 1\\\\ 3 \\end{bmatrix}. \\] (A = cbind(c(1,1,1,1,3),c(1,-1,5,1,1),c(1,1,1,0,2),c(2,-1,8,1,2),c(1,1,1,1,3))) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 2 1 ## [2,] 1 -1 1 -1 1 ## [3,] 1 5 1 8 1 ## [4,] 1 1 0 1 1 ## [5,] 3 1 2 2 3 5.7 Extend to a basis I am interested in the vectors below. I know that they do not span \\(\\mathbb{R}^5\\), because there are not enough of them, but I want to extend this set to a basis of \\(\\mathbb{R}^5\\) by adding some vectors to the set. \\[ \\begin{bmatrix} 5\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 4\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 1\\\\ 1\\end{bmatrix}. \\] I searched online for ideas and one suggested that I make the matrix below and row reduce it. (A = cbind(c(5,4,3,1,2),c(4,4,3,1,2),c(1,1,1,1,1),diag(5))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 5 4 1 1 0 0 0 0 ## [2,] 4 4 1 0 1 0 0 0 ## [3,] 3 3 1 0 0 1 0 0 ## [4,] 1 1 1 0 0 0 1 0 ## [5,] 2 2 1 0 0 0 0 1 Row reduce this matrix. Use the result to come up with a basis for \\(\\mathbb{R}^5\\) that includes my original 3 vectors Explain why this method works. "],["problem-set-6.html", "Section 6 Problem Set 6 6.1 Dimension 6.2 A Tetrahedral Basis 6.3 Matrix Rank 6.4 A Tale of Two Bases", " Section 6 Problem Set 6 Due: Wednesday October 27 by 11:59pm CST. This is the Wednesday after Fall break. However, these problems cover topics that are on Exam 2. I will have you work on and discuss them before the exam. I hope that you can get it all done before break, but to take the pressure off writing it up, I will not have you turn it in until after break. There will be time in class to discuss and work on these. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. 6.1 Dimension Find the dimension of the subspace \\(Z\\) of \\(\\mathbb{R}^5\\) of vectors below. Either use the method of “baking in” the conditions or turn it in to a null space and use our methods of finding a null space basis. \\[ Z = \\left\\{ \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} \\ \\Bigg\\vert\\ \\ x_1 + x_2 + x_3 + x_4 + x_5 = 0, x_4 = 2 x_2 \\ \\right\\}. \\] 6.2 A Tetrahedral Basis In practice, we change bases because problems are computationally easier in another coordinate system or because we learn something by looking at a problem from the point of view of a different coordinate system. The following example illustrates this with ideas that arises both in chemistry and computer graphics. Below is the tetrahedral molecule methane, \\(\\mathsf{CH}_4\\). Its coordinates can be described in 3-dimensional space by the vectors below. \\[ \\mathsf{C}=\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{H}_1=\\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{3}{2\\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_2=\\begin{bmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ -\\frac{1}{2} \\\\ -\\frac{1}{2 \\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_3=\\begin{bmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{2} \\\\ -\\frac{1}{2 \\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_4=\\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\0 \\\\-\\frac{1}{2 \\sqrt{6}} \\end{bmatrix} \\] Find a dependence relation among the vectors \\(\\mathsf{H}_1, \\mathsf{H}_2, \\mathsf{H_3}, \\mathsf{H_4}.\\) Hint: add them together (you can do it “by hand” by just looking at the sum). We can see visually that the set \\(\\mathcal{M} = \\{ \\mathsf{H}_1, \\mathsf{H}_2, \\mathsf{H_3} \\}\\) is a basis of \\(\\mathbb{R}^3\\), which we will call the tetrahedral basis. You can see from the plot that these vectors are linearly independent (not all on the same plane). Give the coordinates of each of the vectors \\(\\mathsf{H}_1, \\mathsf{H}_2,\\mathsf{H}_3,\\mathsf{H}_4\\) in the \\(\\mathcal{M}\\) basis (for \\(\\mathsf{H}_4\\) you will need to use part a). In chemistry and physics, we are interested in symmetry operations. These are linear transformations such that the atom looks the same after the transformation as it did before. For example one such operation is rotation \\(r\\) by 120\\(^o\\) around the \\(\\mathsf{H}_4\\) axis. This rotation sends \\(\\mathsf{H}_1\\) to \\(\\mathsf{H}_3\\), \\(\\mathsf{H}_3\\) to \\(\\mathsf{H}_2\\), and \\(\\mathsf{H}_2\\) to \\(\\mathsf{H}_1\\). Give the matrix of \\(r\\) in the \\(\\mathcal{M}\\) basis. The columns should be the result of applying the symmetry operation to each of the basis vectors and then expressing the answer in the \\(\\mathcal{M}\\) basis. Show, by multiplying by hand, that your matrix \\(r\\) sends \\(\\mathsf{H}_1\\) to \\(\\mathsf{H}_3\\), \\(\\mathsf{H}_3\\) to \\(\\mathsf{H}_2\\), \\(\\mathsf{H}_2\\) to \\(\\mathsf{H}_1\\), and \\(\\mathsf{H}_4\\) to \\(\\mathsf{H}_4\\). Another symmetry operation is a rotation \\(s\\) by \\(180^o\\) around the axis that passes through the midpoing between \\(\\mathsf{H}_1\\) and \\(\\mathsf{H}_2\\) and the midpoint between \\(\\mathsf{H}_3\\) and \\(\\mathsf{H}_4\\). This rotation exchanges \\(\\mathsf{H}_1\\) and \\(\\mathsf{H}_2\\) and exchanges \\(\\mathsf{H}_3\\) and \\(\\mathsf{H}_4\\). Find the matrix of \\(s\\) in the \\(\\mathcal{M}\\) basis. By hand, apply it to each of the four hydrogen atoms and show that they go to the right place. Now we will convert our matrix for \\(r\\) to standard coordinates. Here is the recipe. First, enter the change of basis give the change of basis matrix \\(T\\) that converts from the tetrahedral basis \\(\\mathcal{M}\\) to the standard basis \\(\\mathcal{S}\\) and compute its inverse that converts from the standard basis back to \\(\\mathcal{M}\\). Here are the atoms for you. H1 = c(0,0,3/(2*sqrt(6))) H2 = c(-1/(2*sqrt(3)),-1/2,-1/(2*sqrt(6))) H3 = c(-1/(2*sqrt(3)),1/2,-1/(2*sqrt(6))) H4 = c(1/sqrt(3),0,-1/(2*sqrt(6))) Now, enter the matrix of the rotation \\(r\\) from part c above. It is entered as the all 0s matrix right now. You need to enter your correct entries. r.M = cbind(c(0,0,0),c(0,0,0),c(0,0,0)) rownames(r.M) &lt;- c(&quot;H1&quot;,&quot;H2&quot;,&quot;H3&quot;) colnames(r.M) &lt;- c(&quot;H1&quot;,&quot;H2&quot;,&quot;H3&quot;) r.M ## H1 H2 H3 ## H1 0 0 0 ## H2 0 0 0 ## H3 0 0 0 Compute the matrix of \\([r]_\\mathcal{S}\\) in the standard basis by computing the matrix product below. Notice that, working from right to left, it first converts from standard coordinates to \\(\\mathcal{M}\\) coordinates. Then it does the rotation in \\(\\mathcal{M}\\) coordinates. Then it converts the answer back to standard coordinates. Multiply this matrix (using R) by each of the hydrogen atoms (but now in standard coordinates) and see that they go to the right place. The point of this exercise is that the problem is much easier and nicer in the \\(\\mathcal{M}\\) basis, which is well-suited to the problem. 6.3 Matrix Rank In the table below, each column corresponds to a linear transformation \\(T_\\mathsf{M}: \\mathbb{R}^n \\to \\mathbb{R}^m\\) with a matrix \\(\\mathsf{M}\\). At the top of the column I tell you the values of \\(m\\) and \\(n\\) and the rank of the matrix. Fill in the entries of the table with T = true or F = false or I = not enough information to know. (Hint: draw a “picture” of the RREF of \\(\\mathsf{A}\\) in each case). .col2 { columns: 2 200px; /* number of columns and width in pixels*/ -webkit-columns: 2 200px; /* chrome, safari */ -moz-columns: 2 200px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; } (a) \\(\\mathsf{A}\\) is invertible (b) \\(\\mathsf{rref}(\\mathsf{A}) = I\\) (c) \\(\\mathsf{A}\\) has 8 pivots (d) \\(\\mathsf{A} \\mathbf{0} = \\mathbf{0}\\) (e) \\(\\mathsf{A} \\mathsf{x} = \\mathbf{0}\\) has more than one solution. (f) \\(T\\) is one-to-one (g) \\(T\\) is onto (h) \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has at least one solution for all \\(\\mathsf{b} \\in \\mathbb{R}^8\\). (i) The columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^8\\). (j) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has no solutions. (k) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has infinitely many solutions. (l) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has exactly 17 solutions. (m) There is a vector \\(b \\in \\mathbb{R}^8\\) that can be written as a linear combination of the columns of \\(\\mathsf{A}\\) in more than one way. (n) The rows of \\(\\mathsf{A}\\) span a 7 dimensional subspace of \\(\\mathbb{R}^8\\). (o) The columns of \\(\\mathsf{A}\\) are linearly independent. (p) The rows of \\(\\mathsf{A}\\) are linearly independent \\[ \\begin{array}{|c|c|c|c|c|} \\hline &amp; T_\\mathsf{A}: \\mathbb{R}^8 \\to \\mathbb{R}^8 &amp; T_\\mathsf{B}: \\mathbb{R}^8 \\to \\mathbb{R}^8 &amp; T_\\mathsf{C}: \\mathbb{R}^7 \\to \\mathbb{R}^8 &amp; T_\\mathsf{D}: \\mathbb{R}^9 \\to \\mathbb{R}^8 \\\\ &amp; \\text{$\\mathsf{A}$ has rank 7} &amp; \\text{$\\mathsf{B}$ has rank 8} &amp; \\text{$\\mathsf{C}$ has rank 7} &amp; \\text{$\\mathsf{D}$ has rank 8} \\\\ \\hline (a) &amp; &amp; &amp; &amp; \\\\ \\hline (b) &amp; &amp; &amp; &amp; \\\\ \\hline (c) &amp; &amp; &amp; &amp; \\\\ \\hline (d) &amp; &amp; &amp; &amp; \\\\ \\hline (e) &amp; &amp; &amp; &amp; \\\\ \\hline (f) &amp; &amp; &amp; &amp; \\\\ \\hline (g) &amp; &amp; &amp; &amp; \\\\ \\hline (h) &amp; &amp; &amp; &amp; \\\\ \\hline (i) &amp; &amp; &amp; &amp; \\\\ \\hline (j) &amp; &amp; &amp; &amp; \\\\ \\hline (k) &amp; &amp; &amp; &amp; \\\\ \\hline (l) &amp; &amp; &amp; &amp; \\\\ \\hline (m) &amp; &amp; &amp; &amp; \\\\ \\hline (n) &amp; &amp; &amp; &amp; \\\\ \\hline (o) &amp; &amp; &amp; &amp; \\\\ \\hline (p) &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} \\] 6.4 A Tale of Two Bases I recommend using R on this problem. Consider the subspace \\(S\\) of \\(\\mathbb{R}^5\\) below. \\[ S = \\textsf{span}\\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ -1 \\\\ \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\\\ 3 \\\\ 0 \\\\ -2 \\\\ \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 3 \\\\ \\end{bmatrix} \\right) \\] A = cbind(c(1,1,1,1,2),c(1, 2, 3, 0, -1),c(0, 0, 0, 1, 2), c(-1, 1, 3, 0, -2),c(2, 1, 0, 1, 3)) Give a basis of \\(S\\) consisting of some or all of the vectors used to define \\(S\\) above. Give a basis of \\(S\\) that has the nice standard basis property (i.e., the 0s and 1s property). For the two vectors below, decide if they are in \\(S\\). If the vector is in \\(S\\) then give its coordinates in each of your bases from parts (a) and (b). If you can do one of these “by hand” then explain how. \\[ \\mathbf{w} = \\begin{bmatrix} 8 \\\\ 11 \\\\ 14 \\\\ 7 \\\\ 11 \\end{bmatrix}, \\qquad \\mathbf{v} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\\\ 1 \\\\ 1 \\end{bmatrix}. \\] "],["problem-set-7.html", "Section 7 Problem Set 7 7.1 Rental Cars Revisited 7.2 Diagonalization 7.3 Same Eigenvectors 7.4 Rain and Sunshine Revisited 7.5 The Square Root of a Matrix?", " Section 7 Problem Set 7 Due: Wednesday November 3 by 11:59pm CST. This problem set covers the material from 5.1. Eigenvectors 5.2. Eigenvalues 5.3. Eigenbasis Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. 7.1 Rental Cars Revisited Below is the rental car matrix from PS4.5. StP = c(.85,.09,.06) Roch = c(.30,.60,.10) Dul = c(.35,.05,.60) (M = cbind(StP,Roch,Dul)) ## StP Roch Dul ## [1,] 0.85 0.3 0.35 ## [2,] 0.09 0.6 0.05 ## [3,] 0.06 0.1 0.60 Find its eigenvectors and eigenvalues. You should see that one of the eigenvalues is 1. This always happens when the columns sum to 1 (we will prove this later). An eigenvector of eigenvalue 1 is called a steady state vector because we have A v = v. There are also two complex eigenvalues and eigenvectors (we will talk about these next week). Extract the eigenvector corresponding to eigenvalue 1. You will see that it has a 0 imaginary part. You can get rid of the imaginary part of a vector v by using w = Re(v). This gives the real part of v. Demonstrate that A w = w. Rescale the eigenvector so that it sums to 1. By using sum(w) you can sum the components of the vector. When the vector sums to 1, you can think of the components as the proportion of cars at each rental company. Compare this with the steady state you found by looping when you worked on this in problem set 4. x = c(40,35,25) for (i in 1:100) {x = M %*% x} x 7.2 Diagonalization By hand, find the characteristic polynomial, the eigenvalues, and the eigenvectors of the matrix \\(A = \\begin{bmatrix} 0 &amp; 1 \\\\ 2 &amp; 1 \\end{bmatrix}\\). Then diagonalize the matrix \\(A\\) by writing it as a product \\(A = P D P^{-1}\\) where \\(D\\) is diagonal. The matrix \\(B\\) below has characteristic polynomial \\(f_B(\\lambda) = -\\lambda ^3+6 \\lambda ^2-9 \\lambda = -\\lambda(\\lambda-3)^2\\), \\[ B = \\begin{bmatrix} 2 &amp; -1 &amp; -1 \\\\ -1 &amp; 2 &amp; -1 \\\\ -1 &amp; -1 &amp; 2 \\\\ \\end{bmatrix}. \\] Use the following information to diagonalize \\(B\\). That is, write it as \\(B = P D P^{-1}\\),with \\(D\\) diagonal. You do not have to find \\(P^{-1}\\). \\[ \\begin{bmatrix} 2 &amp; -1 &amp; -1 \\\\ -1 &amp; 2 &amp; -1 \\\\ -1 &amp; -1 &amp; 2 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] \\[ \\begin{bmatrix} -1 &amp; -1 &amp; -1 \\\\ -1 &amp; -1 &amp; -1 \\\\ -1 &amp; -1 &amp; -1 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] Diagonalize the matrix \\(C\\) using R. You should find a matrix \\(P\\) and compute the diagonal matrix \\(D = P^{-1} C P\\). If you use the function zapsmall it will turn any really small numbers into 0. (C = cbind(c(44, 14, -13), c(10, 49, -2), c(-18, 0, 33))) ## [,1] [,2] [,3] ## [1,] 44 10 -18 ## [2,] 14 49 0 ## [3,] -13 -2 33 Show that the matrix below is not diagonalizable by showing that it does not have a basis of eigenvectors. Is it invertible? \\[ D = \\begin{bmatrix} 0 &amp; 1 \\\\ -1 &amp; 2 \\\\ \\end{bmatrix}. \\] 7.3 Same Eigenvectors Here are two matrices (A = cbind(c(-8, 3, 29), c(-40, 24, 46), c(10, 3, 11))) ## [,1] [,2] [,3] ## [1,] -8 -40 10 ## [2,] 3 24 3 ## [3,] 29 46 11 (B = cbind(c(4, 3, 35), c(-49, 42, 55), c(13, 3, 26))) ## [,1] [,2] [,3] ## [1,] 4 -49 13 ## [2,] 3 42 3 ## [3,] 35 55 26 Use R to show that they have the same eigenvectors but different eigenvalues. Show that \\(A B = B A\\) (even though we know that, in general, matrices do not commute). Now let \\(A\\) and \\(B\\) be any \\(n \\times n\\) matrices which have the same eigenvectors. Prove that \\(AB = BA\\). Hint: use the diagonalization of these two matrices. We will cover diagonalization on Friday and Monday. 7.4 Rain and Sunshine Revisited On PS4, we encountered the rain-sunshine matrix \\(A\\) below \\[ A = \\begin{bmatrix} 1/2 &amp; 1/10 \\\\ 1/2 &amp; 9/10 \\\\ \\end{bmatrix}. \\] Perform the following calculations by hand and show your work. Find the characteristic polynomial of \\(A\\) and find its eigenvalues. Remember that the characteristic polynomial is the polynomial \\(f_A(\\lambda) = \\det(A - \\lambda I)\\). Find an eigenvector for each eigenvalue and describe the eigenspaces. Diagonalize \\(A\\). Use your answer to (c) to give a formula for \\(A^n\\) and use this formula to compute \\(\\displaystyle{\\lim_{n\\to \\infty}} A^n\\). Write a loop in R that computes \\(A^{100}\\). Do this as follows: Start with B = A Loop 100 times: at each step in the loop, let B = A %*% B. Compare the answer that you get to your answer to part (d). 7.5 The Square Root of a Matrix? Do this problem by hand. The matrix \\(A =\\begin{bmatrix} 7 &amp; 2 \\\\ -4 &amp; 1 \\end{bmatrix}\\) has characteristic polynomial \\(c(\\lambda) = \\lambda^2 - 8 \\lambda + 15 = (\\lambda -3)(\\lambda - 5).\\) Describe the eigenspaces of \\(A\\). Diagonalize \\(A\\). We will learn how to do this on Friday. Find a matrix that makes sense to call \\(\\sqrt{A}\\). Then show that when you square this matrix, you really do get matrix \\(A\\). "],["problem-set-8.html", "Section 8 Problem Set 8 8.1 Matrix Reconstruction 8.2 Coyotes and Roadrunners 8.3 Hunt Creek 8.4 Halverson Numbers", " Section 8 Problem Set 8 You can download the Rmd source file for this problem set. 8.1 Matrix Reconstruction An unknown \\(3 \\times 3\\) matrix \\(M\\) has eigenvectors and corresponding eigenvalues: \\[ \\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\ \\lambda_1 = 1; \\qquad \\mathsf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix},\\ \\lambda_2 = \\frac{9}{10}; \\qquad \\mathsf{v}_3 = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix},\\ \\lambda_3 = 0. \\] Without using the matrix \\(M\\), compute \\(M^{10} \\mathsf{v}\\) where \\(\\mathsf{v} = \\begin{bmatrix}7\\\\3\\\\4\\end{bmatrix}\\). (That is, use only the eigen-information.) Describe all vectors \\(\\mathsf{v}\\), if there are any, such that \\(M^{n} \\mathsf{v} \\to {\\bf 0}\\) as \\(n \\to \\infty\\). Reconstruct \\(M\\) from the information given. 8.2 Coyotes and Roadrunners This summer, Macalester’s Ordway Natural History Study Area will be stocked with a population of coyotes and roadrunners so that Math 236 students can study real-life predator-prey dynamics. From similar experiments, we expect the predator-prey dynamics to be governed by linear model below. The eigenvalues of the matrix are also given. \\[ \\begin{bmatrix} \\phantom{\\Big\\vert} r_{t+1}\\phantom{\\Big\\vert} \\\\ \\phantom{\\Big\\vert} c_{t+1}\\phantom{\\Big\\vert} \\phantom{\\Big\\vert} \\end{bmatrix} =\\left[ \\begin{array}{cc} \\phantom{\\Big\\vert} \\frac{57}{50} &amp; -\\frac{6}{50} \\\\ \\phantom{\\Big\\vert} \\frac{4}{50} &amp; \\frac{43}{50} \\\\ \\end{array} \\right] \\begin{bmatrix} \\phantom{\\Big\\vert} r_t \\phantom{\\Big\\vert} \\\\ \\phantom{\\Big\\vert} c_t \\phantom{\\Big\\vert} \\end{bmatrix} = \\begin{bmatrix} \\phantom{\\Big\\vert} \\frac{57}{50} r_t - \\frac{6}{50} c_t \\\\ \\phantom{\\Big\\vert} \\frac{4}{50} r_t + \\frac{43}{50} c_t \\end{bmatrix}, \\] The eigenvalues and eigenvectors of this matrix are: \\[ \\begin{array}{lcl} \\lambda_1 = \\frac{11}{10} = 1.1, &amp; \\qquad &amp; \\lambda_2 = \\frac{9}{10} = 0.9 \\\\ \\mathsf{v}_1 = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} &amp;&amp; \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\end{array} \\] A = cbind(c(57/50,4/50),c(-6/50,43/50)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] 0.9486833 0.4472136 ## [2,] 0.3162278 0.8944272 If \\(r_0 = 10\\) roadrunners and \\(c_0 = 15\\) coyotes are introduced to the area, then give closed formulas for the population of coyotes \\(c_t\\) and roadrunners \\(r_t\\) after \\(t\\) years. In the long-term, in this model, what is the ratio of roadrunners to coyotes? When another college tried the same experiment in their Arboretum, they introduced \\(r_0 = 5\\) roadrunners and \\(c_0 = 10\\) coyotes and both populations died off (as is verified in the computation below). Explain why this happens using the eigenvalues and eigenvectors. A = cbind(c(57/50,4/50),c(-6/50,43/50)) v = c(5,10) for (i in 1:100) {v = A %*% v} v ## [,1] ## [1,] 0.0001328069 ## [2,] 0.0002656140 8.3 Hunt Creek Age-structured population models like we saw in the Spotted Owl Example are often called Leslie Matrices, named after the British ecologist P.H. Leslie. Here is the Leslie Matrix of a population of brook trout in Hunt Creek in Michigan. The population is categorized into 5 age categories: fingerlings (0,1), yearlings (1-2), young adults (2-3), adults (3-4), and adults (4-5). Right now the population is seen to be dying off. The vector \\(p_t\\) denotes the population at year \\(t\\) broken into the 5 age categories: \\[ p(t) = \\begin{bmatrix} f_t \\\\ y_t \\\\ ya_t \\\\ a1_t \\\\ a2_t \\end{bmatrix} \\] and the matrix \\(L\\) gives next year’s population from this year’s population: \\(p_{t+1} = L p_t\\). Below is the Leslie matrix for this example. \\[ \\begin{bmatrix} f_{t+1} \\\\ y_{t+1} \\\\ ya_{t+1} \\\\ a1_{t+1} \\\\ a2_{t+1} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 37 &amp; 64 &amp; 82 \\\\ 0.06 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0&amp;0.28 &amp; 0 &amp; 0 &amp; 0 \\\\ 0&amp;0&amp;0.16&amp; 0 &amp; 0 \\\\ 0&amp;0&amp;0&amp;0.08&amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} f_t \\\\ y_t \\\\ ya_t \\\\ a1_t \\\\ a2_t \\end{bmatrix} \\] L = cbind(c(0,.06,0,0,0),c(0,0,.28,0,0),c(37,0,0,.16,0),c(64,0,0,0,.08),c(82,0,0,0,0.00)) The trout population in the creek is known to be dying off largely due to poisoning by the insecticide rotenone. The model demonstrates this behavior here, as can be seen in the folowing plot, which starts with 200 trout in each age group. You shouldn’t need to edit this plot. start = c(200,200,200,200,200) # the starting distribution N = 35 # N is the number of iterations X = matrix(0,nrow=5,ncol=N) # Store the results in a 3 x N matrix called X X[,1] = start # put start in the first column of X # loop N times and put your results in X for (i in 2:N) {X[,i] = L %*% X[,i-1]} # Then plot the results t = seq(1,N) # time plot(t,X[1,],type=&#39;l&#39;,col=1,ylim=c(0,8000),ylab=&quot;population&quot;,xlab=&quot;time (year)&quot;, main=&quot;Population in Age Group&quot;) for (i in 1:5) { lines(t,X[i,],col=i) points(t,X[i,],col=i,pch=20,cex=.8)} legend(22, 7600, legend=c(&quot;Fingerlings (0-1)&quot;, &quot;Yearlings (1-2)&quot;, &quot;Young Adults (2-3)&quot;,&quot;Adults (3-4)&quot;,&quot;Adults (4-5)&quot;), col=1:5, lty=1) By hand, multiply the matrix vector product \\(L x(t)\\) above and use it to give formulas for \\(f_{t+1}, y_{t+1}, ya_{t+1}, a1_{t+1}, a2_{t+1}\\) in terms of \\(f_t, y_t, ya_t, a1_t, a2_t\\). Use these formulas to give the meaning of the values 37, 64, 82, 0.06, 0.28, 0.16, 0.08 that appear in this matrix. Compute the eigenvectors and eigenvalues of \\(L\\) and relate what you find to population dynamics. In particular, use the eigen-information to Give the overall population growth rate. Give the limiting age distribution: that is, the long-run distribution of the population in the different age categories. Give your answer as proportions which sum to 1. You are seeking funding from the Michigan DNR to support a cleanup effort. As part of your proposal, you argue that you believe that such a cleanup will most impact the youngest fish and will improve the survival rate of fingerlings to yearlings. Figure out (by trial and error) how high this survival rate will need to grow in order for the population to stop dying off. Justify your answer with eigenvalues and a plot. You should be able to duplicate the code for the plot above (after changing the matrix). 8.4 Halverson Numbers We will look at the Fibonacci example on Monday (11/8) in class. This is very much like it except that the calculations are easier. In that problem we start with a matrix \\(F\\) that generates the Fibonacci numbers and used it to find a closed-formula formula \\(f_n\\) for the \\(n\\)th Fibonacci number. Here you are to follow the same process to find a formula \\(h_n\\) for the nearly-as-famous Halverson numbers. The Halverson numbers are defined by \\(h_0 = 0\\), \\(h_1 = 1\\), and \\(h_{n+1} = 3 h_{n-1} + 2 h_{n}\\). They start out as \\(0,1,2,7,20,61,182,547,1640,4921,14762\\). Find a matrix \\(H\\) so that \\(H \\begin{bmatrix} h_{n-1} \\\\ h_n \\end{bmatrix} = \\begin{bmatrix} h_{n} \\\\ h_{n+1} \\end{bmatrix}\\). Find the eigenvalues and eigenvectors of \\(H\\). Use the eigenvalues and eigenvectors to find a closed-formula for \\(h_n\\). Show that your formula works to produce \\(h_{10} = 14762.\\) "],["problem-set-9.html", "Section 9 Problem Set 9 9.1 Glucose-Insulin 9.2 The Rise of Moscow 9.3 Orthogonal Complements", " Section 9 Problem Set 9 Download an Rmd Template for this problem set. Upload a completed, knitted .html version of this file on Moodle. If you have collaborated with others on this assignment (encouraged), please include their names here (no penalty). 9.1 Glucose-Insulin The hormone insulin helps regulate glucose metabolism in your blood. The presene of insulin helps your body absorb excess glucose. Here \\(G_t\\) (glucose) and \\(H_t\\) (insulin) are measured as excess values (in mg per 100 ml of blood) above the steady state. \\[ \\begin{bmatrix} G_{t+1} \\\\ H_{t+1} \\end{bmatrix} = \\begin{bmatrix}0.9 &amp; -0.4 \\\\0.1 &amp; 0.9 \\\\\\end{bmatrix} \\begin{bmatrix} G_t \\\\ H_t \\end{bmatrix}= \\begin{bmatrix} 0.9 G_t - 0.4 H_t \\\\ 0.1 G_t + 0.9 H_t \\end{bmatrix} \\] Here is what happens if we start at \\((1,0)\\) and iterate. That is we start with 1 unit excess glucose. Observe that the system spirals back to the steady state of \\((0,0)\\). We can plot the indivdual glucose and insulin coordinates over time. These are the x and y coordinates of the points in the above plot. You see the insulin responding to the excess glucose, and then the glucose being absorbed by the presence of insulin, and so on … The key point here is that the spiraling in the (x,y) plane or oscillating in the (x,t) plane corresponds to the presence of complex eigenvalues. Your job is to perform an eigen-analysis of this problem: Use R to find the eigenvalues and eigenvectors of \\(A\\). Write out the eigenvalues in the form \\(\\lambda = a \\pm b i\\) and the eigenvectors in the form \\(\\vec{v} = \\vec{u} \\pm \\vec{w} i\\) by filling in the values below: \\[ \\begin{array}{lll} \\lambda_1 = a + b i &amp; \\qquad &amp; \\lambda_2 = a - b i \\\\ v_1 = \\begin{bmatrix} xx \\\\ yy \\end{bmatrix} + \\begin{bmatrix} zz \\\\ ww \\end{bmatrix}i &amp;&amp; v_2 = \\begin{bmatrix} xx \\\\ yy \\end{bmatrix} - \\begin{bmatrix} zz \\\\ ww \\end{bmatrix}i \\end{array} \\] Write the matrix \\(A\\) in its rotation-dilationalized form by filling in the entries below with their appropriate values. \\[ \\begin{bmatrix}0.9 &amp; -0.4 \\\\0.1 &amp; 0.9 \\\\\\end{bmatrix} = \\begin{bmatrix} p_{11} &amp; p_{12} \\\\ p_{21} &amp; p_{22} \\\\\\end{bmatrix} \\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\\\\\end{bmatrix} \\begin{bmatrix} q_{11} &amp; q_{12} \\\\ q_{21} &amp; q_{22} \\\\\\end{bmatrix} \\] Use R to multiply the matrices above and show that the really do combine to give \\(A\\). Find the scaling factor \\(|\\lambda|\\) for this matrix and the angle of rotation \\(\\arctan(b/a)\\). Give your answer in degrees. Compare your answers from part (c) to the plots above to confirm that the system is doing what the eigenvalues predict. 9.2 The Rise of Moscow This problem is described in Network Centralities. You will analyze a network of trade routes in medieval Russia. If you were not in class on Nov 10 (or if you need it described again), I made a video last year in Module 1 explaining the Airline Network example. If you collaborate on this one with classmates (and I hope you do), please include the names of the students that you worked with. You need the igraph package for this problem. You will need to find and install it if you did not do so earlier this semester when we looked at the Korean airport problem. library(igraph) Russian historians often attribute the dominance and rise to power of Moscow to its strategic position on medieval trade routes (see Figure 1). Others argue that sociological and political factors aided Moscow’s rise to power, and thus Moscow did not rise to power strictly because of its strategic location on the trade routes. The figure below shows the major cities and trade routes of medieval Russia. Use Gould’s Index to form a geographer’s opinion about this debate. Either: Moscow’s location was the primary reason for its rise to power, or Other forces must have come into play. Here is the adjacency matrix for this transportation network into an adjacency matrix and a plot of the network. RusCity = c(&quot;Novgorod&quot;, &quot;Vitebsk&quot;, &quot;Smolensk&quot;, &quot;Kiev&quot;, &quot;Chernikov&quot;, &quot;Novgorod Severskij&quot;, &quot;Kursk&quot;, &quot;Bryansk&quot;, &quot;Karachev&quot;, &quot;Kozelsk&quot;, &quot;Dorogobusch&quot;, &quot;Vyazma&quot;, &quot;A&quot;, &quot;Tver&quot;, &quot;Vishnij Totochek&quot;, &quot;Ksyatyn&quot;, &quot;Uglich&quot;, &quot;Yaroslavl&quot;, &quot;Rostov&quot;, &quot;B&quot;, &quot;C&quot;, &quot;Suzdal&quot;, &quot;Vladimir&quot;, &quot;Nizhnij Novgorod&quot;, &quot;Bolgar&quot;, &quot;Isad&#39;-Ryazan&quot;, &quot;Pronsk&quot;, &quot;Dubok&quot;, &quot;Elets&quot;, &quot;Mtsensk&quot;, &quot;Tula&quot;, &quot;Dedoslavl&quot;, &quot;Pereslavl&quot;, &quot;Kolomna&quot;, &quot;Moscow&quot;, &quot;Mozhaysk&quot;, &quot;Dmitrov&quot;, &quot;Volok Lamskij&quot;, &quot;Murom&quot;) A = rbind(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)) g=graph_from_adjacency_matrix(A,mode=&#39;undirected&#39;) V(g)$label = RusCity # Plot network plot(g) Create a vector containing the normalized Degree Centralities. See Section 18.2 for help. Create a vector containing the Gould Index values. See Section 18.3.5 for help. Create a table that contains Gould’s Index and Degree Centralities. See Section 18.3.4 for help. The rows should be labeled with the city names and the columns should be named by the centrality measures. Sort the table according to Gould’s Index. Plot the network with the size of the vertices determined by Gould’s Index and the size of the label is determined by degree centrality. Use Gould’s Index to decide whether Moscow’s dominance was solely due to its geographic location. Compare the Gould’s Index and Degree Centrality rankings and note any interesting findings. See Section 18.3.4 for help. 9.3 Orthogonal Complements (We will work on in class on Monday Nov 15 or possibly Wednesday Nov 17) Here are two subspaces of \\(\\mathbb{R}^5\\) that we have seen before. (See PS5.3 and PS6.1) \\[ \\mathsf{Z} = \\left\\{ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} \\ \\bigg\\vert \\ x_1 + x_2 + x_3 + x_4 + x_5 = 0, x_4 = 2 x_2 \\right\\}, \\] \\[ \\mathsf{F} = \\left\\{ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} \\ \\bigg\\vert \\ x_3 = x_1 + x_2, x_4 = x_2 + x_3, x_5 = x_3 + x_4 \\right\\}. \\] Find the orthogonal complement of each subspace in \\(\\mathbb{R}^5\\). For each example, compute \\(\\dim(W) + \\dim(W^\\perp)\\). "],["problem-set-10.html", "Section 10 Problem Set 10 10.1 Projection Onto Subspace (with an orthogonal basis) 10.2 Least-Squares Projection Onto Subspace (without an orthogonal basis) 10.3 Pseudoinverse and Projection 10.4 Least-Squares Polynomials 10.5 Fuel Efficiency 10.6 Fourier Analysis 10.7 Spectral Decomposition 10.8 SVD", " Section 10 Problem Set 10 Due: Friday December 10, 5PM, the last day of classes. Here is an Rmd Template for PS10, Important Notes: This is a long assignment, but I am designing class so that you can get much of it done during class. In parentheses at the beginning of each problem, I indicate the day in class that we will discuss the material. We may even work on it in class that day. On Friday December 3, most of the class will be dedicated to working on Problems 10.4-10.7. 10.1 Projection Onto Subspace (with an orthogonal basis) (Mon 11/29) You can do this one by hand or by R. On Exam 2 you found the basis below for the set of palindromic vectors in \\(\\mathbb{R}^5\\). Note that the basis is orthogonal. \\[ \\mathbf{P} = \\left\\{ \\begin{bmatrix} a \\\\ b \\\\ c \\\\ b \\\\ a \\end{bmatrix} \\mid a,b,c \\in \\mathbb{R} \\right\\} = \\mathsf{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0\\\\ 0 \\end{bmatrix} \\right\\} \\] The vector \\(w\\) below is not a palindrome. Find the orthogonal projection \\(\\hat w = {proj}_\\mathsf{P}(w)\\) of \\(w\\) onto \\(\\mathsf{P}\\). \\[ w= \\begin{bmatrix} 5 \\\\ 4 \\\\ 3 \\\\ 4 \\\\ 6 \\end{bmatrix}. \\] Compute the residual vector \\(z = w - \\hat w\\) and show that it is orthogonal to \\(\\mathsf{P}\\). The vector \\(w&#39;\\) below seems like a good approximation of \\(w\\) by a palindromic vector. \\[ w&#39;= \\begin{bmatrix} 5 \\\\ 4 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}. \\] Find the distance between \\(w\\) and \\(\\hat w\\) and \\(w\\) and \\(w&#39;\\) to show that, in fact, \\(\\hat w\\) is the better approximation of \\(w\\) by a vector in \\(\\mathsf{P}\\). 10.2 Least-Squares Projection Onto Subspace (without an orthogonal basis) (Wed 12/1) On previous assignments we have found bases of the Fibonacci subspace \\(F\\) and the zero-sum space \\(Z\\) of \\(\\mathbb{R}^5\\) shown below, \\[ F = \\left\\{ \\begin{bmatrix} a \\\\ b \\\\ a + b \\\\ a + 2 b \\\\ 2 a + 3 b \\end{bmatrix} \\mid a,b \\in \\mathbb{R} \\right\\} = \\mathsf{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 2 \\\\3 \\end{bmatrix} \\right\\}. \\hskip5in \\] \\[ Z = \\left\\{ \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\end{bmatrix} \\mid a+b+c+d+e=0 \\right\\} = \\mathsf{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{bmatrix} \\right\\}. \\hskip5in \\] Let \\(v = \\begin{bmatrix} 4\\\\ -1 \\\\ 2 \\\\ 1 \\\\ -2 \\end{bmatrix}\\). Find the least-squares projection \\(\\hat v\\) of \\(v\\) onto \\(F\\). Find the residual \\(z\\) and show that it is in \\(F^\\perp\\). Find the distance from \\(v\\) to \\(F\\). Find the least-squares projection \\(\\hat v\\) of \\(v\\) onto \\(Z\\). Find the residual \\(z\\) and show that it is in \\(Z^\\perp\\). Find the distance from \\(v\\) to \\(Z\\). 10.3 Pseudoinverse and Projection (Wed 12/1) Consider the following subspace of \\(\\mathbb{R}^4\\) and vector \\(b \\in \\mathbb{R}^4\\), \\[ W = span\\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\right\\}, \\hskip.6in b = \\begin{bmatrix} 9 \\\\ 5 \\\\ 5 \\\\ 8 \\end{bmatrix}. \\] If we put the basis vectors into the columns of a matrix \\(A\\), then \\(W = Col(A)\\) is the column space of \\(A\\). (A = cbind(c(1,2,-1,-2),c(1,2,3,4),c(1,0,1,0))) ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 2 0 ## [3,] -1 3 1 ## [4,] -2 4 0 b = c(9,5,5,8) Perform a matrix computation on A to show that the basis is not orthogonal. Show that b is not in W by augmenting and row reducing. Find the least-squares projection of b onto W. Find both \\(\\hat x\\) and \\(\\hat b\\). Calculate the residual vector r, show that \\(r \\in W^\\perp\\), and find \\(||r||\\). Consider the following derivation from the normal equations: \\[ A^T A x = A^T b \\qquad \\Longrightarrow \\qquad \\hat x = (A^T A)^{-1} A^T b. \\] The pseudoinverse is the matrix \\[ A^+ = (A^T A)^{-1} A^T \\] From what we see above it gives the least-squares solution to \\(A x = b\\). Compute the matrix \\(A^+\\), multiply it by \\(b\\), and show that you get \\(\\hat x\\). Continuing this story, \\[ \\hat b = A \\hat x \\quad \\text{and}\\quad \\hat x = A^+ b \\qquad \\Longrightarrow \\qquad \\hat b = A (A^T A)^{-1} A^T b. \\] The projection matrix onto the subspace \\(W\\) is the matrix \\[ P = A (A^T A)^{-1} A^T. \\] Compute the matrix \\(P\\), apply it to \\(b\\), and see that you get the projected value \\(\\hat b\\). Use \\(P\\) to project the vector b2 = c(1,2,3,4) onto \\(W\\). What happens and why? Hint: where is b2? Look at the original matrix \\(A\\). Compute \\(P^2\\) and compare it to \\(P\\). Explain why this happens. Find the eigenvalues of \\(P\\). They are nice. Explain (briefly) where the eigenvectors of this matrix are in relation to \\(W\\). 10.4 Least-Squares Polynomials In Fitting a Linear Function and Fitting a Quadratic Function we make the quadratic fit to the data as shown below. Make a cubic, quartic, and quintic fit to this data. Turn in a plot of each. Comupute the length of the residual in each case. Which do you think is the best model of the data? x = c(1,2,3,4,5,6) y = c(7,2,1,3,7,7) (A = cbind(x^0,x,x^2)) ## x ## [1,] 1 1 1 ## [2,] 1 2 4 ## [3,] 1 3 9 ## [4,] 1 4 16 ## [5,] 1 5 25 ## [6,] 1 6 36 xhat = solve(t(A)%*%A,t(A)%*%y) yhat = A %*% xhat r = y - yhat dot(r,r) ## [1] 11.26429 #plot the original set of points plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main=&#39;the best-fit quadratic function&#39;) # generate points for the fitted line and plot it tt = seq(0,7,len=100) lines(tt,xhat[1]+xhat[2]*tt+xhat[3]*tt^2,col=&#39;blue&#39;) # add the residuals to the plot for (i in 1:length(x)) { lines(c(x[i],x[i]),c(y[i],yhat[i]), col=&#39;red&#39;) } #add yhat to the plot points(x,yhat,pch=19,col=&#39;orange&#39;) #put the original points back on the plot last so we can see them points(x,y,pch=19,col=&quot;black&quot;) grid() 10.5 Fuel Efficiency (Fri 12/3) Below is a classic data set of fuel efficiency in 38 different automobiles. MPG=c(16.9,15.5,19.2,18.5,30,27.5,27.2,30.9,20.3,17,21.6,16.2,20.6,20.8,18.6,18.1,17,17.6,16.5,18.2,26.5,21.9,34.1,35.1,27.4,31.5,29.5,28.4,28.8,26.8,33.5,34.2,31.8,37.3,30.5,22,21.5,31.9) lbs=c( 3967.6,3689.14,3280.55,3585.4,1961.05,2329.6,2093,2029.3,2575.3,2857.4,2543.45,3103.1,3075.8,2793.7,3294.2,3103.1,3494.4,3389.75,3599.05,3485.3,2352.35,2648.1,1797.25,1742.65,2429.7,1810.9,1942.85,2429.7,2361.45,2457,2325.96,2002,1838.2,1938.3,1992.9,2561.65,2366,1925) HP= c(155,142,125,150,68,95,97,75,103,125,115,133,105,85,110,120,130,129,138,135,88,109,65,80,80,71,68,90,115,115,90,70,65,69,78,97,110,71) Cyl=c(8,8,8,8,4,4,4,4,5,6,4,6,6,6,6,6,8,8,8,8,4,6,4,4,4,4,4,4,6,6,4,4,4,4,4,6,4,4) Car = c(&quot;BuickEstateWagon&quot;, &quot;FordCountrySquireWagon&quot;, &quot;ChevyMalibuWagon&quot;, &quot;ChryslerLeBaronWagon&quot;, &quot;Chevette&quot;, &quot;ToyotaCorona&quot;, &quot;Datsun510&quot;, &quot;DodgeOmni&quot;, &quot;Audi5000&quot;, &quot;Volvo240GL&quot;, &quot;Saab99GLE&quot;, &quot;Peugeot694SL&quot;, &quot;BuickCenturySpecial&quot;, &quot;MercuryZephyr&quot;, &quot;DodgeAspen&quot;, &quot;AMCConcordD/L&quot;, &quot;ChevyCapriceClassic&quot;, &quot;FordLTD&quot;, &quot;MercuryGrandMarquis&quot;, &quot;DodgeStRegis&quot;, &quot;FordMustang4&quot;, &quot;FordMustangGhia&quot;, &quot;MazdaGLC&quot;, &quot;DodgeColt&quot;, &quot;AMCSpirit&quot;, &quot;VWScirocco&quot;, &quot;HondaAccordLX&quot;, &quot;BuickSkylark&quot;, &quot;ChevyCitation&quot;, &quot;OldsOmega&quot;, &quot;PontiacPhoenix&quot;, &quot;PlymouthHorizon&quot;, &quot;Datsun210&quot;, &quot;FiatStrada&quot;, &quot;VWDasher&quot;, &quot;Datsun810&quot;, &quot;BMW320i&quot;, &quot;VWRabbit&quot;) df = data.frame(cbind(lbs,HP,Cyl,MPG)) #Convert to data frame rownames(df)=Car df ## lbs HP Cyl MPG ## BuickEstateWagon 3967.60 155 8 16.9 ## FordCountrySquireWagon 3689.14 142 8 15.5 ## ChevyMalibuWagon 3280.55 125 8 19.2 ## ChryslerLeBaronWagon 3585.40 150 8 18.5 ## Chevette 1961.05 68 4 30.0 ## ToyotaCorona 2329.60 95 4 27.5 ## Datsun510 2093.00 97 4 27.2 ## DodgeOmni 2029.30 75 4 30.9 ## Audi5000 2575.30 103 5 20.3 ## Volvo240GL 2857.40 125 6 17.0 ## Saab99GLE 2543.45 115 4 21.6 ## Peugeot694SL 3103.10 133 6 16.2 ## BuickCenturySpecial 3075.80 105 6 20.6 ## MercuryZephyr 2793.70 85 6 20.8 ## DodgeAspen 3294.20 110 6 18.6 ## AMCConcordD/L 3103.10 120 6 18.1 ## ChevyCapriceClassic 3494.40 130 8 17.0 ## FordLTD 3389.75 129 8 17.6 ## MercuryGrandMarquis 3599.05 138 8 16.5 ## DodgeStRegis 3485.30 135 8 18.2 ## FordMustang4 2352.35 88 4 26.5 ## FordMustangGhia 2648.10 109 6 21.9 ## MazdaGLC 1797.25 65 4 34.1 ## DodgeColt 1742.65 80 4 35.1 ## AMCSpirit 2429.70 80 4 27.4 ## VWScirocco 1810.90 71 4 31.5 ## HondaAccordLX 1942.85 68 4 29.5 ## BuickSkylark 2429.70 90 4 28.4 ## ChevyCitation 2361.45 115 6 28.8 ## OldsOmega 2457.00 115 6 26.8 ## PontiacPhoenix 2325.96 90 4 33.5 ## PlymouthHorizon 2002.00 70 4 34.2 ## Datsun210 1838.20 65 4 31.8 ## FiatStrada 1938.30 69 4 37.3 ## VWDasher 1992.90 78 4 30.5 ## Datsun810 2561.65 97 6 22.0 ## BMW320i 2366.00 110 4 21.5 ## VWRabbit 1925.00 71 4 31.9 Fit a linear model of the form \\[ mpg = a_0 + a_1 lbs + a_2 HP + a_3 Cyl. \\] Find the coefficients \\(a_0,a_1,a_2,a_3\\) and the length of the residual. If you have taken STAT 155, you can see that we are doing the exact same thing by comparing your results with ## ## Call: ## lm(formula = MPG ~ lbs + HP + Cyl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4669 -1.6011 -0.3246 1.0759 6.6231 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.644579 1.992433 24.917 &lt; 2e-16 *** ## lbs -0.008288 0.002316 -3.579 0.00106 ** ## HP -0.073961 0.043862 -1.686 0.10091 ## Cyl 0.791590 0.730326 1.084 0.28604 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.791 on 34 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.8183 ## F-statistic: 56.55 on 3 and 34 DF, p-value: 2.656e-13 The residual vector \\(\\mathsf{r}\\) measures the quality of fit of our model. But how do we turn this into a meaningful quantity? One method is to look at the coefficient of determination, which is more commonly refered to as the “\\(R^2\\) value.” You can see the \\(R^2\\) value of your fit in part (a) under the “Multiple R-squared” output in the linear model summary above. If \\(\\mathsf{y} = [ y_1, y_2, \\ldots, y_n ]^{\\top}\\) is our target vector with least-squares solution \\(\\hat{\\mathsf{y}} = A \\hat{\\mathsf{x}}\\) and residual vector is \\(\\mathsf{r} = \\mathsf{y} - \\hat{\\mathsf{y}}\\). Let \\[ a = \\frac{1}{n} ( y_1 + y_2 + \\cdots + y_n) \\] be the average or mean of the entries of target vector \\(\\mathsf{y}\\) and let \\(\\mathsf{y}^* = [a, a, \\ldots, a]\\). (We call this vector “y star”, so ystar would be a fine name in R.) The \\(R^2\\) value is \\[ R^2 = 1 - \\frac{\\| \\mathsf{y} - \\hat{\\mathsf{y}} \\|^2 }{\\| \\mathsf{y} - \\mathsf{y}^* \\|^2} = 1 - \\frac{\\| \\mathsf{r} \\|^2}{\\| \\mathsf{y} - \\mathsf{y}^* \\|^2}. \\] The \\(R^2\\) value is a number in \\([0,1]\\). The squared-length \\(|| \\mathsf{y} -\\mathsf{y}^*||^2\\) is the total variance: that is, how much the data varies from the mean, and \\(\\frac{\\| \\mathsf{r} \\|^2}{\\| \\mathsf{y} - \\mathsf{y}^* \\|^2}\\) tells us the fraction of the total variance that is explained by our model. Thus, if \\(R^2\\) is near 1, then our model does a good job at “explaining” the behavior of \\(\\mathsf{y}\\) via a linear combination of the columns of \\(A\\). To do: Find the \\(R^2\\) value for our least squares solution to the cars data in part (a). Here are some helpful functions: mean(vec) returns the mean (average) of the entries of the vector vec rep(a, n) creates a constant vector of length \\(n\\) where every entry is \\(a\\). Norm(vec) from the pracma package returns the magnitude (Euclidean length) of the vector vec. To learn more, you should take STAT 155: Introduction to Statistical Modeling. 10.6 Fourier Analysis (Fri 12/3) In Fourier analysis one uses trigonometric functions to model oscillatory behavior in data. These methods have important applications in the study of sound or video signals, financial data, medicine, and engineering (to mention just a few). For example, consider the following set of 200 data points. t = xx = seq(0,19.9,.1) y = c(3.407646, 3.656257, 4.567893, 3.692689, 4.650019, 4.180795, 4.220037, 4.842083, 4.600134, 3.695645, 3.739377, 4.807793, 4.290227, 4.351877, 4.659800, 4.706735, 4.603592, 4.657165, 5.135868, 4.486025, 4.644551, 4.624029, 5.329163, 5.639380, 5.693772, 4.806000, 5.427808, 5.673742, 5.121300, 5.394885, 4.739374, 5.084819, 5.460250, 4.578189, 4.612040, 4.534047, 4.201825, 4.290607, 3.887900, 3.349325, 3.660084, 3.200437, 2.490044, 2.720811, 2.762054, 3.041436, 2.018788, 2.188567, 2.054767, 2.047622, 2.294727, 2.699933, 3.242642, 3.325224, 3.411680, 2.590417, 3.118911, 2.916444, 3.081886, 4.100586, 4.210242, 3.835767, 3.546563, 4.456711, 3.970233, 4.128838, 4.774915, 3.610540, 4.395443, 3.764436, 4.407476, 4.243399, 3.684473, 3.779193, 3.815080, 4.567609, 4.576654, 4.774486, 4.847797, 3.970489, 4.631950, 4.535347, 5.292626, 4.844237, 5.243421, 4.949116, 4.824773, 4.830172, 5.379016, 5.289537, 5.832770, 4.872205, 4.833122, 4.641696, 4.584196, 5.279393, 4.307142, 4.926093, 3.904820, 3.748701, 3.460324, 3.726250, 3.636625, 3.896051, 3.505842, 2.723539, 3.432293, 2.788161, 2.873195, 2.347629, 2.515592, 2.618861, 2.622653, 2.263514, 2.580999, 2.675959, 3.071311, 3.375476, 2.769042, 3.177973, 3.808895, 3.088136, 3.101224, 3.828743, 4.070292, 4.477982, 3.982855, 4.213733, 4.396489, 4.036487, 4.475438, 4.534266, 3.885322, 4.555555, 4.776902, 4.577201, 4.374555, 4.184732, 3.960706, 3.885492, 4.246883, 4.885794, 5.117945, 4.213779, 4.734693, 5.359801, 4.680284, 5.586846, 4.995826, 5.074366, 4.647961, 4.935794, 5.074724, 5.092661, 4.660553, 5.386633, 5.101599, 5.585815, 4.399249, 4.799980, 4.546865, 4.375893, 4.305302, 3.382458, 3.915698, 2.980115, 3.711861, 3.260457, 2.493755, 2.267661, 2.994923, 2.447978, 2.093928, 2.379100, 2.836308, 2.904491, 2.084674, 2.050629, 2.370026, 2.877150, 3.372492, 3.679573, 3.158224, 3.345067, 3.600110, 3.381230, 4.116003, 3.785123, 4.519719, 3.966509, 3.808330, 4.551462, 3.838009, 3.758539, 3.816730, 4.618030, 3.926753, 4.593788, 3.894390, 4.779126) plot(t,y,col=&quot;orange&quot;,xlim=c(0,20),ylim=c(2,6),pch=19) A first Fourier approximation would fit a model of the form \\[ f_1(t) = c_0 + c_1 \\sin(t) + c_2 \\cos(t). \\] Thus, we make the following matrix (we show here only the first 10 rows; there are 200 rows). A = cbind(t^0, sin(t),cos(t)) A[1:10,] ## [,1] [,2] [,3] ## [1,] 1 0.00000000 1.0000000 ## [2,] 1 0.09983342 0.9950042 ## [3,] 1 0.19866933 0.9800666 ## [4,] 1 0.29552021 0.9553365 ## [5,] 1 0.38941834 0.9210610 ## [6,] 1 0.47942554 0.8775826 ## [7,] 1 0.56464247 0.8253356 ## [8,] 1 0.64421769 0.7648422 ## [9,] 1 0.71735609 0.6967067 ## [10,] 1 0.78332691 0.6216100 Now we solve the normal equations ## [,1] ## [1,] 3.9971143 ## [2,] 1.0207277 ## [3,] -0.4486618 and plot the solution plot(t,y,col=&quot;orange&quot;,xlim=c(0,20),ylim=c(2,6),pch=19) tt = seq(0,20,len=1000) yy = xhat[1] + xhat[2]*sin(tt) + xhat[3]*cos(tt) points(tt,yy,type=&#39;l&#39;,col=&quot;blue&quot;) Your task: Update this to add the second Fourier coefficient terms by fitting the following function to the data. Plot your result. \\[ f_2(t) = c_0 + c_1 \\sin(t) + c_2 \\cos(t) + c_3 \\sin(2t) + c_4 \\cos(2t) \\] Compute the length of the residul vector for both the \\(f_1(t)\\) and the \\(f_2(t)\\) model. Which approximation looks better visually. That is, does the second approximation capture more of the shape of the data, or do you think that the first is a better model? 10.7 Spectral Decomposition Consider the following symmetric \\(4 \\times 4\\) matrix \\[ A = \\begin{bmatrix} 1.6 &amp; 0.7 &amp; 1.4 &amp; 0.3\\\\ 0.7 &amp; 1.6 &amp; 0.3 &amp; 1.4 \\\\ 1.4 &amp; 0.3 &amp; 1.6 &amp; 0.7 \\\\ 0.3 &amp; 1.4 &amp; 0.7 &amp; 1.6 \\\\ \\end{bmatrix} \\] (A= rbind(c(1.6, 0.7, 1.4, 0.3),c(0.7, 1.6, 0.3, 1.4),c(1.4, 0.3, 1.6, 0.7),c(0.3, 1.4, 0.7, 1.6))) ## [,1] [,2] [,3] [,4] ## [1,] 1.6 0.7 1.4 0.3 ## [2,] 0.7 1.6 0.3 1.4 ## [3,] 1.4 0.3 1.6 0.7 ## [4,] 0.3 1.4 0.7 1.6 Orthogonally diagonalze \\(A\\) and write it in the form \\(A = U D U^T\\) as shown here. \\[ \\begin{bmatrix} 1.6 &amp; 0.7 &amp; 1.4 &amp; 0.3\\\\ 0.7 &amp; 1.6 &amp; 0.3 &amp; 1.4 \\\\ 1.4 &amp; 0.3 &amp; 1.6 &amp; 0.7 \\\\ 0.3 &amp; 1.4 &amp; 0.7 &amp; 1.6 \\\\ \\end{bmatrix}= \\begin{bmatrix} a_1 &amp; b_1 &amp; c_1 &amp; c_1\\\\ a_2 &amp; b_2 &amp; c_2 &amp; d_2 \\\\ a_3 &amp; b_3 &amp; c_3 &amp; c_3 \\\\ a_4 &amp; b_4 &amp; c_4 &amp; d_4 \\\\ \\end{bmatrix} \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\lambda_3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\lambda_4 \\\\ \\end{bmatrix} \\begin{bmatrix} a_1 &amp; a_2 &amp; a_3 &amp; a_4 \\\\ b_1 &amp; b_2 &amp; b_3 &amp; b_4 \\\\ c_1 &amp; c_2 &amp; c_3 &amp; c_4 \\\\ d_1 &amp; d_2 &amp; d_3 &amp; d_4 \\\\ \\end{bmatrix} \\] b. Compute the spectral decomposition \\(A = \\lambda_1 \\mathbf{u}_1 \\mathbf{u}_1^T + \\lambda_2 \\mathbf{u}_2 \\mathbf{u}_2^T + \\lambda_3 \\mathbf{u}_3 \\mathbf{u}_3^T+ \\lambda_4 \\mathbf{u}_4 \\mathbf{u}_4^T\\) and write the matrix \\(A\\) as the sum of four rank-1 matrices. NOTE: you can just compute these matrices in R. You don’t need to write them out our typeset them. I realize it is a lot of busy work. \\[ \\begin{bmatrix} 1.6 &amp; 0.7 &amp; 1.4 &amp; 0.3\\\\ 0.7 &amp; 1.6 &amp; 0.3 &amp; 1.4 \\\\ 1.4 &amp; 0.3 &amp; 1.6 &amp; 0.7 \\\\ 0.3 &amp; 1.4 &amp; 0.7 &amp; 1.6 \\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; a_{34} \\\\ a_{41} &amp; a_{42} &amp; a_{43} &amp; a_{44} \\\\ \\end{bmatrix} + \\begin{bmatrix} b_{11} &amp; b_{12} &amp; b_{13} &amp; b_{14} \\\\ b_{21} &amp; b_{22} &amp; b_{23} &amp; b_{24} \\\\ b_{31} &amp; b_{32} &amp; b_{33} &amp; b_{34} \\\\ b_{41} &amp; b_{42} &amp; b_{43} &amp; b_{44} \\\\ \\end{bmatrix} \\\\ + \\begin{bmatrix} c_{11} &amp; c_{12} &amp; c_{13} &amp; c_{14} \\\\ c_{21} &amp; c_{22} &amp; c_{23} &amp; c_{24} \\\\ c_{31} &amp; c_{32} &amp; c_{33} &amp; c_{34} \\\\ c_{41} &amp; c_{42} &amp; c_{43} &amp; c_{44} \\\\ \\end{bmatrix} + \\begin{bmatrix} d_{11} &amp; d_{12} &amp; d_{13} &amp; d_{14} \\\\ d_{21} &amp; d_{22} &amp; d_{23} &amp; d_{24} \\\\ d_{31} &amp; d_{32} &amp; d_{33} &amp; d_{34} \\\\ d_{41} &amp; d_{42} &amp; d_{43} &amp; d_{44} \\\\ \\end{bmatrix} \\] We showed in class that the matrix \\(P_k = \\mathbf{u}_k \\mathbf{u}_k^T\\) is the orthogonal projection onto the \\(k\\)th eigenspace \\(E_{\\lambda_k}\\). Use these matrices to project \\(v\\) (below) onto each eigenspace and then write \\(v\\) as a sum of eigenvectors: \\(v = v_1 + v_2 + v_3 + v_4\\) where \\(v_i\\) is an eigenvector of eigenvalue \\(\\lambda_i\\). \\[ \\begin{array}{cccccccccc} v&amp;&amp; v_1 &amp;&amp; v_2 &amp;&amp; v_3 &amp;&amp; v_4 \\\\ \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 5 \\end{bmatrix} &amp;=&amp; \\begin{bmatrix} v_{11} \\\\ v_{12} \\\\ v_{13} \\\\ v_{14} \\end{bmatrix} &amp;+&amp; \\begin{bmatrix} v_{21} \\\\ v_{22} \\\\ v_{23} \\\\ v_{24} \\end{bmatrix} &amp;+&amp; \\begin{bmatrix} v_{31} \\\\ v_{32} \\\\ v_{33} \\\\ v_{34} \\end{bmatrix} &amp;+&amp; \\begin{bmatrix} v_{41} \\\\ v_{42} \\\\ v_{43} \\\\ v_{44} \\end{bmatrix} \\end{array} \\] 10.8 SVD Here is a \\(5 \\times 6\\) matrix. For this problem see the section SVD and Image Compression. (A = rbind(c(1 , 2 , 1 , 1 , 2 , 1 ),c(1 , 2 , 1 , 2 , -1 , 1 ), c(2 , 4 , 2 , 3 , 1 , 2 ), c(3 , -1 , 2 , -1 , -1 , 1 ), c(1 , 1 , 0 , 1 , 1 , 0 ))) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 1 1 2 1 ## [2,] 1 2 1 2 -1 1 ## [3,] 2 4 2 3 1 2 ## [4,] 3 -1 2 -1 -1 1 ## [5,] 1 1 0 1 1 0 Use the command svd(A,nv=ncol(A)) to command to compute the singular value decomposition of \\(A\\). The SVD tells us that \\(A = U \\Sigma V^T\\). Multiplying by \\(U^T\\) on the left and \\(V\\) on the right gives us \\(U^T A V = \\Sigma\\). Compute this matrix \\(\\Sigma\\) by extracting \\(U\\) and \\(V\\) from the previous part and multiplying \\(U^T A V\\). Take the square roots of the eigenvalues of \\(A^T A\\) to see that you get the singular values (the square root function is “vectorized” so you can do them all at once). The matrix \\(V\\) should be the eigenvectors of the matrix \\(A^T A\\). Compute the eigenvectors of \\(A^T A\\) to confirm this. NOTE: The last eigenspace \\(E_0\\) is two dimensional so there is not a single eigen-direction. There are many sets of orthgonal vectors for this two dimensional space and the svd command does not give the same ones as what you get here. Both are correct. Compute the matrix \\(A_1 + A_2\\) where \\(A_1 = \\sigma_1 u_1 v_1^T\\) and \\(A_2 = \\sigma_2 u_2 v_2^T\\). This is the second singular-value approximation of \\(A\\). Row reduce \\(A_1 + A_2\\) to show that it has rank 2. "],["important-definitions.html", "Section 11 Important Definitions 11.1 Systems of Equations 11.2 Linear Transformations 11.3 Vector Spaces 11.4 Matrices", " Section 11 Important Definitions 11.1 Systems of Equations Row operations The elementary row operations are 1) swap two rows 2) scale a row by a nonzero scalar 3) replace a row by the sum of that row plus a scalar multiple of another row Linear combination A linear combination of a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) is a sum of the form \\[ x_1 \\mathsf{v}_1 + x_2 \\mathsf{v}_2 + \\cdots + x_n \\mathsf{v}_n \\] where the weights \\(x_1, x_2, \\ldots, x_n\\) are real numbers. Span The span of a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) is the set of all possible linear combinations of those vectors, so \\[ span(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n) = \\{ x_1 \\mathsf{v}_1 + x_2 \\mathsf{v}_2 + \\cdots + x_n \\mathsf{v}_n \\mid x_1, x_2, \\ldots, x_n \\in \\mathbb{R}\\}, \\] linear independence A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\ldots, \\mathsf{v}_n\\) are linearly independent if the only way to write \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] is with \\(c_1 = c_2 = \\cdots = c_n = 0\\). Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(A x = \\mathsf{0}\\) has only the trivial solution. This is true if and only if \\(A\\) has a pivot in every column so that there are no free variables. linear dependence Conversely, a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly dependent if there exist scalars \\(c_1, c_2,\\ldots, c_n \\in \\mathbb{R}\\) that are not all equal to 0 such that \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] This is called a dependence relation among the vectors. Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, c_2, \\ldots, c_n]^{\\top}\\) is a nontrivial solution to \\(A \\mathsf{x} = \\mathsf{0}\\). 11.2 Linear Transformations linear transformation A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation if the following three properties hold: \\(T({\\bf 0}) = {\\bf 0}\\). \\(T(\\mathsf{u} + \\mathsf{v}) = T(\\mathsf{u}) + T(\\mathsf{v})\\) for all vectors \\(\\mathsf{u},\\mathsf{v} \\in \\mathbb{R}^n\\). \\(T(c \\mathsf{u}) = c T(\\mathsf{u})\\) for all vectors \\(\\mathsf{v} \\in \\mathbb{R}^n\\) and all scalars \\(c \\in \\mathbb{R}\\). These properties say that \\(T\\) sends 0 to 0 and is preserves addition and scalar multiplication. onto A linear transformation \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) with matrix \\(A\\) is onto if for every \\(\\mathsf{b}\\in \\mathbb{R}^m\\) there is at least one \\(\\mathsf{v}\\in \\mathbb{R}^n\\) so that \\(T(\\mathsf{v}) = A \\mathsf{v}= \\mathsf{b}\\) The function is onto if \\(A\\) has a pivot in every row. onee-to-one A linear transformation \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) with matrix \\(A\\) is one-to-one if for every \\(\\mathsf{b}\\in \\mathbb{R}^m\\) there is at most one \\(\\mathsf{v}\\in \\mathbb{R}^n\\) so that \\(T(\\mathsf{v}) = A \\mathsf{v}= \\mathsf{b}\\) The function is one-to-one if \\(A\\) has a pivot in every column. 11.3 Vector Spaces span A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span a vector space \\(V\\) if for every \\(\\mathsf{v} \\in V\\) there exist a set of scalars (weights) \\(c_1, c_2, \\ldots, c_n \\in \\mathbb{R}\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, \\ldots, c_n]^{\\top}\\) is a solution to \\(A x = \\mathsf{v}\\). linear independence A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\ldots, \\mathsf{v}_n\\) are linearly independent if the only way to write \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] is with \\(c_1 = c_2 = \\cdots = c_n = 0\\). Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(A x = \\mathsf{0}\\) has only the trivial solution. linear dependence Conversely, a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly dependent if there exist scalars \\(c_1, c_2,\\ldots, c_n \\in \\mathbb{R}\\) that are not all equal to 0 such that \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] This is called a dependence relation among the vectors. Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, c_2, \\ldots, c_n]^{\\top}\\) is a nontrivial solution to \\(A \\mathsf{x} = \\mathsf{0}\\). linear transformation A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation when: \\(T(\\mathsf{u} + \\mathsf{v}) = T(\\mathsf{u}) + T(\\mathsf{v})\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\) (preserves addition) \\(T(c \\mathsf{u} ) = c T(\\mathsf{u})\\) for all \\(\\mathsf{u} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\) (preserves scalar multiplication). It follows from these that also \\(T(\\mathsf{0}) = \\mathsf{0}\\). one-to-one A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a one-to-one when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at most one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). onto A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a onto when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at least one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). subspace A subset \\(S \\subseteq \\mathbb{R}^n\\) is a subspace when: \\(\\mathsf{u} + \\mathsf{v} \\in S\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in S\\) (closed under addition) \\(c \\mathsf{u} \\in S\\) for all \\(\\mathsf{u}\\in S\\) and \\(c \\in \\mathbb{R}\\) (closed under scalar multiplication) It follows from these that also \\(\\mathsf{0} \\in S\\). basis A basis of a vector space (or subspace) \\(V\\) is a set of vectors \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) in \\(V\\) such that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span \\(V\\) \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly independent Equivalently, one can say that \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) is a basis of \\(V\\) if for every vector \\(\\mathsf{v} \\in V\\) there is a unique set of scalars \\(c_1, \\ldots, c_n\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] (The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence). dimension The dimension of a subspace \\(W\\) is the number of vectors in any basis of \\(W\\). This is also the fewest number of vectors required to span the subspace. 11.4 Matrices invertible The square \\(n \\times n\\) matrix \\(A\\) is invertible when there exists an \\(n \\times n\\) matrix \\(A^{-1}\\) such that \\(A A^{-1} = I = A^{-1} A\\). The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that \\(A\\) is invertible. null space The null space \\(\\mbox{Nul}(A) \\subset \\mathbb{R}^n\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of solutions to the homogeneous equation \\(A \\mathsf{x} = \\mathbf{0}\\)&gt; We also write this as \\[ \\mbox{Nul}(A) = \\{ \\mathsf{x} \\in \\mathbb{R}^n : A \\mathsf{x} = \\mathbf{0} \\} \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the kernel of \\(T\\) is the null space of matrix \\(A\\). column space The column space \\(\\mbox{Col}(A) \\subset \\mathbb{R}^m\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of all linear combinations of the columns of \\(A\\). For \\(A = \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 &amp; \\cdots &amp; \\mathsf{a}_n \\end{bmatrix}\\), we have \\[ \\mbox{Col}(A) = \\mbox{span} ( \\mathsf{a}_1, \\mathsf{a}_2, \\ldots , \\mathsf{a}_n ) \\] We also write this as \\[ \\mbox{Col}(A) = \\{ \\mathsf{b} \\in \\mathbb{R}^m : \\mathsf{b} = A \\mathsf{x} \\mbox{ for some } \\mathsf{x} \\in \\mathbb{R}^n \\}. \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the range (also called the image) of \\(T\\) is the column space of matrix \\(A\\). rank The rank of the \\(m \\times n\\) matrix \\(A\\) is the dimension of the column space of \\(A\\). This is also the number of pivot columns of the matrix. eigenvalue and eigenvector For a square \\(n \\times n\\) matrix \\(A\\), the scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue for \\(A\\) when there exists a nonzero vector \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(A \\mathsf{x} = \\lambda \\mathsf{x}\\). The nonzero vector \\(\\mathsf{x}\\) is the eigenvector for eigenvalue \\(\\lambda\\). The collection of all of these eigenvalues and eigenvectors is called the eigensystem of A. diagonalization A square \\(n \\times n\\) matrix is diagonalizable when \\(A = P D P^{-1}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an invertible matrix. In this case, the eigenvalues of \\(A\\) are the diagonal entries of \\(D\\) and their corresponding eigenvectors are the columns of \\(P\\). dominant eigenvalue The eigenvalue \\(\\lambda\\) of the square matrix \\(A\\) is the dominant eigenvalue when \\(| \\lambda | &gt; | \\mu |\\) where \\(\\mu\\) is any other eigenvalue of \\(A\\). The dominant eigenvalue determines the long-term behavior of \\(A^t\\) as \\(t \\rightarrow \\infty\\). "],["class-examples.html", "Section 12 Class Examples 12.1 Day 3: Wed Sep 8 12.2 Day 4: Fri Sep 10 12.3 Day 5: Mon Sep 13", " Section 12 Class Examples 12.1 Day 3: Wed Sep 8 Remember that to use RREF we must include the pracma package. Our goal is to solve the following system of equations: \\[ \\left\\{\\begin{array}{rrrrrrrrrr} x_1 &amp;+&amp; 2 x_2 &amp;+&amp;x_3 &amp;+&amp; x_4 &amp; = &amp; 4 \\\\ x_1 &amp;+&amp; 2x_2 &amp;+&amp; -x_3 &amp;+&amp; -3x_4&amp; =&amp; 6 \\\\ &amp;&amp; x_2 &amp;+&amp; x_3 &amp;+&amp; x_4&amp; =&amp; 0 \\\\ -x_1&amp;+&amp; x_2 &amp;+&amp; -x_3&amp;+&amp; -4x_4&amp; = &amp;-1\\\\ \\end{array} \\right\\} \\] We enter the augmented matrix (and echo it back): A = cbind(c(1,1,0,-1),c(2,2,1,1),c(1,-1,1,-1),c(1,-3,1,-4),c(4,6,0,-1)) A ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 1 1 4 ## [2,] 1 2 -1 -3 6 ## [3,] 0 1 1 1 0 ## [4,] -1 1 -1 -4 -1 And then we row reduce it using rref: rref(A) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 1 3 ## [2,] 0 1 0 -1 1 ## [3,] 0 0 1 2 -1 ## [4,] 0 0 0 0 0 The corresponding reduced set of equations is \\[ \\left\\{\\begin{array}{rrrrrrrrrr} x_1 &amp;&amp; &amp;&amp; &amp;+&amp; x_4 &amp; = &amp; 3 \\\\ &amp;&amp; x_2 &amp;&amp; &amp;-&amp; x_4&amp; =&amp; 1 \\\\ &amp;&amp; &amp;+&amp; x_3 &amp;+&amp; 2x_4&amp; =&amp; -1 \\\\ \\end{array} \\right\\} \\] The general solution to this system of equations is \\[ \\begin{align} x_1 &amp;= 3 - x_4 \\\\ x_2 &amp;= 1 - x_5 \\\\ x_3 &amp;= -1 - 2 x_4 \\\\ x_4 &amp;= free \\end{align} \\] 12.2 Day 4: Fri Sep 10 Question Solve the following matrix equation using R: \\[ \\begin{bmatrix} 1 &amp; -2 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ -1 &amp; 1 &amp; ~0~ \\\\ 2 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 13 \\\\ 7 \\\\ -5 \\\\ 6 \\end{bmatrix} \\] Here we enter A and b separately to illustrate (below) how to augment a matrix with a vector. A = cbind(c(1,1,-1,2),c(-2,0,1,1),c(1,1,0,1)) b = c(13,7,-5,6) # notice that b is a vector and not a matrix. Let’s echo them back to see what they look like ## [,1] [,2] [,3] ## [1,] 1 -2 1 ## [2,] 1 0 1 ## [3,] -1 1 0 ## [4,] 2 1 1 ## [1] 13 7 -5 6 Now I will augment A with b and call it Ab. The syntax is nice: Ab = cbind(A,b) Ab ## b ## [1,] 1 -2 1 13 ## [2,] 1 0 1 7 ## [3,] -1 1 0 -5 ## [4,] 2 1 1 6 And then row reduce. rref(Ab) ## b ## [1,] 1 0 0 2 ## [2,] 0 1 0 -3 ## [3,] 0 0 1 5 ## [4,] 0 0 0 0 This tells me that there is a unique solution to Ax = b, and it is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -3 \\\\ 5 \\end{bmatrix}. \\] Question 2 Can we find a vector d for which A x = d has no solution? A = cbind(c(1,1,-1,2),c(-2,0,1,1),c(1,1,0,1)) d = c(1,1,-1,2) # notice that b is a vector and not a matrix. Ad = cbind(A,d) rref(Ad) ## d ## [1,] 1 0 0 1 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 0 12.3 Day 5: Mon Sep 13 "],["linear-systems-in-r.html", "Section 13 Linear Systems in R 13.1 Getting started with R 13.2 Building Vectors and Matrices 13.3 Solving a Linear System 13.4 Solving another Linear System", " Section 13 Linear Systems in R 13.1 Getting started with R To use RStudio, you have two choices: Use the cloud version by logging in to Rstudio.macalester.edu. This is the easiest way to use RStudio and works great for our course. You can also download the free desktop version of RStudio. If you plan to go on to take more MSCS classes, especially in statistics and data science, you may want to use the desktop version. Download the desktop version following the instructions here: rstudio.com/products. Now, let’s learn how to use R to solve systems of linear equations! Download this Rmd file. First, we will create vectors and matrices Then we will see how to create an augmented matrix and then apply Gaussian Elimination to obtain is reduced row echelon form. Gaussian elimination is performed by the rref() command. However, this command is not loaded into R by default. So we have have to tell RStudio to use the practical math package, which is known as pracma. So we need to run the following command once at the beginning of our session. require(pracma) 13.2 Building Vectors and Matrices A vector in R is a list of data. The simplest way to create a vector is to use the c() command. The letter ‘c’ is short for ‘combine these values into a vector.’ For example, we can make a vector v for the numbers 1,2,3 as follows: v=c(1,2,3) v ## [1] 1 2 3 Note that we had to ask R to display the value of v. This is because the assignment of v doesn’t echo the value to the console. But can see the value of v in the Environment tab in the upper right panel of RStudio. For example, run this command and then check to see that the value of v gets updated in the environment. v=c(1,2,3,4,5,6) It is interesting to note that c() returns a dimensionless vector. So you can treat a vector c() as either a row or a column when you construct a matrix. For example, suppose that we want to make the matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] We could create this matrix by binding three row vectors: A = rbind(c(1,1,1), c(2,4,8), c(3,9,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 or we could bind three column vectors: A = cbind(c(1,2,3), c(1,4,9), c(1,8,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 13.3 Solving a Linear System Suppose that we want to solve the linear system \\[\\begin{aligned} x + y + z &amp;= 7 \\\\ 2x + 4y + 8z &amp;= 6 \\\\ 3x +9y+27z &amp;=12 \\end{aligned}\\] which has coefficient matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] and target (column) vector \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix}. \\] This is the same matrix A we defined above. Let’s define a vector b and use cbind() to create an augmented matrix which we will name Ab. (We could have just made the full augmented matrix from the start, but using cbind to add a column to a matrix is a skill we will use later in the course!) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 b = c(4,6,12) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 4 ## [2,] 2 4 8 6 ## [3,] 3 9 27 12 Now we use the rref() command to apply Gaussian Elimination to produce the reduced row echelon form. (And remember: we had to load this function into R by using the require(pracma) command above.) rref(Ab) ## b ## [1,] 1 0 0 7 ## [2,] 0 1 0 -4 ## [3,] 0 0 1 1 We conclude that this is a consistent system no free variables. The unique solution is \\[\\begin{align} x&amp;=7\\\\ y&amp;=-4\\\\ z&amp;=1 \\end{align}\\] We can verify that our answer works by multiplying \\(A\\) by one of the solutions above. Matrix multiplication uses the funny operation %*%. A %*% c(7,-4,1) ## [,1] ## [1,] 4 ## [2,] 6 ## [3,] 12 #A %*% c(1,2,3,0,0) Which matches our target \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix} \\] just as we had hoped. 13.4 Solving another Linear System Now let’s find the solution set for the linear system \\[ \\begin{array}{rrrrrcr} x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ 2x_1 &amp; +x_2 &amp; +2x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; 4 \\\\ -x_1 &amp; +x_2 &amp; +x_3 &amp; &amp; &amp; = &amp; 10 \\\\ x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ \\end{array} \\] which corresponds to augmented matrix \\[ \\left[ \\begin{array}{rrrrr|r} 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ 2 &amp; +1 &amp; +2 &amp; -1 &amp; -1 &amp; 4 \\\\ -1 &amp; +1 &amp; +1 &amp; &amp; &amp; 10 \\\\ 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ \\end{array} \\right] \\] This time, let’s just construct the augmented matrix direclty. Then we define the coefficient matrix \\(A\\). Here we use cbind to combine the vectors into the columns of a matrix named \\(A\\). You can use rbind if you want to combine the vectors into the rows of a matrix. Ab = cbind(c(1,2,-1,1),c(0,1,1,0),c(-1,2,1,-1),c(-1,1,0,-1),c(-1,5,0,-1),c(-2,10,4,-2)) Ab ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 -1 -1 -1 -2 ## [2,] 2 1 2 1 5 10 ## [3,] -1 1 1 0 0 4 ## [4,] 1 0 -1 -1 -1 -2 And now let’s row reduce to get RREF. rref(Ab) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 1 1 ## [2,] 0 1 0 -1 -1 2 ## [3,] 0 0 1 1 2 3 ## [4,] 0 0 0 0 0 0 So the set of solutions in parametric form is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ 1 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] and this is a “plane” in \\(\\mathbb{R}^5\\). It is in \\(\\mathbb{R}^5\\) because these vectors have 5 coordinates. It is a plane because it is spanned by two vectors that are not on the same line. "],["linear-dependence.html", "Section 14 Linear Dependence 14.1 Example 1: a 7x9 integer matrix 14.2 A 5 x 6 Numerical Matrix 14.3 Random Matrices", " Section 14 Linear Dependence In this activity, we will explore linear dependence and independence in the context of solving nonhomogeneous \\(A x = b\\) and homogeneous equations \\(A x = 0\\). Download this Rmd file. Remember that we will use the pracma package to get the rref function, so we first load it in: require(&quot;pracma&quot;) 14.1 Example 1: a 7x9 integer matrix Here is a 7 x 9 coeefficient matrix that we will use. These commands define it and echo it back. A = cbind( c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 And here is a vector b that we hope to use in solving A x = b. b = c(382, 51, -321, -314, -86, -170, -153) b ## [1] 382 51 -321 -314 -86 -170 -153 You can augment A with b, and call it Ab, using cbind: Ab = cbind(A,b) Ab ## b ## [1,] 3 5 3 4 0 -4 5 0 37 382 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 51 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 -321 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 -314 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 -86 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 -170 ## [7,] 1 4 5 -3 8 3 -1 0 -24 -153 And row reduce using rref rref(Ab) ## b ## [1,] 1 0 0 0 5 0 0 0 -2 8 ## [2,] 0 1 0 0 -2 0 0 0 2 10 ## [3,] 0 0 1 0 1 0 0 0 -3 -19 ## [4,] 0 0 0 1 -2 0 0 0 3 21 ## [5,] 0 0 0 0 0 1 0 0 0 6 ## [6,] 0 0 0 0 0 0 1 0 6 61 ## [7,] 0 0 0 0 0 0 0 1 0 8 14.1.1 Solution to the nonhomogeneous equations Ax = b Write out the solution to Ax=b in parametric form using the following formatting. You just need to fill in the correct values of the vectors: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 10 \\\\ -19 \\\\ 21 \\\\ 0 \\\\ 6 \\\\ 61 \\\\ 8 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} -5 \\\\ 2 \\\\ -1 \\\\ 2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} 2 \\\\ -2 \\\\ 3 \\\\ -3 \\\\ 0 \\\\ 0 \\\\ -6 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] Describe this solution space (by fixing up this sentence, which is incorrect right now): the set of solutions to A x= b is a plane in \\(\\mathbb{R}^9\\). 14.1.2 Solution to the nonhomogeneous equations Ax = 0 Now, describe the set of solutions to the homogeneous equations A x = 0. Again, you can just edit this: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} p1 \\\\ p2 \\\\ p3 \\\\ p4 \\\\ p5 \\\\ p6 \\\\ p7 \\\\ p8 \\\\ p9 \\end{bmatrix} + s \\begin{bmatrix} u1 \\\\ u2 \\\\ u3 \\\\ u4 \\\\ u5 \\\\ u6 \\\\ u7 \\\\ u8 \\\\ u9 \\end{bmatrix} + t \\begin{bmatrix} v1 \\\\ v2 \\\\ v3 \\\\ v4 \\\\ v5 \\\\ v6 \\\\ v7 \\\\ v8 \\\\ v9 \\end{bmatrix} \\] And describe, in words, the geometric relationship between the solutions to Ax=b and Ax=0. your answer here 14.1.3 Linearly dependent columns The columns of the matrix A are linearly dependent. You can see that in rref(A). rref(A) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Discuss in your group how you see it. Then write out a dependence relation among the columns by filling in numbers for the weights in this equation \\[ 0 = c_1 \\vec{a}_1 + c_2 \\vec{a}_2 + c_3 \\vec{a}_3 + c_4 \\vec{a}_4 + c_5 \\vec{a}_5 + c_6 \\vec{a}_6 + c_7 \\vec{a}_7 + c_8 \\vec{a}_8 + c_9 \\vec{a}_9. \\] Challenge: give a dependency relation that none of the other groups in the class have. This is telling us that there is some redundancy in the matrix A. Remove columns from A to get a new matrix M whose columns are linearly independent. You can do this by removing the appropriate columns from the code below: M = cbind( # you need to edit this matrix c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24) ) M ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 rref(M) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Your matrix should now be square (7x7) with linearly independent columns. R has a build in solve command, solve, that works for matrices of this form (i.e., square with linearly independent columns). You can try it here. First you need to un-comment-out the solve command. I have it commented out right now, because it does not work with the matrix M (above) until you remove its redundancies. # solve(M,b) Now, you should get a unique solution to the equation M x = b, since M has no free variables, and it should be one of the solutions to the original question A x = b. Which solution is it? That is, which of the many solutions to A x = b are you getting here (forw which values of the paramters?). Compare this with trying to use solve on the original equation A x = b with linearly dependent columns. The solve command in the next bit of code is commented out. Delete the comment command and try executing it. # solve(A,b) 14.2 A 5 x 6 Numerical Matrix So far, all of the matrices we’ve worked with in this class have integer values. This is only so that the calulations are nice to do by hand. All of our theory works over the real numbers. Here we will look at a real matrix with numerical values, something you might find when dealing with real-world data. B = cbind( c(0.717, -0.274, 0.365, 0.482, -0.362), c(0.587, -0.545, 0.5, -0.407, -0.597), c(-0.441, 0.886, 0.784, -0.831, -0.594), c(0.923, -0.466, 0.222, 0.867, 0.493), c(-0.42, -0.745, -0.02, -0.44, 0.209), c(0.621, 0.049, -0.134, -0.844, -0.31) ) B ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.717 0.587 -0.441 0.923 -0.420 0.621 ## [2,] -0.274 -0.545 0.886 -0.466 -0.745 0.049 ## [3,] 0.365 0.500 0.784 0.222 -0.020 -0.134 ## [4,] 0.482 -0.407 -0.831 0.867 -0.440 -0.844 ## [5,] -0.362 -0.597 -0.594 0.493 0.209 -0.310 and here is a vector d in \\(\\mathbb{R}^5\\). d = c(5.886, -4.001, 3.701, -6.621, -2.199) d ## [1] 5.886 -4.001 3.701 -6.621 -2.199 Try answering some of these questions: Are the columns of B linearly independent? Do the columns of B span \\(\\mathbb{R}^5\\)? Give the parametric solution to B x = d. What is the geometric form of this solution (e.g., a plane in \\(\\mathbb{R}^4\\))? Remove redundancies from the columns of B to get a new matrix B2 and use solve to solve the equation B2 x = d. Which of the parametric solutions to you get. 14.3 Random Matrices The following code generates a random 5 x 5 matrix. Every time you enter it, it will give you a new matrix. Use this to try to figure out how likely it is that a random square matrix has linearly dependent columns. R1 = matrix(runif(5*5), nrow = 5, ncol = 5) R1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.03097215 0.81873785 0.31886000 0.8577838 0.6777871 ## [2,] 0.09411756 0.56646534 0.87917740 0.5548869 0.2418353 ## [3,] 0.11173743 0.09190297 0.79333956 0.1145338 0.5308948 ## [4,] 0.08451777 0.03407438 0.41343288 0.5441541 0.3626951 ## [5,] 0.26606353 0.50643729 0.01768342 0.6197667 0.8880359 Try the same using the following code that generates a random 5 x 6 matrix. R2 = matrix(runif(5*6), nrow = 5, ncol = 6) R2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.01768851 0.85064881 0.77976904 0.19981949 0.08286483 0.4234481 ## [2,] 0.93845220 0.28683931 0.09332501 0.05383385 0.42411811 0.3442521 ## [3,] 0.24335492 0.53664855 0.13223440 0.36062911 0.64375132 0.1290519 ## [4,] 0.65102447 0.01060806 0.72282088 0.67316290 0.62073013 0.6265008 ## [5,] 0.79636637 0.79179586 0.96331808 0.36644481 0.53535886 0.6308950 Try the same using the following code that generates a random 5 x 4 matrix. R3 = matrix(runif(5*4), nrow = 5, ncol = 4) R3 ## [,1] [,2] [,3] [,4] ## [1,] 0.09396400 0.75485958 0.9681267 0.8042995 ## [2,] 0.86378775 0.42955939 0.1727002 0.2139418 ## [3,] 0.37354738 0.04285461 0.4944575 0.7370346 ## [4,] 0.02036797 0.90686480 0.5999901 0.4829683 ## [5,] 0.23842519 0.57485198 0.3771804 0.5058002 rref(R3) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## [5,] 0 0 0 0 In each of these cases, how likely is it that the columns of the matrix spans all of \\(\\mathbb{R}^4\\)? "],["matrix-multiplication.html", "Section 15 Matrix Multiplication 15.1 Multiplying matrices 15.2 Transpose 15.3 Identity Matrix 15.4 Writing Loops 15.5 Matrix Inverses", " Section 15 Matrix Multiplication 15.1 Multiplying matrices First, let’s define a few matrices. We use a trick here. By putting the assignment in parentheses, it both assigns the matrix and displays it. (A = cbind(c(1,2,3),c(4,5,6),c(1,1,-1))) ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 2 5 1 ## [3,] 3 6 -1 (B = cbind(c(1,-1,1),c(1,1,1),c(0,2,1))) ## [,1] [,2] [,3] ## [1,] 1 1 0 ## [2,] -1 1 2 ## [3,] 1 1 1 (C = cbind(c(2,1,1),c(1,0,1),c(1,-3,1),c(3,2,1))) ## [,1] [,2] [,3] [,4] ## [1,] 2 1 1 3 ## [2,] 1 0 -3 2 ## [3,] 1 1 1 1 Multiply these matrices to get \\(AB\\) using the %*% command. As seen here: A %*% B ## [,1] [,2] [,3] ## [1,] -2 6 9 ## [2,] -2 8 11 ## [3,] -4 8 11 Note that \\(BA\\) is not the same as \\(AB\\). B %*% A ## [,1] [,2] [,3] ## [1,] 3 9 2 ## [2,] 7 13 -2 ## [3,] 6 15 1 That is, matrix multiplication is not commutative. It can be the case that \\(AB\\) equals \\(BA\\) but most if the time these are not equal. We can multiply \\(BC\\) to get B %*% C ## [,1] [,2] [,3] [,4] ## [1,] 3 1 -2 5 ## [2,] 1 1 -2 1 ## [3,] 4 2 -1 6 But \\(CB\\) does not make sense, since \\(C\\) is 3 x 4 and \\(B\\) is 3 x 3. The inner dimensions to not match. R tells us that the matrices are non-conformable. C %*% B ## Error in C %*% B: non-conformable arguments 15.2 Transpose The transpose of a matrix is computed by t(A). For example t(A) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 1 1 -1 t(B) ## [,1] [,2] [,3] ## [1,] 1 -1 1 ## [2,] 1 1 1 ## [3,] 0 2 1 Note that the transpose is order reversing. That is \\[ (AB)^T = B^T A^T. \\] We can see that here ## [,1] [,2] [,3] ## [1,] -2 -2 -4 ## [2,] 6 8 8 ## [3,] 9 11 11 ## [,1] [,2] [,3] ## [1,] -2 -2 -4 ## [2,] 6 8 8 ## [3,] 9 11 11 15.3 Identity Matrix The command diag(n) gives the n x n identity matrix. This is denoted \\(I_n\\). For example, below are \\(I_3\\), \\(I_4\\), and \\(I_5.\\) diag(3) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 diag(4) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 diag(5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 The identity matrix has the same effect as multiplyng by 1. For example, diag(3) %*% A ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 2 5 1 ## [3,] 3 6 -1 A %*% diag(3) ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 2 5 1 ## [3,] 3 6 -1 15.4 Writing Loops Work on problem 3.8 in the homework. In this problem, you will write out a loop in R to apply a matrix over and over again. Here is an example for you to use to compare with. First, define a vector \\(v\\) and a matrix \\(A\\) and multiply \\(Av\\). v = c(10,90) A = cbind(c(.9,.1),c(.5,.5)) A %*% v ## [,1] ## [1,] 54 ## [2,] 46 If we want to appy \\(A\\) to \\(v\\) again and again, we can keep multiplying by \\(A\\): A %*% A %*% v ## [,1] ## [1,] 71.6 ## [2,] 28.4 A %*% A %*% A %*% v ## [,1] ## [1,] 78.64 ## [2,] 21.36 A %*% A %*% A %*% A %*% v ## [,1] ## [1,] 81.456 ## [2,] 18.544 It is better to write a loop to do something like this. The following loop will multiply by A 10 times. Try changing the 10 to 4 (and compare with above). Try changing the 10 to 100. N = 10 # the number of times that we will multiply by A w = c(10,90) # the starting value for (i in 1:N) { # loop as the index i goes from 1 to N w = A %*% w # multiply Aw and replace w with this new vector } w # show the final value of w ## [,1] ## [1,] 83.32564 ## [2,] 16.67436 15.5 Matrix Inverses Give a square matrix ## [,1] [,2] [,3] ## [1,] 10 -6 1 ## [2,] -2 1 0 ## [3,] -7 5 -1 You find its inverse using solve: ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 3 2 ## [3,] 3 8 2 ## [,1] [,2] [,3] ## [1,] 1.000000e+00 0.000000e+00 0 ## [2,] 1.776357e-15 1.000000e+00 0 ## [3,] 3.552714e-15 1.776357e-15 1 Note that if you look closely at the last matrix, it is within round off error of the identity. For example, the number 1.776357 e-15 is 0.000000000000001776357. To make it look more like the identity, you can use the command zapsmall, which converts very small numbers like this to 0. ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 What happens if you try to invert a matrix that is not invertible. For example in the following matrix, the third column is the sum of the first two, so the columns are linearly dependent. If we try to inverti it A = cbind(c(10,-2,-7),c(-6,1,5),c(4,-1,-2)) A ## [,1] [,2] [,3] ## [1,] 10 -6 4 ## [2,] -2 1 -1 ## [3,] -7 5 -2 solve(A) ## Error in solve.default(A): system is computationally singular: reciprocal condition number = 5.84328e-18 We get the error that the matrix is singular. This is another term for non-invertible. "],["eigenvectors.html", "Section 16 Eigenvectors 16.1 Computing Eigenvectors and Eigenvalues 16.2 Diagonalization 16.3 Dynamical Systems 16.4 Northern Spotted Owl 16.5 Fibonacci Numbers", " Section 16 Eigenvectors Download this Rmd file 16.1 Computing Eigenvectors and Eigenvalues To compute eigenvalues and eigenvectors in R we use the eigen command. For example if our matrix is (A = cbind(c(-14,-20,-23),c(13,19,19),c(-2,-2,1))) ## [,1] [,2] [,3] ## [1,] -14 13 -2 ## [2,] -20 19 -2 ## [3,] -23 19 1 Then we compute its eigenvalues and eigenvectors as eigen(A) ## eigen() decomposition ## $values ## [1] 6 3 -3 ## ## $vectors ## [,1] [,2] [,3] ## [1,] -0.2672612 -0.4082483 0.5773503 ## [2,] -0.5345225 -0.4082483 0.5773503 ## [3,] -0.8017837 0.8164966 0.5773503 One thing to notice about the eigenvectors is that they are scaled to have length one (they are unit vectors). So they often do not look like what we expect. Note for example that the first vector above is a multiple of \\((1,2,3)^T\\), the second is a multiple of \\((-1,-1,2)^T\\) and the third is a multiple of \\((1,1,1)^T\\). We can extract the eigenvectors and eigenvalues as follows vals = eigen(A)$values vecs = eigen(A)$vectors Then, for example, we can see if a vector is an eigenvector as follows. Here I will check the first eigenvalue and first eigenvector: lambda1 = vals[1] v1 = vecs[,1] A %*% v1 ## [,1] ## [1,] -1.603567 ## [2,] -3.207135 ## [3,] -4.810702 lambda1 * v1 ## [1] -1.603567 -3.207135 -4.810702 From this, we see that \\(A v_1 = \\lambda_1 v_1\\). Recall that every scalar multiple of an eigenvector is also an eigenvector of that same eigenvalue. The vectors are currently scaled to have length 1. Another useful scaling is to have them sum to 1. You can accomplish this by dividing them by the sum of their entries. For example, v1 = v1/sum(v1) v1 ## [1] 0.1666667 0.3333333 0.5000000 16.2 Diagonalization In class we diagonalized a few matrices. Here we show how to do this in R. Here is the first matrix from the checkpoint question CP-5.3. (A = cbind(c(-5,-6,-7),c(6,7,8),c(-2,-2,-2))) ## [,1] [,2] [,3] ## [1,] -5 6 -2 ## [2,] -6 7 -2 ## [3,] -7 8 -2 vals = eigen(A)$values vals ## [1] 1.000000e+00 -1.000000e+00 -3.766534e-15 vecs = eigen(A)$vectors vecs ## [,1] [,2] [,3] ## [1,] -0.2672612 0.5773503 -0.6666667 ## [2,] -0.5345225 0.5773503 -0.6666667 ## [3,] -0.8017837 0.5773503 -0.3333333 solve(vecs) %*% A %*% vecs ## [,1] [,2] [,3] ## [1,] 1.000000e+00 3.355686e-15 -6.554309e-16 ## [2,] 6.661338e-15 -1.000000e+00 8.881784e-15 ## [3,] 4.272762e-15 -3.076740e-15 1.776357e-15 Here, we are diagonalizing \\(A\\) by multiplying \\(P^{-1} A P = D\\) where \\(P\\) is the matrix of eigenvectors and \\(D\\) is the diagonal matrix of eigenvalues. We can use zapsmall to round or “zap” very small numbers to 0, and it then looks more like what we are expecting. zapsmall(solve(vecs) %*% A %*% vecs) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 -1 0 ## [3,] 0 0 0 Now we diagonalize the second matrix from CP-5.3 You will recall that this one has a repeated eigenvalue (algebraic multiplicity 2), but it has a 2-dimensional eigenspace (geometric multiplicity 2), so it is digonalizable. B = cbind(c(3,-1,2),c(-1,3,2),c(2,2,0)) eigen(B) ## eigen() decomposition ## $values ## [1] 4 4 -2 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.9128709 0.0000000 -0.4082483 ## [2,] -0.1825742 0.8944272 -0.4082483 ## [3,] 0.3651484 0.4472136 0.8164966 vals = eigen(B)$values vecs = eigen(B)$vectors zapsmall(solve(vecs) %*% B %*% vecs) ## [,1] [,2] [,3] ## [1,] 4 0 0 ## [2,] 0 4 0 ## [3,] 0 0 -2 The third matrix from CP-5.3 is not diagonalizable. It has an eigenvalue of algebraic multiplicity 2 and geometric multiplicity 1. Note that it gives the same two eigenvectors for \\(v_2\\) and \\(v_3\\), because the eigenspace \\(E_2\\) is only 1 dimensional. C = cbind(c(3,-1,1),c(2,2,1),c(1,1,2)) eigen(C) ## eigen() decomposition ## $values ## [1] 3 2 2 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.8017837 0.5773503 0.5773503 ## [2,] -0.2672612 -0.5773503 -0.5773503 ## [3,] 0.5345225 0.5773503 0.5773503 16.3 Dynamical Systems Let \\(A\\) be a square \\(n \\times n\\) matrix and let \\(\\mathsf{x}_0 \\in \\mathbb{R}^n\\). A dynamical system is a sequence of vectors \\(\\mathsf{x}_0,\\mathsf{x}_1,\\mathsf{x}_2, \\ldots, \\mathsf{x}_t, \\ldots\\) where \\[ \\mathsf{x}_{t} = A \\mathsf{x}_{t-1} = A^t \\mathsf{x}_0 \\quad \\mbox{for} \\quad t \\geq 1. \\] The sequence \\(\\mathsf{x}_0,\\mathsf{x}_1,\\mathsf{x}_2, \\ldots, \\mathsf{x}_t, \\ldots\\) is called the trajectory for initial vector \\(\\mathsf{x}_0\\). 16.3.1 Plotting dynamical systems Here use some special code, written by Professor Beveridge, that makes helpful plots. get_traj &lt;- function(mat, x0, num) { traj = cbind(x0) num for (i in 1:num) { traj = cbind(traj, mat %*% traj[,dim(traj)[2]]) traj } return(traj) } plot_traj &lt;- function(mat, x0, num) { traj = get_traj(mat,x0,num) points(traj[1,],traj[2,], pch=20, col=rainbow(length(traj))) } trajectory_plot &lt;- function(mat, t=20, datamax=5, plotmax=10, numpoints=10, showEigenspaces=TRUE) { # initialize plot par(pty = &quot;s&quot;) plot(c(0),c(0),type=&quot;n&quot;, xlim=c(-plotmax,plotmax),ylim=c(-plotmax,plotmax), xlab=&#39;x&#39;, ylab=&#39;y&#39;) abline(h=-plotmax:plotmax, v=-plotmax:plotmax, col=&quot;gray&quot;) mygrid &lt;- expand.grid(x=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1), y=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1)) for (i in 1:dim(mygrid)[1]) { plot_traj(mat,c(mygrid[i,1],mygrid[i,2]),t) } if (showEigenspaces) { eigen = eigen(mat) v1 = zapsmall(eigen$vectors[,1]) v2 = zapsmall(eigen$vectors[,2]) if (! class(v1[1]) == &quot;complex&quot;) { if (v1[1] == 0) {abline(v=0)} else { abline(a=0,b=v1[2]/v1[1], col=&quot;blue&quot;)} if (v2[1] == 0) {abline(v=0)} else {abline(a=0,b=v2[2]/v2[1], col=&quot;blue&quot;) } } } } The code will plot multiple trajectories at once. You specify: the matrix A the number of iterations the size of the square where the initial points lie the size of the plot the number of points along the side of the grid 16.3.2 Video Example Let’s start by looking at the example from the video for November 3: \\[ A = \\frac{1}{30} \\begin{bmatrix} 31 &amp; 4 \\\\ 2 &amp; 29 \\end{bmatrix}. \\] A = 1/30 * cbind(c(31,2),c(4,29)) # the population dynamics matrix x0 = c(5,-2) # the inital value N = 8 # iterate N=10 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } X # display the table ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 5 4.9 4.85 4.849 4.8965 4.99249 5.137445 5.3323249 5.5785792 ## [2,] -2 -1.6 -1.22 -0.856 -0.5042 -0.16096 0.177238 0.5138264 0.8521872 A = 1/30 * cbind(c(31,2),c(4,29)) trajectory_plot(A, t=30, datamax=5, plotmax=15, numpoints=5) Compare this with the eigenvalues and eigenvectors. eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] 0.8944272 -0.7071068 ## [2,] 0.4472136 0.7071068 We can see that we have slight expansion along \\([ 2, 1]^{\\top}\\) and slight contraction along \\([-1,1]\\). The long term behavior is an expansion in the direction of \\([2, 1]^{\\top}\\). 16.3.3 CheckPoint Question for today Here is the checkpoint question for today (for which you found a closed-form solution). A = 1/110 * cbind(c(97,-8),c(6,123)) # the population dynamics matrix x0 = c(1,15) # the inital value N = 8 # iterate N=10 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loop from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } X # display the table ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 1.7 2.41 3.137 3.8881 4.67057 5.491921 6.359978 7.282954 ## [2,] 15 16.7 18.55 20.567 22.7695 25.17767 27.813535 30.701177 33.866954 Here is the eigeninformation: eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.2425356 -0.9486833 ## [2,] -0.9701425 -0.3162278 And here is the corresponding plot of the dynamical system: A = 1/110 * cbind(c(97,-8),c(6,123)) trajectory_plot(A, t=10, datamax=15, plotmax=25, numpoints=5) 16.3.4 Discussion Question 1 In class, we looked at a matrix with eigenvalues 1 and 1/2, and we plotted a trajectory starting at \\((8,7)\\) by hand. Its eigensystem is shown here: A = rbind(c(0.4, 0.4), c(-0.15, 1.1)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0 0.5 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 And here is a trajectory plot trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 16.3.5 Discussion Question 2 Here the matrix has the same eigenvectors, but now the eigenvalues are 1.0 and 0.9. It’s a little easier to see when the smaller eigenvalue converges more slowly. A = rbind(c(0.88, 0.08), c(-0.03, 1.02)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 16.3.6 Discussion Question 3 Here the matrix has the same eigenvectors, but now the eigenvalues are 1.1 and 0.9. A = rbind(c(0.86, 0.16), c(-0.06, 1.14)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 16.3.7 Discussion Question 4 Finally, here again the matrix has the same eigenvectors, but now the eigenvalues are 0.99 and 0.9. A = rbind(c(0.882, 0.072), c(-0.027, 1.008)) eigen(A) ## eigen() decomposition ## $values ## [1] 0.99 0.90 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=50, datamax=10, plotmax=15, numpoints=5) 16.4 Northern Spotted Owl This is the opening example in Chapter 5 of the textbook on page 265. It comes from a 1992 study of the northern spotted owl, which was threatened with extinction due to the loss of forest habitat due to logging in the Pacific Northwest. This is currently a story featured in an NPR Podcast called Timber Wars. 16.4.1 The Dynamical System The vector \\[ x_n = \\begin{bmatrix} j_n \\\\ s_n \\\\ a_n \\end{bmatrix} \\] is an age-stage vector in which \\(j_n, s_n\\), and \\(a_n\\) are the number of female owls in the juvenile (up to 1 year), subadult (1-2 year), and adult (over 2 year) age groups in year \\(n\\). The dynamics that take us from one year to the next is given by, the recursive relation \\(x_{n+1} = A x+n\\), where \\(A\\) is the matrix shown here. This is an age-stage matrix model that was published in Conservation Biology. \\[ \\begin{bmatrix} j_{n+1} \\\\ s_{n+1} \\\\ a_{n+1} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 0.33 \\\\ 0.18 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.71 &amp; 0.94 \\end{bmatrix} \\begin{bmatrix} j_n \\\\ s_n \\\\ a_n \\end{bmatrix} \\] If we multiply this system out, we get \\[ \\begin{array} {rcl} j_{n+1} &amp;=&amp; 0.33 a_n \\\\ s_{n+1} &amp;=&amp; 0.18 j_n \\\\ a_{n+1} &amp;=&amp; 0.71 s_n + 0.94 a_n \\end{array} \\] We see that, in this model, 0.33 represents the fertility or fecundity rate. That is, it is the proportion of new juveniles next year to adults this year (the proportion of offspring the adult population is producing). The 0.18 is the survival rate from juvenile to subadult, 0.71 is the survival rate from subadult to adult, and 0.94 proportion of adults that survive from one year to the next. To see the dynamics play out over time, we will start with an original population of owls is distributed into age groups as follows. \\[ x_0 = \\begin{bmatrix} 100 \\\\ 76 \\\\ 502 \\end{bmatrix} \\] We will write a loop to apply the matrix \\(A\\) over and over again. This time we will make a table and store each value in the table. A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(100,76,502) # the inital value N = 10 # iterate N=10 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } X # display the table ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 100 165.66 173.5272 167.3330 164.27953 161.74110 159.0937 156.47643 153.90912 ## [2,] 76 18.00 29.8188 31.2349 30.11993 29.57032 29.1134 28.63687 28.16576 ## [3,] 502 525.84 507.0696 497.8168 490.12454 482.10222 474.1710 466.39127 458.73997 ## [,10] [,11] ## [1,] 151.38419 148.90038 ## [2,] 27.70364 27.24915 ## [3,] 451.21326 443.81005 Having saved the information, we can now plot the data, Note that it appears to support the claim the claim that the owls are threatened with extinction. tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) Let’s run the iteration further. This time, we won’t display the table (gets too big), and we will just show the plot of 100 iterations A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(100,76,502) # the inital value N = 100 # iterate N times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) They do seem to be dying out. 16.4.2 Eigenanalysis Now we check the eigenvectors and eigenvalues to see if they help us understand what is going on. eigen(A) ## eigen() decomposition ## $values ## [1] 0.9835927+0.0000000i -0.0217964+0.2059185i -0.0217964-0.2059185i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.31754239+0i 0.6820937+0.0000000i 0.6820937+0.0000000i ## [2,] 0.05811107+0i -0.0624124-0.5896338i -0.0624124+0.5896338i ## [3,] 0.94646180+0i -0.0450520+0.4256233i -0.0450520-0.4256233i vals = eigen(A)$values vals ## [1] 0.9835927+0.0000000i -0.0217964+0.2059185i -0.0217964-0.2059185i Mod(vals) ## [1] 0.9835927 0.2070688 0.2070688 The first eigenvalue is \\(\\lambda_1 = 0.98\\), and the other two are complex. R always lists the eigenvalues from largest to smallest, so in this case the largets eigenvalue is less than one. That means that in that direction, the population is dying off by 2% each year. If we extract the corresponding eigenvector, and scale it to sum to 1, we get v1 = eigen(A)$vectors[,1] # get the first eigenvector v1 = Re(v1) # drop the imaginary part v1/sum(v1) # scale it to sum to 1 ## [1] 0.24017754 0.04395311 0.71586935 What this is telling us that as the population dies off, it does so in this eigenvector direction with 24.0% of the population being juveniles, 4.4% subadults, and 71.5% adults. The owls were going extinct because of the logging in the Pacific Northwest. Suppose that we make the case that by stopping logging we will increase the survival rate from juvenile to subadult from 0.18 to 0.26 (by improving the habititat the juvinile owls have a better chance of surviving the first year). In this case, the eigenvalues and eigenvectors becomes: A = cbind(c(0,0.26,0),c(0,0,.71),c(0.33,0,0.94)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0008184+0.0000000i -0.0304092+0.2448335i -0.0304092-0.2448335i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.3121152+0i -0.0754384+0.6073766i -0.0754384-0.6073766i ## [2,] 0.0810836+0i 0.6450012+0.0000000i 0.6450012+0.0000000i ## [3,] 0.9465778+0i -0.4436732-0.1119384i -0.4436732+0.1119384i Notice that the largest eigenvalue now becomes 1. And if we iterate, we see that the population does not die off (it even grows slightly). Finally, we go back to the original system, which is dying out, but start with a totally different age distribution. You can see the non-dominant eigevectors dying out quickly at the beginning and the dominant eigenvector, of eigenvalue 0.98, taking over. A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(70,600,8) # the inital value N = 20 # iterate N times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) 16.5 Fibonacci Numbers In this example, we show how we can use eigenvectors and eigenvalues to find closed formulas for the Fibonacci numbers. The Fibonacci numbers are defined recursively by \\(f_0 = 0\\), \\(f_1 = 1\\) and \\(f_{n+1} = f_n + f_{n-1}\\) for \\(n &gt; 1\\). Using this definition, we get the numbers \\[ \\begin{array}{c|ccccccccccccc} n &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; \\cdots \\\\ \\hline f_n &amp; 0 &amp; 1 &amp; 1 &amp; 2 &amp; 3 &amp; 5 &amp; 8 &amp; 13 &amp; 21 &amp; 34 &amp; 55 &amp; 89 &amp; \\cdots \\end{array} \\] To bring some linear algebra into the picture, consider the matrix \\[ F = \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix}, \\] and observe that it takes us from one pair of Fibonacci numbers to the next since it encodes the rule \\(f_{n+1} = f_n + f_{n-1}\\): \\[ \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} f_{n-1} \\\\ f_{n} \\end{bmatrix} = \\begin{bmatrix} f_n \\\\ f_{n-1} + f_n \\end{bmatrix} = \\begin{bmatrix} f_n \\\\ f_{n+1} \\end{bmatrix} \\] Thus, the matrix \\(F\\) generates the next Fibonacci numbers. If we start with \\(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) and apply \\(F\\) over and over again, it will generate the Fibonacci numbers. You can see that here in R: F = cbind(c(0,1),c(1,1)) # the population dynamics matrix x0 = c(0,1) # the inital value N = 11 # iterate N times X = matrix(0,nrow=nrow(F),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is x_1 for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = F %*% X[,i-1] # Apply A to column i-1 and put the value in column i } X ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0 1 1 2 3 5 8 13 21 34 55 89 ## [2,] 1 1 2 3 5 8 13 21 34 55 89 144 We can use the eigenvectors and eigenvalues to find a closed formula for these numbers, just as we did for the owls and rats problem (and others). Step 1 Find the eigenvalues of \\(F\\). \\[ \\det\\begin{bmatrix} - \\lambda &amp; 1 \\\\ 1 &amp; 1 - \\lambda \\end{bmatrix} = (-\\lambda)(1 - \\lambda) - 1= \\lambda^2 - \\lambda - 1. \\] Then we use the quadratic formula to find its roots: \\[ \\lambda = \\frac{1 \\pm \\sqrt{1 + 4}}{2} = \\frac{1 \\pm \\sqrt{5}}{2}. \\] If we let \\[ \\phi = \\frac{1 + \\sqrt{5}}{2} \\quad\\hbox{and}\\quad \\overline{\\phi} = \\frac{1 - \\sqrt{5}}{2}, \\] then \\(\\phi\\) is the golden ratio and \\(\\overline{\\phi}\\) is its conjugate. Step 2 Find the eigenvectors. Because the eigenvalues are a bit complicated this becomes a little tricky. First we will notice some things about the golden ratio that help us out: \\[ \\phi + \\overline{\\phi} = \\frac{1 + \\sqrt{5}}{2} + \\frac{1 - \\sqrt{5}}{2} = 1, \\qquad\\hbox{so} \\quad 1 - \\phi = \\overline{\\phi}. \\] \\[ \\phi - \\overline{\\phi} = \\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2} = \\sqrt{5}, \\qquad\\hbox{so} \\quad 1 - \\phi = \\overline{\\phi}. \\] \\[ \\phi \\overline{\\phi} = \\frac{1 + \\sqrt{5}}{2} \\frac{1 - \\sqrt{5}}{2} = \\frac{1 - 5}{4} = -1. \\] \\[ \\phi^2 - \\phi - 1 = 0 \\qquad\\hbox{and}\\qquad \\overline{\\phi}^2 - \\overline{\\phi} - 1 = 0 \\] This last one comes from the fact that both \\(\\phi\\) and \\(\\overline{\\phi}\\) are zeros of \\(\\lambda^2 - \\lambda -1 = 0\\). Now we find the eigenvectors and use some of these properties to help us do the algebra: \\[ \\begin{bmatrix} - \\phi &amp; 1 \\\\ 1 &amp; 1 - \\phi \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 1 - \\phi \\\\ - \\phi &amp; 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 1 - \\phi \\\\ 0 &amp; \\phi - \\phi^2 + 1 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\overline{\\phi} \\\\ 0 &amp; 0 \\end{bmatrix} \\quad\\Rightarrow\\quad v_1 = \\begin{bmatrix} -\\overline{\\phi} \\\\ 1 \\end{bmatrix}. \\] \\[ \\begin{bmatrix} - \\overline{\\phi} &amp; 1 \\\\ 1 &amp; 1 - \\overline{\\phi} \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 1 - \\overline{\\phi} \\\\ - \\overline{\\phi} &amp; 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 1 - \\overline{\\phi} \\\\ 0 &amp; \\overline{\\phi} - \\overline{\\phi}^2 + 1 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\phi \\\\ 0 &amp; 0 \\end{bmatrix} \\quad\\Rightarrow\\quad v_2 = \\begin{bmatrix} -\\phi \\\\ 1 \\end{bmatrix}. \\] If we rescale \\(v_1\\) by \\(\\phi\\) and \\(v_2\\) by \\(\\overline{\\phi}\\) then we get some even nicer vectors: \\[ w_1 = \\phi \\begin{bmatrix} -\\overline{\\phi} \\\\ 1 \\end{bmatrix}= \\begin{bmatrix} -\\phi\\overline{\\phi} \\\\ \\phi \\end{bmatrix}= \\begin{bmatrix} 1 \\\\ \\phi \\end{bmatrix}, \\qquad w_2 = \\overline{\\phi} \\begin{bmatrix} -\\phi \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\overline{\\phi} \\phi \\\\ \\overline{\\phi} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{\\phi} \\end{bmatrix} \\] Step 3 Express the initial vector in terms of the eigenvectors. You can augment and row reduce or use the inverse to find the coefficients. This uses the fact (from above) that \\(\\phi - \\overline{\\phi} = \\sqrt{15}.\\) \\[ \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} 1 \\\\ \\phi \\end{bmatrix} -\\frac{1}{\\sqrt{5}} \\begin{bmatrix} 1 \\\\ \\overline{\\phi} \\end{bmatrix} \\] Step 4 Apply \\(F^n\\), \\[ \\begin{bmatrix} f_{n} \\\\ f_{n+1} \\end{bmatrix} = F^n \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = F^n \\left(\\frac{1}{\\sqrt{5}} \\begin{bmatrix} 1 \\\\ \\phi \\end{bmatrix} -\\frac{1}{\\sqrt{5}} \\begin{bmatrix} 1 \\\\ \\overline{\\phi} \\end{bmatrix}\\right) = \\frac{1}{\\sqrt{5}} F^n \\begin{bmatrix} 1 \\\\ \\phi \\end{bmatrix} -\\frac{1}{\\sqrt{5}} F^n \\begin{bmatrix} 1 \\\\ \\overline{\\phi} \\end{bmatrix} \\\\ = \\frac{1}{\\sqrt{5}} \\phi^n \\begin{bmatrix} 1 \\\\ \\phi \\end{bmatrix} -\\frac{1}{\\sqrt{5}} \\overline{\\phi}^n \\begin{bmatrix} 1 \\\\ \\overline{\\phi} \\end{bmatrix} \\] Step 5 Extract a formula for \\(f_n\\) from the first coordinate \\[ f_n = \\frac{1}{\\sqrt{5}} \\phi^n - \\frac{1}{\\sqrt{5}} \\overline{\\phi}^n. \\] This is the very famous Binet’s Formula. The cool thing is that it comes from eigenvalues and eigenvectors. It’s all linear algebra! Step 5 Try it. Let’s see if it works. Here is the golden ratio and its conjugate: g = (1 + sqrt(5))/2 gb = (1 - sqrt(5))/2 c(g,gb) ## [1] 1.618034 -0.618034 Here is the 10th Fibonacci number using the formula: n = 10 1/sqrt(5)*g^n - 1/sqrt(5)*gb^n ## [1] 55 And is the 58th Fibonacci number n = 58 1/sqrt(5)*g^n - 1/sqrt(5)*gb^n ## [1] 591286729879 "],["complex-eigenvalues.html", "Section 17 Complex Eigenvalues 17.1 Motivating Example 17.2 Rotation-Dilation Matrices 17.3 General 2x2 Matrices with Complex Eigenvalues 17.4 Examples from class 17.5 The checkpoint question", " Section 17 Complex Eigenvalues Download this Rmd file Now we will explore what happens if the matrix has complex eigenvalues. 17.1 Motivating Example Recall the rental car problem that has the following matrix and eigenvectors. ## [,1] [,2] [,3] ## [1,] 0.85 0.3 0.35 ## [2,] 0.09 0.6 0.05 ## [3,] 0.06 0.1 0.60 ## eigen() decomposition ## $values ## [1] 1.000+0.000000i 0.525+0.037081i 0.525-0.037081i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.9497414+0i 0.6201737+0.0000000i 0.6201737+0.0000000i ## [2,] 0.2389672+0i -0.3100868-0.4599331i -0.3100868+0.4599331i ## [3,] 0.2022030+0i -0.3100868+0.4599331i -0.3100868-0.4599331i It has complex eigenvalues and eigenvectors. \\[ \\begin{bmatrix} .85 &amp; .30 &amp; .35 \\\\ .09 &amp; .60 &amp; .05 \\\\ .06&amp; .10 &amp; .60 \\end{bmatrix} \\qquad\\qquad \\begin{array}{ccccc} \\lambda_1 = 1.0, &amp;&amp; \\lambda_2 = .525+.037i, &amp;&amp; \\lambda_3 =.525-.037i \\\\ \\begin{bmatrix} .95 \\\\ .24 \\\\ .20 \\end{bmatrix} &amp;&amp; \\begin{bmatrix} ~~.62 + .00i \\\\ -.31 - .46i \\\\ -.31+ .46i\\end{bmatrix} &amp;&amp; \\begin{bmatrix} ~~.62 - .00i \\\\ -.31 + .46i \\\\ -.31 - .46i\\end{bmatrix} \\end{array} \\] We can diagonalize as usual, but it gives us a diagonal matrix with complex eigenvalues. \\[ \\begin{bmatrix} .85 &amp; .30 &amp; .35 \\\\ .09 &amp; .60 &amp; .05 \\\\ .06&amp; .10 &amp; .60 \\end{bmatrix} = \\underbrace{\\begin{bmatrix} .95 &amp; .62\\!+\\!.00i &amp; .62\\!-\\!.00i \\\\ .24 &amp; -.31\\!-\\!.46i &amp; -.31\\!+\\!.46i \\\\ .20 &amp; -.31\\!+\\!46i &amp; -.31\\!-\\!.46i \\end{bmatrix}}_P \\begin{bmatrix} 1.0 &amp; 0 &amp; 0\\\\ 0 &amp; .35\\!-\\!.04i &amp; 0 \\\\ 0 &amp; 0 &amp; .35\\!+\\!.04i \\end{bmatrix} P^{-1}. \\] This diagonalization isn’t as usful to us though, since it requires complex eigenvectors that are not in \\(\\mathbb{R}^n\\). Our original problem involved real numbers, so we can’t extract the information we need from this. Instead, since our eigenvectors are of the form \\(v = u \\pm w i\\). We use the real part \\(u\\) and the imaginary part \\(w\\) in our eigenbasis instead. ## v w u ## v 1 0.000000 0.000000 ## w 0 0.525000 -0.037081 ## u 0 0.037081 0.525000 This matrix is not diagonal, but it has a nice form. Notice that the real and complex parts of the eigenvalues are in the 2x2 matrix inside. \\[ \\lambda = .525 \\pm .037i. \\hskip6in \\] \\[ \\begin{bmatrix} .85 &amp; .30 &amp; .35 \\\\ .09 &amp; .60 &amp; .05 \\\\ .06&amp; .10 &amp; .60 \\end{bmatrix} = \\underbrace{\\begin{bmatrix} .95 &amp; .00 &amp; .62 \\\\ .24 &amp; -.46 &amp; -.31 \\\\ .20 &amp; .46 &amp; -.31 \\end{bmatrix}}_P \\begin{bmatrix} 1.0 &amp; 0 &amp; 0\\\\ 0 &amp; .525 &amp; -.037 \\\\ 0 &amp; .037 &amp; .525 \\end{bmatrix} \\underbrace{\\begin{bmatrix} .72 &amp; .72 &amp; .72 \\\\ .03 &amp; -1.06 &amp; 1.12 \\\\ .51 &amp; -1.10 &amp; -1.10 \\end{bmatrix}}_{P^{-1}} \\] Next we explore what 2x2 matrices of the following form do: \\[ \\begin{bmatrix} .525 &amp; -.037 \\\\ .037 &amp; .525 \\end{bmatrix} =\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix}, \\qquad \\lambda = a\\pm b i. \\] 17.2 Rotation-Dilation Matrices First we explore a special case of 2x2 matrices with complex eigenvalues of the following form: \\[ R=\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\\\ \\end{bmatrix} \\] As we see in the image below, this matrix rotates by angle of \\(\\theta\\) and expands (dilates) or contracts by a factor of \\(r\\) where \\[ \\begin{align} \\theta &amp;= \\arctan(b/a) \\\\ r &amp;= \\sqrt{a^2 + b^2} \\end{align} \\] Furthermore the eigenvalues of this matrix are the complex values \\[ \\lambda_1 = a + b i \\qquad \\lambda_2 = a - b i \\] where \\(i = \\sqrt{-1}\\). These eigenvalues are conjugate pairs and are often written as \\(\\lambda = a \\pm b i\\). They come from applying the quadratic formula to the characteristic polynomial and getting a negative discriminant under the square root. It is important to note that both the angle of rotation \\(\\theta\\) and the dilation factor \\(r\\) are contained in the eigenvalues. The fact that these are the eigenvalues is derived in the video. We will illustrate it here in three examples. 17.2.1 Example 1 Our first example has \\(a = .9\\) and \\(b = .2\\). ## [,1] [,2] ## [1,] 0.9 -0.2 ## [2,] 0.2 0.9 We look at its eigenvectors and eigenvalues and see that \\(\\lambda = .9 \\pm .2 i\\): eigen(A) ## eigen() decomposition ## $values ## [1] 0.9+0.2i 0.9-0.2i ## ## $vectors ## [,1] [,2] ## [1,] 0.7071068+0.0000000i 0.7071068+0.0000000i ## [2,] 0.0000000-0.7071068i 0.0000000+0.7071068i Notics that the eigenvectors also come in conjugate pairs, with a real and a complex part. This always happens. \\[ \\vec{\\mathsf{v}} = \\begin{bmatrix}0.707 \\\\ 0.000 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ .707 \\end{bmatrix} i \\] Now, let’s find the angle of rotation. We will use the Arg command which finds the angle (in radians) of a complex number. We also convert it to degrees here. vals = eigen(A)$values v1 = vals[1] Arg(v1) # gives the argument, or angle, of a complex number (in radians) ## [1] 0.2186689 Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 12.52881 For good measure, we can compare with using the arctan function. atan(.2/.9) ## [1] 0.2186689 Next we find the dilation/contraction factor. We can do so using the Mod command, which finds the “modulus” or absolute value or length of a complex number. Mod(v1) # gives the length of a complex number ## [1] 0.9219544 And, again for good measure, we compare with using the Pythagorean theorem: sqrt(.9^2 + .2^2) ## [1] 0.9219544 Now, we observe the trajectory of a single point \\((0,1)^T\\) under this matrix. In this picture you can see that it is contracting and rotating by 12.5 degrees. Note that 360/12.5 is about 29, and it takes 29 applications to go once around the circle. You can count them in the plot below. Furthermore, \\((0.9219544)^29 =0.095\\) and after 29 applications the vector is about 1/10 of its original length. We can also view this by looking at a plot of the x and y coordinates over time as the point (x,y) circles around in the xy-plane. Key point: complex eigenvalues lead to oscillating values of the individual coordinates We can also use trajectory_plot from [Dynamical Systems in 2D] to watch what happens to a whole grid of points under this transformation. It is beautiful! trajectory_plot(A, t=30, datamax=5, plotmax=5, numpoints=10) 17.2.2 Example 2 Here is a second example of a rotation-dilation matrix, this time with \\(a = .96\\) and \\(b = .28\\). (A = cbind(c(.96,.28),c(-.28,.96))) ## [,1] [,2] ## [1,] 0.96 -0.28 ## [2,] 0.28 0.96 eigen(A) ## eigen() decomposition ## $values ## [1] 0.96+0.28i 0.96-0.28i ## ## $vectors ## [,1] [,2] ## [1,] 0.7071068+0.0000000i 0.7071068+0.0000000i ## [2,] 0.0000000-0.7071068i 0.0000000+0.7071068i We check the angle of rotation and the dilation factor vals = eigen(A)$values v1 = vals[1] Arg(v1) # gives the argument, or angle, of a complex number (in radians) ## [1] 0.2837941 Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 16.2602 Mod(v1) # gives the length of a complex number ## [1] 1 Notice that the dilation factor is 1, which is seen in the following plots. Here are 21 iterations: And here are 200 iterations And a trajectory plot: 17.2.3 Example 3 A third and final example. (A = cbind(c(.99,.16),c(-.16,.99))) ## [,1] [,2] ## [1,] 0.99 -0.16 ## [2,] 0.16 0.99 eigen(A) ## eigen() decomposition ## $values ## [1] 0.99+0.16i 0.99-0.16i ## ## $vectors ## [,1] [,2] ## [1,] 0.0000000-0.7071068i 0.0000000+0.7071068i ## [2,] -0.7071068+0.0000000i -0.7071068+0.0000000i vals = eigen(A)$values v1 = vals[1] Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 9.180542 Mod(v1) # gives the length of a complex number ## [1] 1.002846 We see that the dilation factor is \\(r = 1.0028\\) and the angle of rotation is \\(9.18\\) degrees. Here are 100 iterations. And a trajectory plot: 17.3 General 2x2 Matrices with Complex Eigenvalues Now suppose we have a 2x2 matrix with complex eigenvalues \\(\\lambda = a \\pm b i\\) and complex eigenvectors \\(\\mathsf{v} = \\mathsf{u } \\pm \\mathsf{w} i\\) that is not in rotation-dilation form. Here is an example: \\[ A = \\begin{bmatrix} 1.19 &amp; -0.38 \\\\ 0.29 &amp; 0.78 \\end{bmatrix} \\] It has eigenvalues and eigenvectors \\[ \\lambda = 0.985 \\pm 0.261 i \\qquad \\mathsf{v} = \\begin{bmatrix} 0.753 \\\\ 0.406 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ -0.517 \\end{bmatrix} i \\] as seen here: (A = cbind(c(1.19,0.29),c(-.38,.78))) ## [,1] [,2] ## [1,] 1.19 -0.38 ## [2,] 0.29 0.78 eigen(A) ## eigen() decomposition ## $values ## [1] 0.985+0.2611034i 0.985-0.2611034i ## ## $vectors ## [,1] [,2] ## [1,] 0.7531030+0.0000000i 0.7531030+0.0000000i ## [2,] 0.4062793-0.5174679i 0.4062793+0.5174679i The angle of rotation and factor of dilation are \\(\\theta = 14.8\\) degrees and \\(r = 1.019\\) as we see from these computations: ## [1] 14.84649 ## [1] 1.019019 A trajectory plot shows us that it is still rotating by 14.8 degrees and dilating by 1.019, but it is taking more of an elliptical pattern. We can also view this by looking at a plot of the x and y coordinates over time as the point (x,y) circles around in the xy-plane. Key point: complex eigenvalues lead to oscillating values of the individual coordinates To see precisely what happens, we change to basis \\(\\{\\mathsf{w}, \\mathsf{u}\\}\\) where \\(\\mathsf{w}\\) and \\(\\mathsf{u}\\) are the imaginary and real parts of the eigenvector \\(\\mathsf{v} =\\mathsf{u} + \\mathsf{w} i\\). In this case the eigenvalues and eigenvectors are \\[ \\lambda = 0.985 \\pm 0.261 i \\qquad \\mathsf{v} = \\begin{bmatrix} 0.753 \\\\ 0.406 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ -0.517 \\end{bmatrix} i \\] So if we make the change of basis matrix \\(P = [\\mathsf{u},\\mathsf{w}]\\) \\[ P = \\begin{bmatrix} 0.000 &amp; 0.753 \\\\ -0.517 &amp; 0.406 \\end{bmatrix} \\] then we can factor \\(A\\) as \\[ A = \\begin{bmatrix} 1.19 &amp; -0.38 \\\\ 0.29 &amp; 0.78 \\end{bmatrix} = \\underbrace{\\begin{bmatrix}0.000 &amp; 0.753 \\\\-0.517 &amp; 0.406 \\end{bmatrix}}_P \\underbrace{\\begin{bmatrix} 0.985 &amp; - 0.261 \\\\ 0.261 &amp; 0.985 \\end{bmatrix}}_R \\underbrace{\\begin{bmatrix}0.000 &amp; 0.753 \\\\-0.517 &amp; 0.406 \\end{bmatrix}^{-1}}_{P^{-1}} \\] We have not diagonalized \\(A\\). Rather we have rotation-dilationalized (made up term) the matrix \\(A\\). At is core \\(A\\) is a rotation-dilation matrix whose angle and dilation factor come from the eigenvalue. The matrix \\(P\\) is a change of basis matrix. It is rotating and dilating in this new coordinate system, which are the vectors in the plot above. If we multiply the other way, we get \\[ P^{-1} A P = \\begin{bmatrix} 0.985 &amp; - 0.261 \\\\ 0.261 &amp; 0.985 \\end{bmatrix} = R \\] Which we can see using R ## [,1] [,2] ## [1,] 0.0000000 0.7531030 ## [2,] -0.5174679 0.4062793 ## [,1] [,2] ## [1,] 0.9850000 -0.2611034 ## [2,] 0.2611034 0.9850000 17.4 Examples from class 17.4.1 (A = cbind(c(.9,.2),c(-.2,.9))) ## [,1] [,2] ## [1,] 0.9 -0.2 ## [2,] 0.2 0.9 (vals=eigen(A)$values) ## [1] 0.9+0.2i 0.9-0.2i Mod(vals) ## [1] 0.9219544 0.9219544 Arg(vals)/(2*pi)*360 ## [1] 12.52881 -12.52881 17.4.2 (A = cbind(c(1.19,.29),c(-.38,.78))) ## [,1] [,2] ## [1,] 1.19 -0.38 ## [2,] 0.29 0.78 (vals=eigen(A)$values) ## [1] 0.985+0.2611034i 0.985-0.2611034i Mod(vals) ## [1] 1.019019 1.019019 Arg(vals)/(2*pi)*360 ## [1] 14.84649 -14.84649 17.5 The checkpoint question (A = cbind(c(1,1),c(-1,2))) ## [,1] [,2] ## [1,] 1 -1 ## [2,] 1 2 eigen(A) ## eigen() decomposition ## $values ## [1] 1.5+0.866025i 1.5-0.866025i ## ## $vectors ## [,1] [,2] ## [1,] -0.3535534+0.6123724i -0.3535534-0.6123724i ## [2,] 0.7071068+0.0000000i 0.7071068+0.0000000i vals=eigen(A)$values Mod(vals) ## [1] 1.732051 1.732051 Arg(vals)/(2*pi)*360 ## [1] 30 -30 "],["network-centralities.html", "Section 18 Network Centralities 18.1 Graphs and Networks 18.2 Degree Centrality 18.3 Gould’s Index", " Section 18 Network Centralities Download this Rmd file. You can cut and paste it into an Rmd file. In this example, we will use a package called igraph. To install it, you need to go to the packages window (bottom right), choose install, and search for and install igraph from the packages window. library(igraph) 18.1 Graphs and Networks Graphs consists of vertices and the edges between them. These edges are used to model connections in a wide array of applications, including but not limited to, physical, biological, social, and information networks. 18.1.1 Adjacency Matrices Matrices are used to represent graphs and networks in a very direct way: we place a 1 in position \\((i,j)\\) of the adjacency matrix \\(A\\) of the graph \\(G\\), if there is an edge from vertex \\(i\\) to vertex \\(j\\) in \\(G\\). Here is the adjacency matrix we will use today. A = rbind( c(0,1,0,1,0,0,0,0,1,0,0,0),c(1,0,1,1,1,0,1,0,0,0,0,0),c(0,1,0,0,1,0,0,0,0,0,0,0), c(1,1,0,0,0,1,0,1,0,0,0,0), c(0,1,1,0,0,0,1,1,0,0,0,1), c(0,0,0,1,0,0,1,0,0,0,0,0), c(0,1,0,0,1,1,0,1,0,0,0,0), c(0,0,0,1,1,0,1,0,0,1,1,0), c(1,0,0,0,0,0,0,0,0,0,0,0), c(0,0,0,0,0,0,0,1,0,0,0,0), c(0,0,0,0,0,0,0,1,0,0,0,0), c(0,0,0,0,1,0,0,0,0,0,0,0)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0 1 0 1 0 0 0 0 1 0 0 0 ## [2,] 1 0 1 1 1 0 1 0 0 0 0 0 ## [3,] 0 1 0 0 1 0 0 0 0 0 0 0 ## [4,] 1 1 0 0 0 1 0 1 0 0 0 0 ## [5,] 0 1 1 0 0 0 1 1 0 0 0 1 ## [6,] 0 0 0 1 0 0 1 0 0 0 0 0 ## [7,] 0 1 0 0 1 1 0 1 0 0 0 0 ## [8,] 0 0 0 1 1 0 1 0 0 1 1 0 ## [9,] 1 0 0 0 0 0 0 0 0 0 0 0 ## [10,] 0 0 0 0 0 0 0 1 0 0 0 0 ## [11,] 0 0 0 0 0 0 0 1 0 0 0 0 ## [12,] 0 0 0 0 1 0 0 0 0 0 0 0 We make a graph from the adjacency matrix using the code below. Observe that there is an edge from vertex \\(i\\) to vertex \\(j\\) if and only if there is a 1 in position \\((i,j)\\) in the matrix. g=graph_from_adjacency_matrix(A,mode=&#39;undirected&#39;) plot(g, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) This network is the route map of a small airline. Here we will add the locations of the vertices, add vertex labels, and change the vertex size: locations = rbind(c(20,0),c(-10,0),c(11,7),c(10,15),c(3,12),c(25,10),c(-10,10),c(-12,15),c(20,6),c(-15,12),c(12,4),c(25,13)) airports = c(&quot;ATL&quot;,&quot;LAX&quot;,&quot;ORD&quot;,&quot;MSP&quot;,&quot;DEN&quot;,&quot;JFK&quot;,&quot;SFO&quot;,&quot;SEA&quot;,&quot;PHL&quot;,&quot;PDX&quot;,&quot;MDW&quot;,&quot;LGA&quot;) V(g)$label = airports plot(g,vertex.size=20, layout=locations, vertex.label.cex=0.85, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 18.2 Degree Centrality If we are considering placing an office in one of our airport locations, we may want to chose the most central hub for that office. It turns out that there are many interesting centrality measures for networks. We will talk about two of them today. The simplest measure centrality is the degree of the vertex, or the number of edges connected to that vertex. We calculate the degree centralities from the adjacency matrix as follows. Make a vector \\(\\mathsf{v}\\) of all 1’s and multiply \\(\\mathsf{d} = A\\mathsf{v}\\) to get the vector of degrees. Divide the vector \\(\\mathsf{p}\\) by the sum of its entries to get the degree proportions. The result is a normalized vector \\(\\mathsf{p}\\) whose entries sum to 1. Each entry of vector \\(\\mathsf{p}\\) represents to proportion of edges incident with the corresponding vertex. v=rep(1,nrow(A)) # all 1s vector d = A %*% v # degrees p=d/sum(d) # proportion of degrees Z = cbind(d,p) # show d and p together side-by-side in a matrix rownames(Z) = airports colnames(Z) = c(&quot;deg&quot;,&quot;proportion&quot;) ii=order(d,decreasing=TRUE) # sort from largest to smallest Z = Z[ii,] Z ## deg proportion ## LAX 5 0.14705882 ## DEN 5 0.14705882 ## SEA 5 0.14705882 ## MSP 4 0.11764706 ## SFO 4 0.11764706 ## ATL 3 0.08823529 ## ORD 2 0.05882353 ## JFK 2 0.05882353 ## PHL 1 0.02941176 ## PDX 1 0.02941176 ## MDW 1 0.02941176 ## LGA 1 0.02941176 Now we create a data visualization that plots the size of each vertex according to the vector \\(p\\). The larger vertices have more edges connected to them. plot(g, layout=locations, vertex.size=250*p,vertex.label.cex=0.65, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 18.3 Gould’s Index Gould’s Index is a measure of centrality that uses the dominant eigenvector of a matrix. It was introduced by geographer P. R. Gould in 1967 to analyze the geographical features on maps. We will build up Gould’s Index step-by-step so that we can understand what it measures. 18.3.1 Step 1: Add Layovers The first step is typically to add the identity matrix to the adjancency matrix \\(A\\) to get a new matrix \\[ B = A + I. \\] The \\(n \\times n\\) identity matrix in R is obtained by using diag(n). Adding the identity gives a connection from a vertex to itself. This loop edge corresponds to staying at the current city during a layover. (B = A + diag(nrow(A))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 1 1 0 1 0 0 0 0 1 0 0 0 ## [2,] 1 1 1 1 1 0 1 0 0 0 0 0 ## [3,] 0 1 1 0 1 0 0 0 0 0 0 0 ## [4,] 1 1 0 1 0 1 0 1 0 0 0 0 ## [5,] 0 1 1 0 1 0 1 1 0 0 0 1 ## [6,] 0 0 0 1 0 1 1 0 0 0 0 0 ## [7,] 0 1 0 0 1 1 1 1 0 0 0 0 ## [8,] 0 0 0 1 1 0 1 1 0 1 1 0 ## [9,] 1 0 0 0 0 0 0 0 1 0 0 0 ## [10,] 0 0 0 0 0 0 0 1 0 1 0 0 ## [11,] 0 0 0 0 0 0 0 1 0 0 1 0 ## [12,] 0 0 0 0 1 0 0 0 0 0 0 1 g2=graph_from_adjacency_matrix(B,mode=&#39;undirected&#39;) V(g2)$label = airports plot(g2, layout=locations, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 18.3.2 Step 2: Dynamical System Starting with the all 1’s vector \\(\\mathsf{v}_0\\) create the dynamical system \\[ \\mathsf{v}_0, \\quad \\mathsf{v}_1 = B \\mathsf{v}_0, \\quad \\mathsf{v}_2 = B \\mathsf{v}_1, \\quad \\mathsf{v}_3 = B \\mathsf{v}_2, \\quad \\ldots , \\quad \\mathsf{v}_n = B \\mathsf{v}_{n-1}. \\] Here we calculate \\(\\mathsf{v}_1, \\ldots, \\mathsf{v}_{10}\\) using a loop: N = 10 X = matrix(0,nrow=nrow(B),ncol=N+1) # make a a table of 0s X[,1] = rep(1,nrow(B)) # put v0 in first column for (i in 1:N) { # loop N times X[,i+1] = B %*% X[,i] # apply B to the ith column and make it the (i+1)st column } rownames(X) = airports colnames(X) = 0:10 X ## 0 1 2 3 4 5 6 7 8 9 10 ## ATL 1 4 17 76 347 1603 7442 34638 161411 752642 3510616 ## LAX 1 6 29 139 650 3044 14211 66352 309652 1445058 6743119 ## ORD 1 3 15 72 343 1614 7567 35389 165336 771972 3603377 ## MSP 1 5 24 109 507 2349 10936 50930 237450 1107376 5165837 ## DEN 1 6 28 132 621 2909 13611 63595 296984 1386347 6470458 ## JFK 1 3 13 63 294 1377 6418 29939 139617 651292 3038392 ## SFO 1 5 26 122 576 2692 12585 58748 274225 1279724 5971890 ## SEA 1 6 26 120 551 2563 11923 55591 259246 1209469 5642972 ## PHL 1 2 6 23 99 446 2049 9491 44129 205540 958182 ## PDX 1 2 8 34 154 705 3268 15191 70782 330028 1539497 ## MDW 1 2 8 34 154 705 3268 15191 70782 330028 1539497 ## LGA 1 2 8 36 168 789 3698 17309 80904 377888 1764235 Discuss with your group: Each of the entries of the vector \\(\\mathsf{v}_{t}\\) in the columns of the table above corresponds to “a trip of length \\(t\\).” What kinds of trips do the entries of \\(\\mathsf{v}_{t}\\) count? To figure this out, compare the table of vectors with the picture of the network with layovers. Figure out the maning of the \\(t=1\\) column, then the \\(t = 2\\) column, and so on. Why does the rule \\(\\mathsf{v}_t = B \\mathsf{v}_{t-1}\\) leads to this result? Normalize: These numbers get big fast! Let’s normalize by dividing by the sum each time. By doing this, the vectors will always be proportions which sum to 1. See the table below. What do the entries in this table tell us? N = 10 X = matrix(0,nrow=nrow(B),ncol=N+1) X[,1] = rep(1,nrow(B)) for (i in 2:(N+1)) { X[,i] = B %*% X[,i-1] X[,i] = X[,i]/sum(X[,i]) } rownames(X) = airports colnames(X) = 0:10 X ## 0 1 2 3 4 5 6 7 ## ATL 1 0.08695652 0.08173077 0.07916667 0.07773297 0.07708213 0.07674064 0.07657108 ## LAX 1 0.13043478 0.13942308 0.14479167 0.14560932 0.14637430 0.14654141 0.14667834 ## ORD 1 0.06521739 0.07211538 0.07500000 0.07683692 0.07761108 0.07802962 0.07823125 ## MSP 1 0.10869565 0.11538462 0.11354167 0.11357527 0.11295441 0.11277017 0.11258632 ## DEN 1 0.13043478 0.13461538 0.13750000 0.13911290 0.13988267 0.14035431 0.14058369 ## JFK 1 0.06521739 0.06250000 0.06562500 0.06586022 0.06621466 0.06618132 0.06618343 ## SFO 1 0.10869565 0.12500000 0.12708333 0.12903226 0.12944797 0.12977438 0.12986887 ## SEA 1 0.13043478 0.12500000 0.12500000 0.12343190 0.12324485 0.12294795 0.12288997 ## PHL 1 0.04347826 0.02884615 0.02395833 0.02217742 0.02144643 0.02112894 0.02098089 ## PDX 1 0.04347826 0.03846154 0.03541667 0.03449821 0.03390075 0.03369906 0.03358136 ## MDW 1 0.04347826 0.03846154 0.03541667 0.03449821 0.03390075 0.03369906 0.03358136 ## LGA 1 0.04347826 0.03846154 0.03750000 0.03763441 0.03793999 0.03813315 0.03826343 ## 8 9 10 ## ATL 0.07647933 0.07643081 0.07640399 ## LAX 0.14671848 0.14674567 0.14675521 ## ORD 0.07833906 0.07839377 0.07842281 ## MSP 0.11250792 0.11245405 0.11242772 ## DEN 0.14071617 0.14078356 0.14082110 ## JFK 0.06615295 0.06613871 0.06612665 ## SFO 0.12993256 0.12995600 0.12997042 ## SEA 0.12283525 0.12282160 0.12281194 ## PHL 0.02090908 0.02087259 0.02085358 ## PDX 0.03353774 0.03351435 0.03350515 ## MDW 0.03353774 0.03351435 0.03350515 ## LGA 0.03833372 0.03837453 0.03839628 18.3.3 Step 4: Eigen-analysis We see that the vectors are converging to a common direction, and we know that dynamical systems converge to the dominant eigenvector (if there is one). We can see below that there is a dominant eigenvector in this case. eigen(B) ## eigen() decomposition ## $values ## [1] 4.66618847 2.64207538 2.41909839 1.80037113 1.27260439 1.00000000 1.00000000 ## [8] 0.49835918 0.02718633 -0.67732874 -0.92557189 -1.72298263 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.23401334 0.57249329 -0.03677166 0.289569576 -0.11347970 0.000000e+00 ## [2,] -0.44971357 0.23508235 0.28327135 -0.009053428 0.29280642 2.626816e-16 ## [3,] -0.24039858 -0.04730698 0.44304618 0.102332378 0.42192015 -1.576610e-16 ## [4,] -0.34439328 0.35635470 -0.30954196 -0.120977575 0.09253829 4.851644e-01 ## [5,] -0.43163295 -0.31276398 0.34545477 0.090957309 -0.17778914 1.198948e-16 ## [6,] -0.20257820 0.10572025 -0.24645181 -0.612347635 -0.18646435 -5.005332e-18 ## [7,] -0.39829657 -0.18275409 -0.04019741 -0.369127791 -0.14336929 -4.851644e-01 ## [8,] -0.37630557 -0.32813461 -0.43931838 0.235004528 0.03236396 -1.021498e-16 ## [9,] -0.06383014 0.34864008 -0.02591199 0.361794131 -0.41627977 -4.851644e-01 ## [10,] -0.10264218 -0.19982920 -0.30957570 0.293619448 0.11872135 -1.709709e-01 ## [11,] -0.10264218 -0.19982920 -0.30957570 0.293619448 0.11872135 1.709709e-01 ## [12,] -0.11773343 -0.19046871 0.24343257 0.113643916 -0.65218735 4.851644e-01 ## [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0.000000e+00 0.17344569 0.19570619 6.653425e-01 0.08742351 0.026476499 ## [2,] 1.203957e-16 0.27532162 0.33415547 -4.571765e-01 -0.14917762 0.397124122 ## [3,] -4.434216e-17 -0.59911594 -0.08232149 1.379670e-01 0.40635541 -0.072909426 ## [4,] 1.208947e-01 -0.01657232 -0.32336571 -2.621537e-01 0.02623868 -0.459495824 ## [5,] 1.395385e-16 0.02521939 -0.25407199 2.257605e-01 -0.63328894 -0.198593023 ## [6,] -9.291441e-17 -0.52296040 0.18953651 1.563407e-01 -0.22162883 0.307139725 ## [7,] -1.208947e-01 0.27891061 0.13898200 -8.104161e-05 0.40052356 -0.376840313 ## [8,] -3.393381e-17 0.08250644 -0.40482358 7.521122e-02 0.23285847 0.520458800 ## [9,] -1.208947e-01 -0.34575674 -0.20117541 -3.966679e-01 -0.04540132 -0.009723345 ## [10,] 6.861261e-01 -0.16447314 0.41613681 -4.483988e-02 -0.12092951 -0.191135557 ## [11,] -6.861261e-01 -0.16447314 0.41613681 -4.483988e-02 -0.12092951 -0.191135557 ## [12,] 1.208947e-01 -0.05027381 0.26117231 -1.345953e-01 0.32888356 0.072932167 For an adjacency matrix \\(A\\), the dominant eigenvector of \\(B + I\\), scaled to sum to 1, is called Gould’s Index of network centrality. Here we compute Gould’s index. vecs = eigen(B)$vectors gould = vecs[,1] # get the dominant eigenvector gould = gould/sum(gould) # rescale it to sum to 1 gould ## [1] 0.07637062 0.14676474 0.07845446 0.11239329 0.14086410 0.06611172 0.12998472 ## [8] 0.12280792 0.02083107 0.03349744 0.03349744 0.03842249 Now we show that the dynamical system is converging to Gould’s index. # Compute the dynamical system N = 30 X = matrix(0,nrow=nrow(B),ncol=N+1) X[,1] = rep(1,nrow(B))/nrow(B) for (i in 1:N) { X[,i+1] = B %*% X[,i] X[,i+1] = X[,i+1]/sum(X[,i+1]) } # Display the data Y = cbind(X[,1],X[,2],X[,3],X[,11],X[,21],gould) rownames(Y) = airports colnames(Y) = cbind(&quot;n=0&quot;,&quot;n=1&quot;,&quot;n=2&quot;,&quot;n=10&quot;,&quot;n=20&quot;,&quot;Gould&quot;) Y ## n=0 n=1 n=2 n=10 n=20 Gould ## ATL 0.08333333 0.08695652 0.08173077 0.07640399 0.07637073 0.07637062 ## LAX 0.08333333 0.13043478 0.13942308 0.14675521 0.14676475 0.14676474 ## ORD 0.08333333 0.06521739 0.07211538 0.07842281 0.07845441 0.07845446 ## MSP 0.08333333 0.10869565 0.11538462 0.11242772 0.11239338 0.11239329 ## DEN 0.08333333 0.13043478 0.13461538 0.14082110 0.14086400 0.14086410 ## JFK 0.08333333 0.06521739 0.06250000 0.06612665 0.06611175 0.06611172 ## SFO 0.08333333 0.10869565 0.12500000 0.12997042 0.12998468 0.12998472 ## SEA 0.08333333 0.13043478 0.12500000 0.12281194 0.12280789 0.12280792 ## PHL 0.08333333 0.04347826 0.02884615 0.02085358 0.02083114 0.02083107 ## PDX 0.08333333 0.04347826 0.03846154 0.03350515 0.03349742 0.03349744 ## MDW 0.08333333 0.04347826 0.03846154 0.03350515 0.03349742 0.03349744 ## LGA 0.08333333 0.04347826 0.03846154 0.03839628 0.03842243 0.03842249 18.3.4 Step 5: Visualize Now let’s plot the network with: the vertices sized by Gould’s Index the labels sized by degree centrality plot(g, layout=locations, vertex.size=250*gould,vertex.label.cex=8*p, vertex.color=&#39;tan1&#39;,vertex.frame.color=&quot;dodgerblue&quot; ) And we show the data containing Gould’s Index and the Degree Centrality. We order the data using the Gould Index and then compare the two. Observe that degree centrality and Gould’s Index do not always agree. Z = cbind(gould,p) rownames(Z)=airports colnames(Z)=c(&#39;Gould&#39;, &#39;Degree&#39;) ii=order(gould,decreasing=TRUE) Z = Z[ii,] Z ## Gould Degree ## LAX 0.14676474 0.14705882 ## DEN 0.14086410 0.14705882 ## SFO 0.12998472 0.11764706 ## SEA 0.12280792 0.14705882 ## MSP 0.11239329 0.11764706 ## ORD 0.07845446 0.05882353 ## ATL 0.07637062 0.08823529 ## JFK 0.06611172 0.05882353 ## LGA 0.03842249 0.02941176 ## PDX 0.03349744 0.02941176 ## MDW 0.03349744 0.02941176 ## PHL 0.02083107 0.02941176 Discuss with your group: Degree centrality and Gould’s Index give different rankings. Look at the table and observe that: LAX, DEN and SEA have the same degree centrality. However LAX and DEN have higher Gould Index than SEA. SFO has lower degree centrality than SEA, but higher Gould centrality! So these two centralities give different rankings. Why does the Gould Index value SFO more than SEA? Find another pair of cities where the rankings of degree centrality and Gould’s Index differ. Look at the plot of the network and explain why this is the case. 18.3.5 Gould Index Summary Now that we understand what Gould’s Index means, let’s summarize how to find the Gould Index values for an adjacency matrix \\(A\\). Create the matrix \\(B = A+I\\). Find the dominant eigenvector \\(\\mathbf{v}\\) of \\(B\\). Normalize the values of \\(\\mathbf{v}\\) so that the entries sum to 1. "],["geometry-in-mathbbrn.html", "Section 19 Geometry in \\(\\mathbb{R}^n\\) 19.1 Dot Product 19.2 Length, Distance, Angle 19.3 Orthogonal Complement", " Section 19 Geometry in \\(\\mathbb{R}^n\\) Download this Rmd file 19.1 Dot Product We will look at the gometry of \\(n\\)-dimensional vectors. For example, here are three vectors in \\(\\mathbb{R}^6\\). We can compute the dot product two different ways. If you have included lit pracma library, you can use the dot command. ## [1] 79 ## [1] -1 ## [1] 0 We can also compute the dot product in native R by multiplying \\(u^T v\\). It is important to remember that this works. ## [,1] ## [1,] 79 ## [,1] ## [1,] -1 ## [,1] ## [1,] 0 19.2 Length, Distance, Angle The length of a vector can be computed using \\(\\sqrt{u\\cdot u}\\) or as the built in 2-norm of a vector. The reason it is the 2-norm is because we are squaring (second power) and taking the square root. ## [,1] ## [1,] 9.539392 ## [1] 9.539392 The distance between two vectors, is the length of the difference between them ## [,1] ## [1,] 1.732051 ## [1] 1.732051 The angle between two vectors is given by the formula \\[ \\theta = \\arccos\\left(\\frac{ v \\cdot w } {||v|| ||w||} \\right) \\] which can be comuted using arccosine function acos. ## [,1] ## [1,] 0.1427914 Sometimes we use the cosine of the angle between the two vectors (we will see an example of this in the homework) \\[ \\cos(\\theta) = \\frac{ v \\cdot w } {||v|| ||w||} \\] which is computed as ## [,1] ## [1,] 0.9898226 This is a number between -1 and 1, with numbers close to 1 meaning that they are closely aligned, numbers close to 0 meaning that they are close to orthogonal, and numbers close to -1 meaning that they are close to opposite. ## [,1] ## [1,] 0 ## [,1] ## [1,] -0.05345225 19.3 Orthogonal Complement The orthogonal complement of a vector space \\(W\\) is \\[ W^\\perp = \\left\\{ v \\in \\mathbb{R}^n \\mid v \\cdot w = 0 \\text{ for every } w \\in W \\right\\}. \\] The orthogonal complement is a subspace. Furthermore, it is enough to check that \\(w\\) is orthogonal to a basis of \\(W\\). Tnat is, you don’t have to check every vector in \\(W\\); if you are orthogonal to the basis then you are orthogonal to \\(W\\). For example, if \\[ W = \\mathsf{span} \\left\\{ \\begin{bmatrix} 1\\\\2\\\\3\\\\4\\\\5 \\end{bmatrix}, \\begin{bmatrix} 1\\\\1\\\\1\\\\1\\\\1 \\end{bmatrix}, \\begin{bmatrix} 1\\\\2\\\\2\\\\2\\\\1 \\end{bmatrix}, \\begin{bmatrix} 3\\\\5\\\\6\\\\7\\\\7 \\end{bmatrix}, \\begin{bmatrix} 0\\\\2\\\\1\\\\0\\\\-4\\end{bmatrix} \\right\\}, \\] then we can put the vectors of \\(W\\) into the rows of a matrix. So in this case, we make the matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 1 1 1 1 1 ## [3,] 1 2 2 2 1 ## [4,] 3 5 6 7 7 ## [5,] 0 2 1 0 -4 Now, \\(W\\) is the row space of \\(A\\). That is, \\(W = \\mathsf{Row}(A)\\). And the row space is orthogonal to the null space. Therefore \\(W^\\perp = \\mathsf{Nul}(A)\\), so we row reduce, ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 1 ## [2,] 0 1 0 -1 -4 ## [3,] 0 0 1 2 4 ## [4,] 0 0 0 0 0 ## [5,] 0 0 0 0 0 There are 2 free variables, so the null space and, thus, \\(W^\\perp\\) are 2 dimensional. We describe a basis of the null space \\[ W^\\perp = \\mathsf{Nul}(A) = \\mathsf{span} \\left\\{ \\begin{bmatrix} 0 \\\\ 1 \\\\ -2 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 4 \\\\ -4 \\\\ 0 \\\\ 1 \\end{bmatrix} \\right\\}. \\] We can check that these vectors are orthogonal to \\(W\\) by multiplying ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 0 ## [3,] 0 0 ## [4,] 0 0 ## [5,] 0 0 "],["least-squares-approximation.html", "Section 20 Least Squares Approximation 20.1 Least Squares 20.2 Example 20.3 Template 20.4 Example from Class: Dec 1 20.5 Fitting a Linear Function 20.6 Fitting a Quadratic Function 20.7 Checkpoint 6.5", " Section 20 Least Squares Approximation Download this Rmd file 20.1 Least Squares The Why: Given a matrix \\(A\\) and a vector \\(\\mathsf{b}\\) that is not in \\(W = \\mathrm{Col}(A)\\), we want to find the “best approximate solution” \\(\\hat{\\mathsf{b}} \\in W\\). In other words, we want to pick the best possible \\(\\hat{\\mathsf{b}} \\approx \\mathsf{b}\\) that lies in the column space of \\(A\\). The What: The answer is to use projections. This “best approximation” is the projection \\(\\hat{\\mathsf{b}} = \\mbox{proj}_W \\mathsf{b}\\). The residual vector vector \\(\\mathsf{r} = \\mathsf{b} - \\hat{\\mathbf{b}}\\) is in \\(W^{\\perp}\\). The length \\(\\| \\mathsf{r} \\|\\) of the residual vector measures the closeness the approximation. The approximate solution to our original problem is the vector \\(\\hat{\\mathsf{x}}\\) such that \\(A \\hat{\\mathsf{x}} = \\hat{\\mathsf{b}}\\). The How: A clever way to solve this is to use the normal equations. The best choice for \\(\\hat{\\mathsf{x}}\\) satisfies \\[ A^{\\top} A \\hat{\\mathsf{x}} = A^{\\top} \\mathsf{b}. \\] 20.2 Example Find the least-squares solution to \\({\\mathsf{A}}x = \\mathsf{b}\\) if \\[\\begin{equation} {\\mathsf{A}}= \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 0 &amp; -1 \\\\ \\end{bmatrix} \\quad \\mbox{and} \\quad \\mathsf{b}= \\begin{bmatrix} 1 \\\\ 1 \\\\ 3 \\end{bmatrix}. \\tag{20.1} \\end{equation}\\] First, for good measure, let’s see if the system is inconsistent A = cbind(c(1,1,0),c(1,2,-1)) b = c(1,1,3) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 ## [2,] 1 2 1 ## [3,] 0 -1 3 rref(Ab) ## b ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 True indeed: \\(\\mathsf{b}\\not \\in Col({\\mathsf{A}})\\). Now we compute the normal equations to see what they look like: t(A) %*% A ## [,1] [,2] ## [1,] 2 3 ## [2,] 3 6 t(A) %*% b ## [,1] ## [1,] 2 ## [2,] 0 So we are going to instead solve the following normal equations instead of (20.1): \\[\\begin{equation} \\begin{bmatrix} 2 &amp; 3 \\\\ 3 &amp; 6 \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}. \\tag{20.2} \\end{equation}\\] We can do this in the following nice, single R command (xhat = solve(t(A) %*% A, t(A) %*% b)) ## [,1] ## [1,] 4 ## [2,] -2 To compute \\(\\hat b\\) we use (bhat = A %*% xhat) ## [,1] ## [1,] 2 ## [2,] 0 ## [3,] 2 And to get the residual, we use (r = b - bhat) ## [,1] ## [1,] -1 ## [2,] 1 ## [3,] 1 sqrt(t(r) %*% r) ## [,1] ## [1,] 1.732051 We can also check that the residual is orthogonal to \\(Col({\\mathsf{A}})\\): t(A) %*% r ## [,1] ## [1,] 0 ## [2,] 0 20.3 Template The following R code does it all. You can use this as a template for future problems. Just enter the matrix A and the vector b. # Given: the matrix A A = cbind(c(1,1,0),c(1,2,-1)) # Given: the target vector b b = c(1,1,3) #solve the normal equation (xhat = solve(t(A) %*% A, t(A) %*% b)) # find the projection (bhat = A %*% xhat) # find the residual vector (r = b - bhat) # check that z is orthogonal to Col(A) t(A) %*% r # measure the distance between bhat and b sqrt(dot(r,r)) 20.4 Example from Class: Dec 1 Use the template from the previous section. # Given: the matrix A A = cbind(c(1,0,1,2),c(2,1,2,1)) # Given: the target vector b b = c(3,5,9,9) #solve the normal equation (xhat = solve(t(A) %*% A, t(A) %*% b)) ## [,1] ## [1,] 3 ## [2,] 2 # find the projection (bhat = A %*% xhat) ## [,1] ## [1,] 7 ## [2,] 2 ## [3,] 7 ## [4,] 8 # find the residual vector (r = b - bhat) ## [,1] ## [1,] -4 ## [2,] 3 ## [3,] 2 ## [4,] 1 # check that z is orthogonal to Col(A) t(A) %*% r ## [,1] ## [1,] 0 ## [2,] 0 # measure the distance between bhat and b sqrt(dot(r,r)) ## [1] 5.477226 20.5 Fitting a Linear Function Here are some points that we’d like to fit to a linear function \\(y = a_0 + a_1 x\\). Note: Here we use y instead of b because we like to write linear equations as “\\(y = cx + d\\).” So the expression “\\(b = a_0 + a_1 x\\)” looks funny to us. So we will talk about y and yhat instead of b and bhat. x = c(1,2,3,4,5,6) y = c(7,2,1,3,7,7) plot(x,y,pch=19,ylim=c(0,10)) grid() The linear equations that we want to fit are as follows. \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\\\ 1 &amp; 4 \\\\ 1 &amp; 5 \\\\ 1 &amp; 6 \\\\ \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 2 \\\\ 1 \\\\ 3 \\\\ 7 \\\\ 7 \\end{bmatrix}. \\] These equations are inconsistent, so we solve the normal equations \\(A^T A x = A^T y\\) and find an approximate solution instead. Pro Tip: a clever way to create the desired matrix \\(A\\) is to use the fact that \\(x^0=1\\) for any number \\(x\\). (A = cbind(x^0,x)) ## x ## [1,] 1 1 ## [2,] 1 2 ## [3,] 1 3 ## [4,] 1 4 ## [5,] 1 5 ## [6,] 1 6 Let’s take a look at the normal equations: t(A) %*% A ## x ## 6 21 ## x 21 91 t(A) %*% y ## [,1] ## 27 ## x 103 So the normal equations to solve are below. It’s surprising how, even though there are 6 variables, we only have to solve a 2x2 equation, since there are 2 unknowns. \\[ \\begin{bmatrix} 6 &amp; 21 \\\\ 21 &amp; 91 \\\\ \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix} = \\begin{bmatrix} 27 \\\\ 103 \\end{bmatrix}. \\] (xhat = solve(t(A) %*% A, t(A) %*% y)) ## [,1] ## 2.8000000 ## x 0.4857143 This tells us that the desired intercept is \\(a_0 = 2.8\\), the desired slope is \\(a_1 = 0.4856\\), and the linear model is \\(y = 2.8 + 0.4856x\\). We can plot the points together with the solution using: #plot the original set of points plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main=&#39;the best-fit linear function&#39;) # generate points for the fitted line and plot it tt = seq(1,6,len=100) lines(tt,xhat[1]+xhat[2]*tt,col=&#39;blue&#39;) # get yhat yhat = A %*% xhat # add the residuals to the plot for (i in 1:length(x)) { lines(c(x[i],x[i]),c(y[i],yhat[i]), col=&#39;red&#39;) } #add yhat to the plot points(x,yhat,pch=19,col=&#39;orange&#39;) #put the original points back on the plot last so we can see them points(x,y,pch=19,col=&quot;black&quot;) grid() In this visualization we see the following: The black points: the original data points cbind(x,y). This represents the entries of the desired target vector y. The blue curve: the fitted curve, created from the approximate solution xhat. The orange points: the approximations cbind(x,yhat) of the data points cbind(x,y). This represents entries of the projection yhat. The red line segments: the distances between the original data points (block dots) and their approximations (orange dots). The lengths of these red segments are the entries of the residual vector r. Let’s look at the residual and see that it is indeed orthogonal to the columns of \\(A\\). yhat = A %*% xhat r = y - yhat res=cbind(y,yhat,r) colnames(res) = c(&quot;y&quot;,&quot;yhat&quot;,&quot;r&quot;) res ## y yhat r ## [1,] 7 3.285714 3.714286 ## [2,] 2 3.771429 -1.771429 ## [3,] 1 4.257143 -3.257143 ## [4,] 3 4.742857 -1.742857 ## [5,] 7 5.228571 1.771429 ## [6,] 7 5.714286 1.285714 t(A) %*% r ## [,1] ## -8.881784e-16 ## x 3.552714e-15 t(r) %*% r ## [,1] ## [1,] 35.37143 20.6 Fitting a Quadratic Function The data we have been working with has a quadratic look to it, so let’s try adding an \\(x^2\\) term. That is, we will fit the model \\(y = a_0 + a_1 x + a_2 x^2\\). The equations we want to solve are In this case, the linear model that we’d like to solve is: \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 2 &amp; 4 \\\\ 1 &amp; 3 &amp; 9 \\\\ 1 &amp; 4 &amp; 16 \\\\ 1 &amp; 5 &amp; 25 \\\\ 1 &amp; 6 &amp; 36 \\\\ \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 2 \\\\ 1 \\\\ 3 \\\\ 7 \\\\ 7 \\end{bmatrix}. \\] It is easy enough to add this to our matrix \\(A\\). (A = cbind(x^0,x,x^2)) ## x ## [1,] 1 1 1 ## [2,] 1 2 4 ## [3,] 1 3 9 ## [4,] 1 4 16 ## [5,] 1 5 25 ## [6,] 1 6 36 In this case our normal equations are 3x3 t(A) %*% A ## x ## 6 21 91 ## x 21 91 441 ## 91 441 2275 t(A) %*% y ## [,1] ## 27 ## x 103 ## 499 \\[ \\begin{bmatrix} 6 &amp; 21 &amp; 91 \\\\ 21 &amp; 91 &amp; 441 \\\\ 91 &amp; 441 &amp; 2275 \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} 27 \\\\ 103 \\\\ 499 \\end{bmatrix}. \\] Whose solution is computed by (xhat = solve(t(A) %*% A, t(A) %*% y)) ## [,1] ## 10.3000000 ## x -5.1392857 ## 0.8035714 Notice that our solution is now \\(y = 10.3 - 5.1393 x + 0.8036 x^2\\). The linear term is now negative, but there is a positive quadratic term. Let’s look at the same plo but with the addex \\(x^2\\) term. We see that the residuals are smaller and, importantly, the model appears to better fit the data. #plot the original set of points plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main=&#39;the best-fit quadratic function&#39;) # generate points for the fitted line and plot it tt = seq(0,7,len=100) lines(tt,xhat[1]+xhat[2]*tt+xhat[3]*tt^2,col=&#39;blue&#39;) # get yhat yhat = A %*% xhat # add the residuals to the plot for (i in 1:length(x)) { lines(c(x[i],x[i]),c(y[i],yhat[i]), col=&#39;red&#39;) } #add yhat to the plot points(x,yhat,pch=19,col=&#39;orange&#39;) #put the original points back on the plot last so we can see them points(x,y,pch=19,col=&quot;black&quot;) grid() Let’s look again at the residual and see that it is indeed orthogonal to the columns of \\(A\\) and see that the residual got shorter. yhat = A %*% xhat r = y - yhat res=cbind(y,yhat,r) colnames(res) = c(&quot;y&quot;,&quot;yhat&quot;,&quot;r&quot;) res ## y yhat r ## [1,] 7 5.964286 1.035714 ## [2,] 2 3.235714 -1.235714 ## [3,] 1 2.114286 -1.114286 ## [4,] 3 2.600000 0.400000 ## [5,] 7 4.692857 2.307143 ## [6,] 7 8.392857 -1.392857 t(A) %*% r ## [,1] ## -1.820766e-14 ## x -2.486900e-14 ## -1.847411e-13 t(r) %*% r ## [,1] ## [1,] 11.26429 20.7 Checkpoint 6.5 Here we have 4 data points xdata = c(1,2,3,4) ydata = c(6,10,12,11) plot(xdata,ydata,xlim=c(0,5),ylim=c(0,15),xlab=&#39;x&#39;,ylab=&#39;x&#39;,col=&#39;red&#39;,pch=19) 20.7.1 Linear Fit Fit a line to the data: \\(a + b x\\) A = cbind(xdata^0,xdata^1) t(A)%*% A ## [,1] [,2] ## [1,] 4 10 ## [2,] 10 30 t(A)%*% ydata ## [,1] ## [1,] 39 ## [2,] 106 (xhat = solve(t(A)%*%A,t(A)%*%ydata)) ## [,1] ## [1,] 5.5 ## [2,] 1.7 And plot it: t = seq(0,6,by=.01) plot(t,xhat[1] + xhat[2]*t,col=&#39;blue&#39;,xlim=c(0,5),ylim=c(0,15),xlab=&#39;x&#39;,ylab=&#39;x&#39;,type=&#39;l&#39;) points(xdata,ydata,,col=&#39;red&#39;,pch=19) Compute the residual: (yhat = A %*% xhat) ## [,1] ## [1,] 7.2 ## [2,] 8.9 ## [3,] 10.6 ## [4,] 12.3 (r = ydata - yhat) ## [,1] ## [1,] -1.2 ## [2,] 1.1 ## [3,] 1.4 ## [4,] -1.3 t(A) %*% r ## [,1] ## [1,] 7.105427e-15 ## [2,] 3.552714e-15 sqrt(t(r) %*% r) ## [,1] ## [1,] 2.50998 20.7.2 Quadratic Fit Fit a parabola to the data: \\(a + b x + c x^2\\) A = cbind(xdata^0,xdata^1,xdata^2) t(A)%*% A ## [,1] [,2] [,3] ## [1,] 4 10 30 ## [2,] 10 30 100 ## [3,] 30 100 354 t(A)%*% ydata ## [,1] ## [1,] 39 ## [2,] 106 ## [3,] 330 (xhat = solve(t(A)%*%A,t(A)%*%ydata)) ## [,1] ## [1,] -0.75 ## [2,] 7.95 ## [3,] -1.25 And plot it: t = seq(0,6,by=.01) plot(t,xhat[1] + xhat[2]*t+ xhat[3]*t^2,col=&#39;blue&#39;,xlim=c(0,5),ylim=c(0,15),xlab=&#39;x&#39;,ylab=&#39;x&#39;,type=&#39;l&#39;) points(xdata,ydata,,col=&#39;red&#39;,pch=19) Compute the residual: (yhat = A %*% xhat) ## [,1] ## [1,] 5.95 ## [2,] 10.15 ## [3,] 11.85 ## [4,] 11.05 (r = ydata - yhat) ## [,1] ## [1,] 0.05 ## [2,] -0.15 ## [3,] 0.15 ## [4,] -0.05 t(A) %*% r ## [,1] ## [1,] 4.440892e-15 ## [2,] 5.062617e-14 ## [3,] 3.286260e-14 sqrt(t(r) %*% r) ## [,1] ## [1,] 0.2236068 And add the residuals to the plot t = seq(0,6,by=.01) plot(t,xhat[1] + xhat[2]*t + xhat[3]*t^2,col=&#39;blue&#39;,xlim=c(0,5),ylim=c(0,15),xlab=&#39;x&#39;,ylab=&#39;x&#39;,type=&#39;l&#39;) for (i in 1:length(xdata)){lines(c(xdata[i],xdata[i]),c(ydata[i],yhat[i]),type=&#39;l&#39;)} points(xdata,ydata,,col=&#39;red&#39;,pch=19) points(xdata,yhat,pch=16) "],["svd-and-image-compression.html", "Section 21 SVD and Image Compression 21.1 Example 21.2 Image Compression", " Section 21 SVD and Image Compression Download this Rmd file Here we will illustrate one application of the singular value decomposition SVD of a matrix \\(A\\). To learn more about this, and other cool applications, take MATH 365 Computational Linear Algebra. 21.1 Example Here is a \\(4 \\times 5\\) matrix \\(A\\). (A = cbind(c(1,-1,1,0),c(-2,3,0,2),c(1,-1,1,0),c(0,1,4,0),c(1,-1,5,-4))) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 -2 1 0 1 ## [2,] -1 3 -1 1 -1 ## [3,] 1 0 1 4 5 ## [4,] 0 2 0 0 -4 We will call svd(A,nv=ncol(A)) to compute the singular value decomposition. (The nv=ncol(A) forces svd to give us all of the columns. For technical reasons, the default output does not). svd(A,nv=ncol(A)) ## $d ## [1] 7.657063e+00 4.528454e+00 1.965323e+00 1.981025e-16 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] -0.2248593 0.3962930 -0.4593417 -0.7624929 ## [2,] 0.2098332 -0.7003225 0.3056557 -0.6099943 ## [3,] -0.8000554 -0.5005874 -0.2933731 0.1524986 ## [4,] 0.5150919 -0.3192373 -0.7807125 0.1524986 ## ## $v ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.1612561 0.13161844 -0.5385224 0.1709288 -0.7984047 ## [2,] 0.2754845 -0.77996334 0.1395320 -0.4039829 -0.3648207 ## [3,] -0.1612561 0.13161844 -0.5385224 -0.7769031 0.2511736 ## [4,] -0.3905399 -0.59682006 -0.4415746 0.4039829 0.3648207 ## [5,] -0.8482805 -0.02856877 0.4533541 -0.2019914 -0.1824103 The singular value decomposition (SVD) is the matrix factorization \\[ A = U \\Sigma V^T \\qquad \\hbox{or} \\qquad U^T A V = \\Sigma, \\] where \\(\\Sigma\\) is the \\(4 \\times 5\\) matrix with the singular values down the column. We can confirm this with the following computation that gives the matrix \\(\\Sigma\\). U = svd(A,nv=ncol(A))$u V = svd(A,nv=ncol(A))$v zapsmall(t(U) %*% A %*% V) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 7.657063 0.000000 0.000000 0 0 ## [2,] 0.000000 4.528454 0.000000 0 0 ## [3,] 0.000000 0.000000 1.965323 0 0 ## [4,] 0.000000 0.000000 0.000000 0 0 The spectral decomposition is \\[ A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\sigma_3 \\mathsf{u}_3 \\mathsf{v}_3^{\\top} + \\sigma_4 \\mathsf{u}_4 \\mathsf{v}_4^{\\top}. \\] For example, we can get the first matrix in this deccomposition as ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.2776445 -0.4743187 0.2776445 0.6724166 1.460537 ## [2,] -0.2590912 0.4426227 -0.2590912 -0.6274829 -1.362938 ## [3,] 0.9878667 -1.6876386 0.9878667 2.3924763 5.196629 ## [4,] -0.6360087 1.0865360 -0.6360087 -1.5403248 -3.345696 21.2 Image Compression We need the jpeg package. We will use the cameraman image, which is a famous photo from image and signal processing. where = &quot;https://www.macalester.edu/~dshuman1/data/cameraman_small.jpg&quot; img = readJPEG(readBin(where,&quot;raw&quot;,1e6)) dim(img) ## [1] 256 256 The matrix img is a 256 x 256 matrix with each entry representing the grayscale value of a single pixel. So to store the image, we need to store 65,536 floating point numbers. We can plot it with the following function: imPlot = function(img,...) { plot(1:2, type=&#39;n&#39;,xlab=&quot; &quot;,ylab= &quot; &quot;,...) rasterImage(img, 1.0, 1.0, 2.0, 2.0) } imPlot(img,main=&quot;Cameraman Image&quot;) Here are the singular values of the image The following code is used to choose only the first \\(k\\) singular values in the spectral decomposition. SVDApprox = function(A,k = floor(1/2*min(nrow(A),ncol(A)))) { foo = svd(A) sings = foo$d U = foo$u V = foo$v if(k==1) D=matrix(sings[1],nrow=1,ncol=1) else D=diag(sings[1:k]) M=U[,1:k]%*%D%*%t(V[,1:k]) return(M) } approxImg=function(img,k){ approxIm = SVDApprox(img,k) approxIm[approxIm&lt;0] = 0 approxIm[approxIm&gt;1] = 1 plot(1:2, type=&#39;n&#39;) rasterImage(approxIm, 1.0, 1.0, 2.0, 2.0) } And here we show the singular value approximation with increasing numbers of singular values: approxImg(img,1) approxImg(img,2) approxImg(img,3) approxImg(img,4) approxImg(img,5) approxImg(img,10) approxImg(img,25) approxImg(img,50) approxImg(img,100) "],["exam-1-review.html", "Section 22 Exam 1 Review 22.1 Overview 22.2 Practice Problems 22.3 Solutions to Practice Problems", " Section 22 Exam 1 Review 22.1 Overview Our first exam covers sections 1.1 - 2.3 in Lay’s book.This corresponds to Problem Sets 1-4. I will hand it out right away, even a few minutes early, so you can start right on time. You must turn it in by 9:35 (for the 8:30 class) and 10:45 for the 10:50 class). No exceptions. The next class needs to come in and get started. No calculators are allowed, and none are needed. If row reductions are needed, they will be easy (integer) calculations, and there will not be many. It will be closed book but you can bring a 3\" x 5\" notecard with notes written on both sides. These notes should be hand written by you. You do not need to turn in the note card with the exam. There will be some basic calculations, but the problems will focus more on the ideas than on the calculations. I will ask some problems that are very similar to homework problems, Edfinity problems, and examples from class or the videos. I will ask other problems that are somewhat different from things you have done. On these, you are to apply your knowledge in a slightly new setting to demonstrate an even higher mastery of the material. You will be allowed to re-write one problem to earn back half of the points that you lost. This will be due the class period after I hand the exam back. The exam is worth 12% of your course grade as outlined in the syllabus. 22.1.1 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly (see also the important definitions): elementary row operations REF and RREF and pivot positions linear combination span linear independence homogeneous and nonhomogeneous equations Understand the geometric relationship between the solutions to \\(Ax = 0\\) and \\(Ax=b\\) Understand Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b\\) in \\(\\mathbb{R}^m\\), \\(A x = b\\) has a solution Each \\(b\\) in \\(\\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. The columns of \\(A\\) are linearly independent if and only if \\(Ax=0\\) only has the trivial solution Understand Theorem 8 in Section 1.7: if you have more than \\(n\\) vectors in \\(\\mathbb{R}^n\\) they must be linearly dependent. linear transformations the matrix of a linear transformation One-to-one and onto Matrix Multiplication Matrix Inverses The Invertible Matrix Theorem 22.1.2 Skills You should be able to perform these linear algebra tasks. Identify linear systems from nonlinear systems Make the augmented matrix from a set of equations Row reduce a system of equations into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Write the solution set to \\(Ax=b\\) as a parametric vector equation. Convert back and forth between systems of equations, vector equations, and matrix equations. Compute the matrix-vector product \\(Ax\\) Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) Manipulate matrix vector products using: \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) Determine if a function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is linear. Find the matrix of a linear transformation \\(T\\). find a matrix inverse solve equations using matrix inverses 22.2 Practice Problems 22.2.1 I have performed some row operations below for you on a matrix \\(A\\). Write out the complete set of solutions to \\(A \\mathsf{x} = {\\bf 0}\\). \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{bmatrix} \\] 22.2.2 I have performed some row operations below for you on a matrix \\(B\\). \\[ B= \\begin{bmatrix} 1&amp; 1&amp; 0 \\\\ 0&amp; 1&amp; 1 \\\\ 2&amp; 1&amp; 2 \\\\ 1&amp; -1&amp; 1 \\\\ 2&amp; 3&amp; 1 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 0&amp; 0 \\\\ 0&amp; 1&amp; 0 \\\\ 0&amp; 0&amp; 1 \\\\ 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 \\\\ \\end{bmatrix} \\] Describe the solutions to the equation \\(B \\mathsf{x} = {\\bf 0}\\). Fill in the boxes: the transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{\\square}\\) to \\(\\mathbb{R}^{\\square}\\). Are the columns of \\(B\\) linearly independent or dependent? Do the columns of \\(A\\) span \\(\\mathbb{R}^5\\)? Is the transformation \\(T\\) one-to-one? Is it onto? 22.2.3 I want to know if it is possible to write \\(\\mathsf{w}\\) as a linear combination of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) below. Write down, but do not solve, a matrix equation that would solve this problem. Your answer should be of the form \\(A \\mathsf{x} = \\mathsf{b}\\), where I can clearly see what \\(A, \\mathsf{x}\\), and \\(\\mathsf{b}\\) are. I should also be able to tell how many unknowns there are. \\[ \\mathsf{v}_1 = \\left[ \\begin{matrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_2 = \\left[ \\begin{matrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_3 = \\left[ \\begin{matrix} 1 \\\\ 1 \\\\ 0 \\\\ -2 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{w} = \\left[ \\begin{matrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\\\ \\end{matrix}\\right] . \\] 22.2.4 Describe all vectors that are not in the span of the columns of the matrix \\(A\\) below: \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 4 \\\\ -3&amp; -5&amp; -11\\\\ 1&amp; 1&amp; 3 \\\\ \\end{bmatrix} \\] 22.2.5 The matrix below is \\(3 \\times 3\\) but the third column is missing. Add a nonzero third column so that the columns of \\(A\\) are linearly dependent and add a 3rd column so that the columns of \\(A\\) are linearly independent. Briefly describe your strategy. \\[ \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\qquad\\qquad \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\] 22.2.6 In each case below, find the matrix of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is not linear. The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix}\\right) = \\begin{bmatrix} x_1 + x_2 \\\\ 2 x_1 \\\\ -x_2 \\\\\\end{bmatrix}. \\] The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)= \\begin{bmatrix} x_1 + x_2 + x_3 \\\\ x_1 x_2 \\\\ -x_2 + 2 x_3 \\end{bmatrix}. \\] 22.2.7 Write the following systems of equations in vector and matrix form. \\[ \\begin{array} {ccccccccccc} 5 x_1 &amp;+&amp; 3 x_2 &amp;+&amp; x_3 &amp;+&amp; 11 x_4 &amp;-&amp; x_5 &amp;=&amp; 10 \\\\ 4 x_1 &amp;+&amp; x_2 &amp;+&amp; 3 x_3 &amp;+&amp; 2 x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 11 \\\\ - x_1 &amp;+&amp; 3 x_2 &amp;-&amp; 2 x_3 &amp;+&amp; x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 12 \\\\ \\end{array} \\] 22.2.8 Let \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) be the vectors in the columns of the matrix \\(A\\) below. \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 3 &amp; 1 \\\\ 2 &amp; 0 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 3 &amp; 1 \\\\ -1 &amp; 0 &amp; -1 &amp; 0 \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right] \\] a. Are the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) linear independent or dependent? If they are linearly dependent, please give a dependence relation among them. b. Describe the span of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) inside of \\(\\mathbb{R}^4\\)? 22.2.9 Find a solution to \\(A \\mathsf{x}=0\\) that no one else in the class has. \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 2 &amp; 0 &amp; 4 &amp; 1 &amp; 4 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 1 &amp; 0 &amp; 2 &amp; 1 &amp; 3 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\] 22.2.10 fixed! in an earlier version the image was from a linear transformation, this one is not. Explain why it is not possible for a linear transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) to transform my hous as seen below: 22.2.11 Consider the following vectors in \\(\\mathbb{R}^3\\): \\[ \\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\-1 \\\\1 \\\\ \\end{bmatrix}, \\mathsf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ \\end{bmatrix}, \\mathsf{v}_3 = \\begin{bmatrix} 3 \\\\ -1 \\\\ 5 \\\\ \\end{bmatrix}. \\] Determine whether the set \\(\\{\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\}\\) is linearly independent or linearly dependent. If it is dependent, give a dependence relation. If it is independent show the work that justifies this conclusion. Find a vector \\(\\mathsf{b}\\in \\mathbb{R}^3\\) that is not in \\(\\mathsf{span}(\\mathsf{v}_1,\\mathsf{v}_2,\\mathsf{v}_3)\\). Justify your answer. If \\(T: \\mathbb{R}^3 \\to \\mathbb{R}^3\\) is a linear transformation such that \\(T(\\mathsf{v}_1) = \\mathsf{v}_2\\) and \\(T(\\mathsf{v}_2) = \\mathsf{v}_3\\) then compute \\(T(\\mathsf{v}_3)\\). 22.2.12 Below is the row reduction of the augmented matrix for the equation \\({\\mathsf{A}}x = \\mathsf{b}\\). \\[ [{\\mathsf{A}}\\mid \\mathsf{b}] = \\left[\\begin{array}{ccccc|c} 1 &amp; 1 &amp; -1 &amp; 1 &amp; 4 &amp; 2 \\\\ 2 &amp; -1 &amp; -5 &amp; -3 &amp; -3 &amp; 2 \\\\ 1 &amp; 1 &amp; -1 &amp; 0 &amp; 3 &amp; 1 \\\\ 1 &amp; 1 &amp; -1 &amp; 1 &amp; 4 &amp; 2 \\\\ \\end{array}\\right] \\longrightarrow \\left[\\begin{array}{ccccc|c} 1 &amp; 0 &amp; -2 &amp; 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array}\\right] \\hskip5in \\] Give the general solution to \\({\\mathsf{A}}x = \\mathsf{b}\\) in parametric form. Describe geometrically the general solution you gave in part (a) ( e.g., a single point in \\(\\mathbb{R}^3\\), a line in \\(\\mathbb{R}^2\\), a plane in \\(\\mathbb{R}^4\\), a three dimensional space in \\(\\mathbb{R}^9\\)). If \\(T\\) is the linear transformation whose matrix is \\({\\mathsf{A}}\\), then $T: ^{} ^{} $ (fill in the blanks). Determine if these statements are true T, false F, or that there is not enough information, I, to decide. No justification needed. \\(T\\) is one-to-one \\(T\\) is onto \\({\\mathsf{A}}x = \\mathsf{b}\\) has a solution for all \\(\\mathsf{b}\\) in the codomain. \\({\\mathsf{A}}x = \\mathsf{b}\\) has exactly one solution for some \\(\\mathsf{b}\\) in the The columns of \\({\\mathsf{A}}\\) are linearly independent. The columns of \\({\\mathsf{A}}\\) span \\(\\mathbb{R}^4\\). The 3rd column of \\({\\mathsf{A}}\\) is in the span of the first 2 columns of \\({\\mathsf{A}}\\). 22.2.13 Given that \\(A,B,C\\) are invertible matrices, solve the following matrix equation for \\(X\\) \\[ C (A + X) B^{-1} = I \\] 22.2.14 We’ve seen a few problems that ask you to interpret the meaning of matrix multiplication: the rental car problem, the rain-sunshine matrix, and the graves and pottery matrix. Here is another problem in which the goal is to interpret the meaning of matrix multiplication. Here is a graph of the network of domestic airline flights in Korea. They are not located geographically in the plane, but there is a connection if there is a direct flight from one airport to the other. It is common to study these networks using a matrix called the adjacency matrix. Here is the adjacency matrix of this network. A = rbind(c(0,1,0,0,1,0,0,0,0,0,0,0),c(1,0,1,1,0,1,0,0,0,0,0,0), c(0,1,0,0,1,0,1,1,1,1,1,0),c(0,1,0,0,0,1,0,0,0,0,0,0), c(1,0,1,0,0,1,0,0,0,0,0,0),c(0,1,0,1,1,0,1,1,1,1,0,1), c(0,0,1,0,0,1,0,0,0,0,0,0),c(0,0,1,0,0,1,0,0,0,0,0,0), c(0,0,1,0,0,1,0,0,0,0,0,0),c(0,0,1,0,0,1,0,0,0,0,0,0), c(0,0,1,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,1,0,0,0,0,0,0)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0 1 0 0 1 0 0 0 0 0 0 0 ## [2,] 1 0 1 1 0 1 0 0 0 0 0 0 ## [3,] 0 1 0 0 1 0 1 1 1 1 1 0 ## [4,] 0 1 0 0 0 1 0 0 0 0 0 0 ## [5,] 1 0 1 0 0 1 0 0 0 0 0 0 ## [6,] 0 1 0 1 1 0 1 1 1 1 0 1 ## [7,] 0 0 1 0 0 1 0 0 0 0 0 0 ## [8,] 0 0 1 0 0 1 0 0 0 0 0 0 ## [9,] 0 0 1 0 0 1 0 0 0 0 0 0 ## [10,] 0 0 1 0 0 1 0 0 0 0 0 0 ## [11,] 0 0 1 0 0 0 0 0 0 0 0 0 ## [12,] 0 0 0 0 0 1 0 0 0 0 0 0 Look at the location of the 0s and 1s in this matrix in the context of the network and decide when there is a 1 in the matrix. Let \\(v\\) be the vector in \\(\\mathbb{R}^{12}\\) consisting of all 1s. Multiply \\(Av\\) and decide what the entries of this vectors tell us about the network. Square the matrix, i.e., compute \\(A^2\\). This amounts to dotting the rows of \\(A\\) with the columns of \\(A\\). Decide what the entries of \\(A^2\\) mean in terms of the network. Multiply \\(A^2\\) by the all 1s vector \\(v\\). What do the entries of that vector mean. 22.2.15 Suppose that you are given four linearly dependent vectors, \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\), in \\(\\mathbb{R}^n\\). And suppose that you have a linear transformation \\(T: \\mathbb{R}^n \\to\\mathbb{R}^m\\). State precisely what it means for \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) to be linearly dependent. Apply \\(T\\) to your answer to a and use it to show that \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3), T(\\mathsf{v}_4)\\) must also be linearly dependent in \\(\\mathbb{R}^m\\). This problem shows that linear transformations send dependent vectors to dependent vectors. 22.2.16 Challenge Prove that if \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) are linearly independent in \\(\\mathbb{R}^n\\) and \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is one-to-one, then \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3), T(\\mathsf{v}_4)\\) are linearly independent in \\(\\mathbb{R}^m\\) Note: I won’t ask anything this hard on the exam but we are going to start doing proofs like this after exam 1. 22.3 Solutions to Practice Problems 22.3.1 The parametric vector form of the solution is \\[\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\\\ 1 \\\\0 \\\\ 0 \\end{bmatrix} u \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\-2 \\\\ 1 \\end{bmatrix}\\] 22.3.2 There is one solution: \\(\\mathsf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\). The transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{3}\\) to \\(\\mathbb{R}^{5}\\). Independent Do not span One-to-one but not onto. 22.3.3 \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 0 &amp; 1 \\\\ 3 &amp; 1 &amp; 0 \\\\ 4 &amp; 0 &amp; -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\end{bmatrix} \\] 22.3.4 We want to find all target vectors \\(\\mathsf{b}\\) such that \\(A \\mathsf{x} = \\mathsf{b}\\) is inconsistent. So we want the augmented matrix \\(\\begin{bmatrix} A \\,| \\, b \\end{bmatrix}\\) to have a pivot in the last column. \\[ \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ -3&amp; -5&amp; -11 &amp; b_2\\\\ 1&amp; 1&amp; 3 &amp; b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; -1&amp; -1 &amp; -b_1+b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; 0&amp; 0 &amp; 2b_1+b_2+b_3 \\\\ \\end{array} \\right] \\] So the set of target vectors that are not in the span of the columns of \\(A\\) are the vectors \\[ \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\qquad \\mbox{where} \\qquad 2b_1 + b_2 + b_3 \\neq 0. \\] 22.3.5 In the first case, add a vector that is in the span of the first two vectors. For example, you might add the sum of the two vectors. In the second case, add a vector that is not in the span of the two vectors. Add something that row reduces to the identity. 22.3.6 This is a linear transformation with \\[A = \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}.\\] This is not a linear transformation because \\[ 2 \\, T \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 2 \\end{bmatrix} \\quad \\mbox{while} \\quad T \\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 6 \\\\ 4 \\\\ 2 \\end{bmatrix}. \\] 22.3.7 Vector Form: \\[ x_1 \\begin{bmatrix} 5 \\\\ 4 \\\\ -1 \\end{bmatrix} + x_2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 3 \\end{bmatrix} + x_3 \\begin{bmatrix} 1 \\\\ 3 \\\\ -2 \\end{bmatrix} + x_4 \\begin{bmatrix} 11 \\\\ 2 \\\\ 1 \\end{bmatrix} + x_5 \\begin{bmatrix} -1 \\\\ 6 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] Matrix Form: \\[ \\begin{bmatrix} 5 &amp; 3 &amp; 1 &amp; 11 &amp; -1 \\\\ 4 &amp; 1 &amp; 3 &amp; 2 &amp; 6 \\\\ -1 &amp; 3 &amp; -2&amp; 1 &amp; 6 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] 22.3.8 \\(-\\mathsf{v}_1 - 2\\mathsf{v}_2 + \\mathsf{v}_3 + 0 \\mathsf{v}_4 = 0\\). \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4)\\) looks like a copy of \\(\\mathbb{R}^3\\) sitting inside \\(\\mathbb{R}^4\\). In other words, is 3-dimensional subset of \\(\\mathbb{R}^4\\). 22.3.9 The general solution is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] My solution is \\[77,083,679 \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - 72,159,215 \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] 22.3.10 Hint: Look at the relation of the peak of the house to \\(\\mathsf{e}_1\\) and \\(\\mathsf{e}_2\\). 22.3.11 This one might be too much row-reduction for an in-class exam. Not sure. But these ideas are important. They are dependent. Find a nonzero solution to \\(A x = 0\\) to get a dependence relation. You can augment with a symbolic vector \\([b_1,b_2,b_3]\\) and row reduce. This will give you a formula telling you what conditions on \\(b_1,b_2,b_3\\) are needed for the equation to be inconsistent. You could also guess a strange choice for \\(b\\) and row reduce to show that it is not in the span. Use the dependence relation. 22.3.12 Two free variables, so it is a plane in \\(\\mathbb{R}^5\\). \\(\\mathbb{R}^5 \\to \\mathbb{R}^4\\). F, F, F, F, F, F, T. Not (col3) = -2(col1) + (col2), which can be seen from the RREF. 22.3.13 Multiply on the left by \\(C^{-1}\\) and on the right by \\(B\\): \\[ \\begin{array}{rcl} C (A + X) B^{-1} &amp;=&amp; I \\\\ C^{-1}C (A + X) B^{-1} &amp;=&amp; C^{-1} I \\\\ I (A + X) B^{-1} &amp;=&amp; C^{-1} \\\\ (A + X) B^{-1} &amp;=&amp; C^{-1} \\\\ (A + X) B^{-1} B &amp;=&amp; C^{-1}B \\\\ (A + X) I &amp;=&amp; C^{-1}B \\\\ A + X &amp;=&amp; C^{-1}B \\\\ X &amp;=&amp; C^{-1}B - A \\\\ \\end{array} \\] 22.3.14 Will discuss it in class on Wednesday. 22.3.15 There exists scalars \\(c_1, c_2, c_3, c_4\\), not all equal to 0, so that \\[ c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_3 + c_4 \\mathsf{v}_4 = {\\bf 0}. \\] Apply \\(T\\) to both sides of this equation and use the linearity properties: \\[ \\begin{array}{rcl} T(c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_3 + c_4 \\mathsf{v}_4) &amp;=&amp; T({\\bf 0}) \\\\ c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_3) + c_4 T(\\mathsf{v}_4) &amp;=&amp; {\\bf 0} \\\\ \\end{array} \\] This is a dependence relation for \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3), T(\\mathsf{v}_4)\\) using the same scalars, which we know are not all 0, so these vectors are also linearly depedent. Note that we used the fact that \\(T({\\bf 0}) = {\\bf 0}.\\) "],["exam-2-review.html", "Section 23 Exam 2 Review 23.1 Overview 23.2 Practice Problems 23.3 Solutions to Practice Problems", " Section 23 Exam 2 Review 23.1 Overview I will hand it out right away, even a few minutes early, so you can start right on time. You must turn it in by 9:35 (for the 8:30 class) and 10:45 for the 10:50 class). No exceptions. The next class needs to come in and get started. No calculators are allowed, and none are needed. If row reductions are needed, they will be easy (integer) calculations, and there will not be many. It will be closed book but you can bring a 3\" x 5\" notecard with notes written on both sides. These notes should be hand written by you. You do not need to turn in the note card with the exam. There will be some basic calculations, but the problems will focus more on the ideas than on the calculations. I will ask some problems that are very similar to homework problems, Edfinity problems, and examples from class or the videos. I will ask other problems that are somewhat different from things you have done. On these, you are to apply your knowledge in a slightly new setting to demonstrate an even higher mastery of the material. You will be allowed to re-write one problem to earn back half of the points that you lost. This will be due the class period after I hand the exam back. The exam is worth 12% of your course grade as outlined in the syllabus. 23.1.1 Vocabulary and Concepts Subspaces 4.1: Subspaces of \\(\\mathbb{R}^n\\) 4.2: Null Space and Column Space 4.3: Bases 4.4: Coordinates 4.5: Dimension, Rank, and Nullity 4.6: Row Space This corresponds to Problem Sets 5 and 6. The best way to study is to do practice problems. The Exam will have some calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assignments Redo the class exercises problems Try to re-solve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 23.1.2 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly: all of the Important Definitions found here. subspaces null space and column space of a matrix kernel and image of a linear transformation basis (span and linearly independent) coordinate vector with respect to a basis \\(\\mathcal{B}\\) change-of-coordinates matrix dimension rank row space 23.1.3 Skills You should be able to perform these linear algebra tasks. show that a subset is a subspace or demonstrate that it is not a subspace describe the null space and the column space of a matrix A, including find a basis for theses spaces. determine if a vector v is in Nul(A), Col(A), or Row(A). find a basis of a subspace, including the null space and column space answer questions about the connections between all these ideas. For example, What is the connection between the column space and null space and solving equations \\(A x = b\\). What is the connection between the column space and null space and the linear transformation \\(T_A: \\mathbb{R}^n \\to \\mathbb{R}^m\\)? What is the connection between the column space and null space and the vectors in the matrix of \\(A\\): linear independence and span? write short proofs of basic statements using the Important Definitions. For example, something like the Getting into a Subspace problem 23.2 Practice Problems 23.2.1 Here are the row reductions of 4 matrices into reduced row echelon form. \\[ \\begin{array}{ll} A \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 5 &amp; -3 &amp; 0\\\\ 0 &amp; 1 &amp; -2 &amp; 8 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\qquad &amp; B \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\\\ \\\\ C \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} &amp; D \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{bmatrix} \\end{array} \\] In each case, if \\(T_M\\) is the linear transformation given by the matrix product \\(T_M(x) = M x\\), where \\(M\\) is the given matrix, then \\(T_M: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a transformation from domain \\(\\mathbb{R}^n\\) to codomain (aka target) \\(\\mathbb{R}^m\\). Determine the appropriate values for \\(n\\) and \\(m\\), and decide whether \\(T_M\\) is one-to-one and/or onto. Submit your answers in table form, as shown below. \\[ \\begin{array} {|c|c|c|c|c|c|} \\hline &amp; n &amp; m &amp; \\text{one-to-one?} &amp; \\text{onto?} &amp; \\text{rank}=dim(Col(A)) &amp; \\text{nullity}=dim(Nul(A)) \\\\ \\hline T_A &amp;\\phantom{\\Big\\vert XX}&amp;\\phantom{\\Big\\vert XX}&amp;&amp; \\\\ \\hline T_B &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline T_C &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline T_D &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline \\end{array} \\hskip5in \\] 23.2.2 Suppose that \\(A\\) is an \\(n \\times n\\) matrix with the property that \\(A \\mathsf{u} = A\\mathsf{v}\\) for \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\) with \\(\\mathsf{u} \\not= \\mathsf{v}\\). Decide if the following are true or false. Justify each answer, briefly. \\(Nul(A) = \\{0\\}\\) \\(Col(A) = \\mathbb{R}^n\\). The columns of \\(A\\) are linearly independent. The columns of \\(A\\) form a basis of \\(\\mathbb{R}^n\\). 23.2.3 Suppose that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) are four vectors in \\(\\mathbb{R}^4\\) and that there is a vector \\(\\mathsf{v} \\in \\mathbf{R}^4\\) such that \\(\\mathsf{v}\\) can be expressed in two different ways as linear combinations of these vectors: \\[ \\begin{array}{rrrrrrrrr} \\mathsf{v} &amp;=&amp; 2 \\mathsf{v}_1 &amp;+&amp; 7 \\mathsf{v}_2 &amp;+&amp; 5 \\mathsf{v}_3 &amp;+&amp; (-5) \\mathsf{v}_4 \\end{array} \\] and \\[ \\begin{array}{rrrrrrrrr} \\mathsf{v} &amp;=&amp; 3 \\mathsf{v}_1 &amp;+&amp; 5 \\mathsf{v}_2 &amp;+&amp; (-2) \\mathsf{v}_3 &amp;+&amp; \\mathsf{v}_4. \\end{array} \\] Use this information to show that \\(\\{\\mathsf{v}_1,\\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v_4}\\}\\) is a linearly dependent set by finding a dependence relation among these vectors. 23.2.4 Let \\(U\\) and \\(W\\) be subspaces of a vector space \\(\\mathbb{R}^n\\). Prove or disprove the following statements. Prove them by showing that the conditions are being a subspace are satisfied. Disprove them with a specific counter example. \\(U \\cap W = \\{ \\mathsf{v} \\in \\mathbb{R}^n \\mid \\mathsf{v} \\in U \\mbox{ and } \\mathsf{v} \\in W \\}\\) is a subspace \\(U \\cup W = \\{ \\mathsf{v} \\in \\mathbb{R}^n \\mid \\mathsf{v} \\in U \\mbox{ or } \\mathsf{v} \\in W \\}\\) is a subspace \\(U+W = \\{\\mathsf{u} + \\mathsf{w} \\mid \\mathsf{u} \\in U \\mbox{ and } \\mathsf{w} \\in W \\}\\) is a subspace 23.2.5 I have performed some row operations below for you on a matrix \\(A\\). Find a basis for the column space and the null space of \\(A\\). \\[ A= \\left[ \\begin{matrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{matrix}\\right] \\longrightarrow \\left[ \\begin{matrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{matrix}\\right] \\] 23.2.6 Consider the matrix \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 5 &amp; 2 &amp; -4 \\\\ 3 &amp; 10 &amp; 2 &amp; 8 \\\\ 4 &amp; 15 &amp; 4 &amp; 4 \\end{array} \\right] \\] Find a basis for \\(\\mathrm{Col}(A)\\). Find a basis for \\(\\mathrm{Nul}(A)\\). 23.2.7 Are the vectors in \\({\\mathcal B}\\) a basis of \\(\\mathbb{R}^3\\)? If not, find a basis of the span of those vectors. Explain your reasoning. \\[ \\mathcal{B}=\\left\\{ \\begin{bmatrix} 1 \\\\ -1 \\\\ -2 \\end{bmatrix},\\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix},\\begin{bmatrix} -1 \\\\ -1 \\\\ -8 \\end{bmatrix} \\right\\} \\] 23.2.8 Find the coordinates of \\(\\mathsf{w}\\) in the standard basis and of \\(\\mathsf{v}\\) in the \\(\\mathcal{B}\\)-basis. \\[ \\mathcal{B} = \\left\\{ \\mathsf{v}_1=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_2=\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_3=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_4=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right\\}, \\] \\[ \\mathsf{w} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\\\ -1 \\end{bmatrix}_{\\mathcal{B}}, \\qquad \\mathsf{v} = \\begin{bmatrix} 10 \\\\ 9 \\\\ 7 \\\\ 4 \\end{bmatrix}_{\\mathcal{S}} \\] 23.2.9 The subspace \\(S \\subset \\mathbb{R}^5\\) is given by \\[ \\mathsf{S} = \\mathsf{span} \\left( \\begin{bmatrix}1\\\\ 1\\\\ 0\\\\ -1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ 1\\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 3\\\\ 1\\\\ -2\\\\ -5\\\\ 4 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 0\\\\ 1\\\\ 0\\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 2\\\\ -1\\\\ -1\\\\ -3\\\\ 1 \\end{bmatrix}, \\right)\\] Use the following matrix to find a basis for \\(S\\). What is the dimension of \\(S\\)? \\[ A=\\left[ \\begin{array}{ccccc} 1 &amp; 0 &amp; 3 &amp; 1 &amp; 2 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; -1 \\\\ -1 &amp; 1 &amp; -5 &amp; 0 &amp; -3 \\\\ 2 &amp; 1 &amp; 4 &amp; 1 &amp; 1 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{ccccc} 1 &amp; 0 &amp; 3 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -2 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Find a basis for \\(\\mathrm{Nul}(A)\\). What is the dimension of this null space? 23.2.10 A \\(6 \\times 8\\) matrix \\(A\\) has rank 5. For each of \\(\\mathrm{Col}(A)\\) and \\(\\mathrm{Nul}(A)\\), Determine the dimension of the subspace, Indicate whether it is subspace of \\(\\mathbb{R}^6\\) or \\(\\mathbb{R}^8\\), and Describe how you would find a basis of the subspace. 23.2.11 Here is a matrix \\(A\\) and \\(rref(A)\\). ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 1 1 1 6 3 ## [2,] 1 2 -1 -2 6 2 ## [3,] 0 1 1 0 3 2 ## [4,] 1 2 -1 -2 6 2 ## [5,] -1 1 1 -1 0 1 ## [6,] 2 2 -1 -1 9 3 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 1 3 1 ## [2,] 0 1 0 -1 2 1 ## [3,] 0 0 1 1 1 1 ## [4,] 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 ## [6,] 0 0 0 0 0 0 Here is \\(A^T\\) and \\(rref(A^T)\\). ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 1 0 1 -1 2 ## [2,] 1 2 1 2 1 2 ## [3,] 1 -1 1 -1 1 -1 ## [4,] 1 -2 0 -2 -1 -1 ## [5,] 6 6 3 6 0 9 ## [6,] 3 2 2 2 1 3 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 -1 1 ## [2,] 0 1 0 1 0 1 ## [3,] 0 0 1 0 2 -1 ## [4,] 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 ## [6,] 0 0 0 0 0 0 Give two bases of \\(Col(A)\\). One of the vectors below is in \\(Col(A)\\) and one is not. For the vector that is in \\(Col(A)\\) give the coordinates of the vector with respect to one of your bases. \\[ \\mathsf{u} = \\begin{bmatrix} 5\\\\ 7\\\\ 4\\\\ 7 \\\\ 3\\\\ 8 \\end{bmatrix}, \\qquad \\mathsf{v} = \\begin{bmatrix} 5\\\\ 4\\\\ 3\\\\ 2 \\\\ 1\\\\ 1 \\end{bmatrix}. \\] 23.2.12 Suppose that \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) and \\(S: \\mathbb{R}^n \\to \\mathbb{R}^m\\) are linear transformations. Let \\(V \\subset \\mathbb{R}^n\\) be the set \\[ V = \\{ \\mathsf{v} \\in \\mathbb{R}^n\\mid T(\\mathsf{v}) = S(\\mathsf{v}) \\}. \\] Prove that the set \\(V\\) is a subspace. 23.3 Solutions to Practice Problems 23.3.1 \\[ \\begin{array} {|c|c|c|c|c|} \\hline &amp; n &amp; m &amp; \\text{one-to-one?} &amp; \\text{onto?} &amp; \\text{onto?} &amp; \\text{rank}=dim(Col(A)) &amp; \\text{nullity}=dim(Nul(A)) \\\\ \\hline T_A &amp;5&amp;5&amp; No &amp; No \\\\ \\hline T_B &amp;4&amp;5&amp;Yes&amp; No\\\\ \\hline T_C &amp;4&amp;4&amp;Yes&amp;Yes \\\\ \\hline T_D &amp;4&amp;3&amp;No&amp;Yes\\\\ \\hline \\end{array} \\hskip5in \\] 23.3.2 All are false! 23.3.3 Hint: subtract the two representations of \\(\\mathsf{v}\\). 23.3.4 True Since \\(U\\) and \\(W\\) are subspaces, we know that \\(\\mathbb{0} \\in U\\) and \\(\\mathbb{0} \\in W\\). Therefore \\(\\mathbb{0} \\in U \\cap W\\). Let \\(\\mathsf{v}_1 \\in U \\cap W\\) and \\(\\mathsf{v}_2 \\in U \\cap W\\). We know that \\(\\mathsf{v}_1 \\in U\\) and \\(\\mathsf{v}_2 \\in U\\). Since \\(U\\) is a subspace, we have \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in U\\). We know that \\(\\mathsf{v}_1 \\in W\\) and \\(\\mathsf{v}_2 \\in W\\). Since \\(W\\) is a subspace, we have \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in W\\). Therefore \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in U \\cap W\\). Let \\(\\mathsf{v} \\in U \\cap W\\) and \\(c \\in \\mathbb{R}\\). We know that \\(\\mathsf{v} \\in U\\) and \\(c \\in R\\). Since \\(U\\) is a subspace, we have \\(c \\mathsf{v} \\in U\\). We know that \\(\\mathsf{v} \\in W\\) and \\(c \\in R\\). Since \\(W\\) is a subspace, we have \\(c \\mathsf{v} \\in W\\). Therefore \\(c \\mathsf{v} \\in U \\cap W\\). False. Here is an example that shows this is not always true. Let \\(V= \\mathbb{R}^2\\), \\(U = \\{ { x \\choose 0} \\mid x \\in \\mathbb{R} \\}\\) and \\(W= \\{ { 0 \\choose y} \\mid y \\in \\mathbb{R} \\}\\). The set \\(U \\cup W\\) is not closed under addition. For example, \\({1 \\choose 0} + {0 \\choose 1} = { 1 \\choose 1} \\notin U \\cup W\\). True. Since \\(U\\) and \\(W\\) are subspaces, we know that \\(\\mathbb{0} \\in U\\) and \\(\\mathbb{0} \\in W\\). Therefore \\(\\mathbb{0} = \\mathbb{0} + \\mathbb{0} \\in U + W\\). Let \\(\\mathsf{u}_1 + \\mathsf{w}_1 \\in U + W\\) and \\(\\mathsf{u}_1 + \\mathsf{w}_2 \\in U + W\\), where \\(\\mathsf{u}_1, \\mathsf{u}_2 \\in U\\) and \\(\\mathsf{w}_1, \\mathsf{w}_2 \\in W\\). Then \\[ (\\mathsf{u}_1 + \\mathsf{w}_1) + (\\mathsf{u}_2 + \\mathsf{w}_2) = (\\mathsf{u}_1 + \\mathsf{u}_2) + (\\mathsf{w}_1 + \\mathsf{w}_2) \\] and \\(\\mathsf{u}_3 = (\\mathsf{u}_1 + \\mathsf{u}_2) \\in U\\) (because \\(U\\) is a subspace) and \\(\\mathsf{w}_3 = (\\mathsf{w}_1 + \\mathsf{w}_2) \\in W\\) (because \\(W\\) is a subspace). Therefore \\((\\mathsf{u}_1 + \\mathsf{v}_1) + (\\mathsf{u}_2 + \\mathsf{w}_2) = \\mathsf{u}_3 + \\mathsf{w}_3 \\in U + W\\). Let \\(\\mathsf{u} + \\mathsf{w} \\in U + W\\) and \\(c \\in \\mathbb{R}\\). Then \\(c(\\mathsf{u} + \\mathsf{w}) = c \\mathsf{u} + c \\mathsf{w}\\). We know that \\(c \\mathsf{u} \\in U\\) (since \\(U\\) is a subspace) and \\(c \\mathsf{w} \\in W\\) (since \\(W\\) is a subspace). Therefore \\(c(\\mathsf{u} + \\mathsf{w}) = c \\mathsf{u} + c \\mathsf{w} \\in U+W\\). 23.3.5 A basis for \\(\\mathrm{Col}(A)\\) is \\[ \\begin{bmatrix} 1 \\\\1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\1 \\\\ -2 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\0 \\\\ 1 \\\\ -1 \\end{bmatrix} \\] and a basis for \\(\\mathrm{Nul}(A)\\) is \\[ \\begin{bmatrix} -2 \\\\1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} -2 \\\\0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\0 \\\\ 1\\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] 23.3.6 Using R we find: ## [,1] [,2] [,3] [,4] ## [1,] 1 5 2 -4 ## [2,] 3 10 2 8 ## [3,] 4 15 4 4 ## [,1] [,2] [,3] [,4] ## [1,] 1 0 -2.0 16 ## [2,] 0 1 0.8 -4 ## [3,] 0 0 0.0 0 A basis for \\(\\mathrm{Col}(A)\\) is \\[ \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\quad \\begin{bmatrix} 5 \\\\ 10 \\\\ 15 \\end{bmatrix}. \\] A basis for \\(\\mathrm{Nul}(A)\\) is \\[ \\begin{bmatrix} 2 \\\\ -0.8 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} -16 \\\\ 4 \\\\ 0 \\\\ 1 \\end{bmatrix}. \\] 23.3.7 A = cbind(c(1,-1,-2),c(2,-1,1),c(-1,-1,-8)) A ## [,1] [,2] [,3] ## [1,] 1 2 -1 ## [2,] -1 -1 -1 ## [3,] -2 1 -8 rref(A) ## [,1] [,2] [,3] ## [1,] 1 0 3 ## [2,] 0 1 -2 ## [3,] 0 0 0 The vectors are linearly dependent. If we choose the first two vectors \\(\\mathcal{B} = \\{ \\mathsf{v}_1, \\mathsf{v}_2\\}\\), then \\(\\mathcal{B}\\) is a basis for \\(S = \\mathsf{span}(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3)\\). 23.3.8 We use the change of basis matrix. \\[ P_{\\cal B} = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Then, the desired coordinate vectors are \\[ \\mathsf{w} = \\begin{bmatrix} 0 \\\\ -3 \\\\ -1 \\\\ -1 \\end{bmatrix}_{\\mathcal{S}}, \\qquad \\mathsf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}_{\\mathcal{B}} \\] You can find these vectors by multiplying by \\(P_\\mathcal{B}\\) and by augmenting and row reducing as seen here. A = cbind(c(1,0,0,0),c(1,1,0,0),c(1,1,1,0),c(1,1,1,1)) w = c(3,-2,0,-1) v = c(10,9,7,4) A %*% w ## [,1] ## [1,] 0 ## [2,] -3 ## [3,] -1 ## [4,] -1 Av = cbind(A,v) rref(Av) ## v ## [1,] 1 0 0 0 1 ## [2,] 0 1 0 0 2 ## [3,] 0 0 1 0 3 ## [4,] 0 0 0 1 4 Or we can use the inverse of \\(P_\\mathcal{B}\\). \\[ P_{\\cal B}^{-1} = \\begin{bmatrix} 1 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp;-1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Ainv = solve(A) Ainv %*% v ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 23.3.9 \\(\\dim(S) = 3\\) and a basis for \\(S\\) is \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ -1 \\\\2 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\1 \\end{bmatrix}. \\] \\(\\dim(\\mathrm{Nul}(A))=2\\) and a basis is \\[ \\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\\\ 0 \\\\0\\end{bmatrix}, \\quad \\begin{bmatrix} -1 \\\\ 2 \\\\ 0 \\\\ -1 \\\\1 \\end{bmatrix}. \\] 23.3.10 \\(\\mathrm{Col}(A)\\) has dimension 5, and it is a subspace of \\(\\mathbb{R}^6\\). You would find a basis by taking the pivot columns of \\(A\\). \\(\\mathrm{Nul}(A)\\) has dimension 3, and it is a subspace of \\(\\mathbb{R}^8\\). You would find a basis by finding the parametric solution to \\(A \\mathsf{x}= \\mathbb{0}\\). "],["exam-3-review.html", "Section 24 Exam 3 Review 24.1 Overview 24.2 Vocabulary, Concepts and Skills 24.3 Practice Problems 24.4 Solutions to Practice Problems", " Section 24 Exam 3 Review 24.1 Overview Our third exam covers eigenvectors and eigenvalues and a little bit of orthogonality. This covers sections 5.1-5.6 and 6.1-6.2. The exam will be in the same style as the first two exams: I will hand it out right away, even a few minutes early, so you can start right on time. You must turn it in by 9:35 (for the 8:30 class) and 10:45 for the 10:50 class). No exceptions. The next class needs to come in and get started. No calculators are allowed, and none are needed. If row reductions are needed, they will be easy (integer) calculations, and there will not be many. It will be closed book but you can bring a 3\" x 5\" note card with notes written on both sides. These notes should be hand written by you. You do not need to turn in the note card with the exam. There will be some basic calculations, but the problems will focus more on the ideas than on the calculations. I will ask some problems that are very similar to homework problems, Edfinity problems, and examples from class or the videos. I will ask other problems that are somewhat different from things you have done. On these, you are to apply your knowledge in a slightly new setting to demonstrate an even higher mastery of the material. You will be allowed to re-write one problem to earn back half of the points that you lost. This will be due the class period after I hand the exam back. As outlined in the syllabus, syllabus, in your course grade, 3 semester exams will be weighted 13%+13% + 10% = 36% so that your lowest of the three quizzes counts for 10% and your other two scores count 13%. The best way to study is to do practice problems. The Exam will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assingments Look over and redo the class examples problems. Look at class examples that we didn’t get to. Try to resolve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 24.2 Vocabulary, Concepts and Skills Here are the knowledge and skills you should master by the end of the fifth and sixth weeks. 24.2.1 Skills Check whether a given vector \\(\\mathsf{v}\\) is an eigenvector for square matrix \\(A\\). Find the eigenvalues of a matrix \\(2 \\times 2\\) matrix by hand, using the characteristic equation Find the eigenvalues of a triangular matrix by inspection. Given the eigenvalues of matrix \\(A\\), find the eigenvectors by solving \\((A - \\lambda I) = \\mathbf{0}\\). Find the eigenvalues and eigenvectors of an \\(n \\times n\\) matrix \\(A\\) by reading RStudio output. Determine whether a matrix is diagonalizable. Factor a diagonalizable \\(n \\times n\\) matrix as \\(A = PDP^{-1}\\) where \\(D\\) is a diagonal matrix of eigenvalues and \\(P\\) is the matrix whose columns are the corresponding eigenvectors. You only have to compute \\(P^{-1}\\) in the 2x2 case. Compute matrix powers using the diagonalization. Given the eigenvalues and eigenvectors, factor a \\(2 \\times 2\\) matrix with complex eigenvalues as \\(A = P R P^{-1}\\) where \\(R\\) is a rotation-dilation matrix \\(\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix}\\) and \\(P = [ \\mathsf{w}, \\mathsf{u}]\\) where \\(\\mathsf{v} = \\mathsf{u} + i \\mathsf{w}\\) is the eigenvector for \\(\\lambda = a + b i\\). Find the angle of rotation and the scaling factor in a \\(2 \\times 2\\) matrix with complex eigenvalues. Use the dominant eigenvalue and dominant eigenvector to determine the long-term behavior of a dynamical system. Use eigenvalues to investigate a population modeled with a Leslie matrix. Give a close-formula for a dynamical system using the eigen decomposition of a matrix Find the length of a vector. Find the distance betwen two vectors. Find the cosine of the angle between two vectors. Find the orthogonal complement of a vector space. Find the coordinates of a vector in an orthogonal basis. 24.2.2 Vocabulary I should know and be able to use and explain the following terms or properties. eigenvalue, eigenvector and eigenspace characteristic equation diagonalizable matrix similar matrices algebraic multiplicity of an eigenvalue geometric multiplicity of an eigenvalue rotation-dilation matrix discrete dynamical system trajectory dominant eigenvalue and dominant eigenvector population model Leslie matrix 24.2.3 Conceptual Thinking I should understand and be able to explain the following concepts: An eigenspace of \\(A\\) is a subspace that is fixed under the linear transformation \\(T(\\mathsf{x}) = A \\mathsf{x}\\). An eigenvalue \\(\\lambda\\) with \\(1 &lt;| \\lambda |\\) corresponds to expansion. An eigenvalue \\(\\lambda\\) with \\(0 &lt; | \\lambda | &lt; 1\\) corresponds to contraction. A complex eigenvalue corresponds to a rotation in a 2D subspace. The eigenspace for \\(\\lambda\\) is the subspace \\(E_\\lambda = \\mathrm{Nul}(A - \\lambda I)\\). A matrix is not diagonalizable when it has an eigenvalue whose algebraic multiplicity is larger than its geometric multiplicity. The long-term behavior of a dynamical system is determined by its dominant eigenvalue and eigenvector. Population model predicts one of: long term growth, extinction, convergence to a stable population. 24.3 Practice Problems 24.3.1 Consider the \\(3 \\times 3\\) matrix \\[ A = \\left[ \\begin{array}{rrr} 2 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -2 &amp; 5 &amp; -2 \\\\ \\end{array} \\right] \\] with characteristic equation \\[ p(\\lambda) = -(\\lambda -1)(\\lambda -2)(\\lambda +2). \\] Find the eigenvalues and corresponding eigenvectors for \\(A\\). 24.3.2 Let \\(A\\) be a \\(2 \\times 2\\) matrix. We view \\(A\\) as a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). Describe the eigenvalues for each of the following types of matrices. \\(A\\) maps all of \\(\\mathbb{R}^2\\) onto a line through the origin in \\(\\mathbb{R}^2\\). \\(A\\) is a reflection of \\(\\mathbb{R}^2\\) over the line \\(y = x\\). \\(A\\) is a reflection of \\(\\mathbb{R}^2\\) through the origin; that is, it sends \\((x,y)\\) to \\((-x,-y)\\). \\(A\\) is a horizontal shear of the form d form \\[ \\begin{bmatrix} 1 &amp; a \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x + a y \\\\ y \\end{bmatrix} \\] 24.3.3 Below are the eigenvalues of four different \\(5 \\times 5\\) matrices. For each, decide if the matrix is invertible and if it is diagonalizable. Answer Yes, No or “Not enough information to determine this.” \\(A\\) has eigenvalues \\(\\lambda = -4, -3,0,1, 2\\) \\(B\\) has eigenvalues \\(\\lambda = -3, -1, 1, \\sqrt{2}, 8.\\) \\(C\\) has eigenvalues \\(\\lambda = 1, 2, 2, 7, 8.\\) \\(D\\) has eigenvalues \\(\\lambda = -1, 0, 3,3, 10\\) 24.3.4 Here the diagonalization of a matrix: \\[ \\mathsf{A}=\\left[ \\begin{array}{ccc} 5 &amp; 2 &amp; -1 \\\\ 2 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\\\ \\end{array} \\right] = \\left[ \\begin{array}{ccc} -5 &amp; 0 &amp; 1 \\\\ -2 &amp; 1 &amp; -2 \\\\ 1 &amp; 2 &amp; 1 \\\\ \\end{array} \\right] \\left[ \\begin{array}{ccc} 6 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\left[ \\begin{array}{ccc} -\\frac{1}{6} &amp; -\\frac{1}{15} &amp; \\frac{1}{30} \\\\ 0 &amp; \\frac{1}{5} &amp; \\frac{2}{5} \\\\ \\frac{1}{6} &amp; -\\frac{1}{3} &amp; \\frac{1}{6} \\\\ \\end{array} \\right]. \\] Is the matrix \\(\\mathsf{A}\\) invertible? Find a nonzero vector in \\(\\mathrm{Nul}(\\mathsf{A})\\) if one exists. Find a steady-state vector \\(\\mathsf{v}\\) such that \\(\\mathsf{A} \\mathsf{v} = \\mathsf{v}\\) if one exists. Give the coordinates of \\(\\mathsf{v} = [1,2,3]^T\\) in the eigenbasis without row reductions. Find a formula for \\(\\mathsf{A}^{2021} \\mathsf{v}\\) if \\(\\mathsf{v} = [1,2,3]^T\\) in terms of the eigenbasis. 24.3.5 The eigensystem of matrix \\(A\\) is given below. It has complex eigenvalues. \\[ \\begin{bmatrix} 3 &amp; -5 \\\\ 1 &amp; -1 \\end{bmatrix}, \\qquad \\lambda = 1 \\pm i, \\qquad v = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\pm \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} i. \\] a. What angle does it rotate by? b. What factor does it scale by? c. Factor it as \\(A = P R P^{-1}\\) where \\(R\\) is a rotation-dilation matrix. 24.3.6 Using the matrix \\(B = \\begin{bmatrix} .97 &amp; -.71 \\\\ .71 &amp; .97 \\end{bmatrix}\\) and the starting vector \\(\\mathsf{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), I plotted the points \\[\\mathsf{v}, B \\mathsf{v}, B^2\\mathsf{v}, B^3 \\mathsf{v}, \\ldots.\\] I saw that these points are, roughly, going around in a circle. How many multiplications by \\(B\\) does it take to get back around to the positive \\(x\\)-axis? When I come full circle, am I closer to the origin, farther from the origin, or the same distance to the origin? 24.3.7 For each matrix below, decide if it is diagonalizable. You do not need to diagonalize the matrix (though you can!), but you must give a reason for why the matrix is or is not diagonalizable. \\(A = \\begin{bmatrix} 0 &amp; -4 &amp; 2 \\\\ 2 &amp; -4 &amp; -1 \\\\ -6 &amp; 4 &amp; 7 \\end{bmatrix}\\) has eigenvalues \\(4, -1, 0\\). \\(B = \\begin{bmatrix} 3 &amp; -1 &amp; 2 \\\\ -1 &amp; 3 &amp; 2 \\\\ 2&amp;2 &amp; 0 \\end{bmatrix}\\) has eigenvalues \\(4,4,-2\\). 24.3.8 Consider the matrix with eigenvalues and eigenvectors \\[ A = \\begin{bmatrix} 0.7 &amp; 0.2 \\\\ 0.3 &amp; 0.8 \\end{bmatrix} \\qquad \\begin{array}{cc} \\lambda_1 = 1 &amp; \\lambda_2 = .5 \\\\ \\mathsf{v}_1 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} &amp; \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\end{array} \\] Diagonalize \\(A\\). What can you say about \\(\\displaystyle{\\lim_{n \\to \\infty}} A^n\\)? Give a formula for \\(A^n \\mathsf{x}_0\\) if \\(\\mathsf{x}_0 = \\begin{bmatrix} 25 \\\\ 0 \\end{bmatrix}\\) in terms of the eigenbasis. What is \\(\\displaystyle{\\lim_{n \\to \\infty}} A^n \\begin{bmatrix} 25 \\\\ 0 \\end{bmatrix}\\)? 24.3.9 The matrix \\(A\\) below has the given eigenvalues and eigenvectors. \\[ A = \\left[ \\begin{array}{cc} \\frac{1}{2} &amp; \\frac{1}{5} \\\\ -\\frac{2}{5} &amp; \\frac{9}{10} \\\\ \\end{array} \\right] \\qquad \\begin{array}{c} \\lambda = .7 \\pm .2 i \\\\ \\mathsf{v} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\end{bmatrix} \\pm \\begin{bmatrix} -\\frac{1}{2} \\\\ 0 \\end{bmatrix} i \\end{array}\\hskip5in \\] Factor \\(A=PRP^{-1}\\) where \\(R\\) is a rotation-dilation matrix. What is the angle of rotation? What is the factor of dilation? 24.3.10 In a 1962 study of rainfall in Tel Aviv, it was determined that if today is a wet day, then the probability that tomorrow will be wet is 0.662 and the probability that tomorrow it will be dry is 0.338. If today is a dry day, then the probability that tomorrow is wet is 0.250 and the probability that tomorrow is dry will be 0.75. From this I computed the following: \\[ A = \\begin{bmatrix} 0.662 &amp; 0.25 \\\\ 0.338 &amp; 0.75\\end{bmatrix}; \\qquad \\begin{array}{cc} \\lambda_1 = 1.0 &amp; \\lambda_2 = 0.412 \\\\ \\mathsf{v}_1 = \\begin{bmatrix}-0.595 \\\\ -0.804 \\end{bmatrix} &amp; \\quad \\mathsf{v}_2 = \\begin{bmatrix}-0.707\\\\ 0.707 \\end{bmatrix} \\end{array} \\] If Monday is a dry day, what is the probability that Wednesday will be wet? In the long-run, what is the distribution of wet and dry days? 24.3.11 A population of female bison is split into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years. Each year, * 80% of the juveniles survive to become yearlings. * 90% of the yearlings survive to become adults. * 80% of the adults survive. * 40% of the adults give birth to a juvenile Let \\(\\mathsf{x}_t = \\begin{bmatrix} J_t \\\\ Y_t \\\\ A_t \\end{bmatrix}\\) be the state of the system in year \\(t\\). Find the Leslie matrix \\(L\\) such that \\(\\mathsf{x}_{t+1} = B \\mathsf{x}_t.\\). Find the eigenvalues of \\(L\\). The matrix \\(L\\) has two complex eigenvalues and one real eigenvalue. How do the complex eigenvectors manifest in the trajectory of a population? What is the long-term behavior of the herd? Will the size of the herd grow, stablilize or shrink? What will be the proportions of juveniles, yearlings and adults in the herd? 24.3.12 Let \\(A\\) and \\(B\\) be \\(n \\times n\\) matrices. Suppose that \\(v\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) and \\(v\\) is an eigenvector of \\(B\\) with eigenvalue \\(\\mu\\) such that \\(\\lambda \\not= \\mu\\). Is \\(v\\) an eigenvector of either of the matrices below? If so give its eigenvalue. \\(A + B\\) \\(AB\\) 24.3.13 Suppose that \\(A\\) is invertible. Show that if \\(v\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\), then \\(v\\) is an eigenvector of \\(A^{-1}\\) with eigenvalue \\(1/\\lambda\\). If \\(A\\) is diagonalizable with diagonalization \\(A = P D P^{-1}\\), then show that \\(A^{-1}\\) is diagonalizable and find its diagonalization from that of \\(A\\). 24.3.14 Suppose that \\(A\\) is an \\(n \\times n\\) matrix with eigenvector \\(\\vec w\\) of eigenvalue 5 and eigenvector \\(\\vec v\\) of eigenvalue -3. Is \\(\\vec v + \\vec w\\) an eigenvector of \\(A\\), and if so, what is its eigenvalue? Is \\(2021 \\vec v\\) an eigenvector of \\(A\\), and if so what is its eigenvalue? Is \\(\\vec w\\) an eigenvector of \\(A^2\\), and if so what is its eigenvalue? Is \\(\\vec v\\) an eigenvector of \\(A - 2021 I_n\\) and if so, what is its eigenvalue? 24.3.15 \\(A\\) is a \\(2 \\times 2\\) matrix that sends \\(v\\) to \\(A v\\) as shown in the plot below with its two eigenspaces \\(E_{\\lambda_1}\\) and \\(E_{\\lambda_2}\\). Estimate, as accurately as possible from the given information, the eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\). Indicate on the plot above where \\(A^2 v\\) will be. What happens in the limit: \\(\\displaystyle{\\lim_{n\\to \\infty} }A^n v\\)? If \\(A^n v = \\begin{bmatrix} x_n \\\\ y_n \\end{bmatrix}\\) what happens to the ratio \\(x_n/y_n\\) as \\(n\\) grows larger and larger? 24.3.16 Consider vectors \\(\\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\-1 \\end{bmatrix}\\) and \\(\\mathsf{v}_2= \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\) in \\(\\mathbb{R}^3\\). Let \\(W=\\mbox{span}(\\mathsf{v}_1, \\mathsf{v}_2)\\). Show that \\(\\mathsf{v}_1\\) and \\(\\mathsf{v}_2\\) are orthogonal. Find a basis for \\(W^{\\perp}\\). 24.3.17 Let \\(W\\) be the span of the vectors \\[ \\begin{bmatrix} 1 \\\\ -2 \\\\ 1 \\\\ 0 \\\\1 \\end{bmatrix}, \\quad \\begin{bmatrix} -1 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 3 \\\\1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\\\4 \\end{bmatrix} \\] Find a basis for \\(W\\). What is the dimension of this subspace? Find a basis for \\(W^{\\perp}\\) 24.3.18 Let \\(P\\) be the vector space of palendromic vectors from Exam 2. See below. Find \\(P^\\perp\\). \\[ P = \\left\\{\\ \\begin{bmatrix} a \\\\ b \\\\ c \\\\ b \\\\ a \\end{bmatrix} \\quad \\mid \\quad a,b,c \\in \\mathbb{R}\\ \\right\\}. \\] 24.3.19 In the Fibonacci vector space problem, we use the basis below \\[ F = span \\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\right\\} \\] What is the cosine of the angle between the two basis vectors? What is the distance between the two basis vectors? 24.3.20 Find a basis for the plane that is orthogonal to the vector \\[ v = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}. \\] 24.3.21 What does the following matrix product tell us? \\[ \\underbrace{\\left[\\begin{array}{cccc} 1 &amp; 1 &amp; 2 &amp; 3 \\\\ 8 &amp; 8 &amp; 1 &amp; -6 \\\\ -6 &amp; 5 &amp; 2 &amp; -1 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 \\\\ \\end{array}\\right]}_{A^T} \\underbrace{\\left[\\begin{array}{cccc} 1 &amp; 8 &amp; -6 &amp; 0 \\\\ 1 &amp; 8 &amp; 5 &amp; 1 \\\\ 2 &amp; 1 &amp; 2 &amp; -2 \\\\ 3 &amp; -6 &amp; -1 &amp; 1 \\\\ \\end{array}\\right]}_A= \\left[\\begin{array}{cccc} 15 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 165 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 66 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 6 \\\\ \\end{array} \\right] \\] Use the dot-product formula to find the coordinates of the vector \\(v = (1,1,1,1)^T\\) with respect to the vectors in the columns of \\(A\\). 24.4 Solutions to Practice Problems 24.4.1 There are three eigenvalues: 1, 2, and \\(-2\\). We find an eigenvector for each of them. * Eigenvalue \\(\\lambda = 1\\) \\[ A - I = \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ -2 &amp; 5 &amp; -3 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 3 &amp; -3 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([1,1,1]^{\\top}\\) Eigenvalue \\(\\lambda = 2\\) \\[ A - 2I = \\left[ \\begin{array}{rrr} 0 &amp; -1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ -2 &amp; 5 &amp; -4 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} -2 &amp; 5 &amp; -4 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} -2 &amp; 0 &amp; -4 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([-2,0,1]^{\\top}\\) Eigenvalue \\(\\lambda = -2\\) \\[ A - 2I = \\left[ \\begin{array}{rrr} 4 &amp; -1 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ -2 &amp; 5 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -2 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([0,0,1]^{\\top}\\) 24.4.2 In this problem, we are to think about the geometry of a 2D transformation, and see if we can find any vectors which get re-scaled by the transformation. The direction of these vectors cannot change (other than to flip to the opposite direction). This maps all of \\(\\mathbb{R}^2\\) to a line. Therefore it is not one-to-one, nor onto, and so it is not invertible. This means that \\(\\lambda = 0\\) is an eigenvalue Any vector that is already on the line must stay on the line, so it is an eigenvector, but we don’t know its eigenvalue. Thus, the eigenvalues are \\(\\lambda_1 = 0\\) and \\(\\lambda_2\\) we don’t know. There are two kinds of eigenvectors. Those vectors on the line are fixed, so they are eigenvectors of eigenvalue 1. Vectors that are perpendicular to the line get sent to their negatives, so they are eigenvectors of eigenvalue \\(-1\\). Thus, the eigenvalues are \\(\\lambda_1 = 1\\) and \\(\\lambda_2=-1\\). In this transformation, every vector gets sent to its negative. \\[ T\\left( \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\right) = \\begin{bmatrix} -x_1 \\\\ -x_2 \\end{bmatrix} = \\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\] This means that every vector is an eigenvector of eigenvalue \\(-1\\). The eigenvalues are \\(\\lambda_1 = \\lambda_2=-1\\). A horizontal shear (we did not talk about these very much) has a matrix of the form \\[ \\begin{bmatrix} 1 &amp; a \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x + a y \\\\ y \\end{bmatrix} \\] It fixes the \\(x\\)-axis since \\((x,0)^\\top\\) maps to \\((x,0)^\\top\\), but no other directions are fixed. You can see by the fact that the matrix is upper triangluar that the eigenvalues are on the diagonal and are \\(\\lambda_1 = \\lambda_2 = 1\\). Note: if you calculate, you find that the geometric multiplicity of \\(\\lambda = 1\\) is 1 (only the \\(x\\)-axis), and this matrix is not diagonalizable. The only eigenspace is the \\(x\\)-axis. 24.4.3 \\(A\\) is not invertible because \\(0\\) is an eigenvalue. \\(A\\) is diagonalizable because it have 5 distinct eigenvalues. \\(B\\) is invertible because \\(0\\) is not an eigenvalue. \\(B\\) is diagonalizable because it have 5 distinct eigenvalues. \\(C\\) is invertible because \\(0\\) is not an eigenvalue. We cannot tell whether \\(C\\) is diagonalizable without more information. The eigenvalue \\(\\lambda=2\\) has algebraic multiplicity 2. We need to know whether the geometric multiplicity is 1 or 2. \\(D\\) is not invertible because \\(0\\) is an eigenvalue. We cannot tell whether \\(D\\) is diagonalizable without more information. The eigenvalue \\(\\lambda=3\\) has algebraic multiplicity 2. We need to know whether the geometric multiplicity is 1 or 2. 24.4.4 No, \\(A\\) is not invertible because \\(0\\) is an eigenvalue. \\(\\mathsf{v} = [1, -2, 1]^{\\top}\\) is an eigenvector for \\(\\lambda=0\\). Therefore \\(\\mathsf{v} \\in \\mbox{Nul}(A)\\). The vector \\(\\mathsf{v} = [0,1,2]^{\\top}\\) is an eigenvector for \\(\\lambda=1\\). So this is a steady-state vector. (However, the dynamical system will not converge to this steady state because \\(\\lambda=6\\) is the dominant eigenvalue.) When \\(A=P D P^{-1}\\), we can find the coordinates of a vector with respect to the eigenbasis via multiplication by \\(P^{-1}\\). Pinv =cbind(c(-1/6,0,1/6),c(-1/15,1/5,-1/3),c(1/30,2/5,1/6)) v = c(1,2,3) Pinv %*% v ## [,1] ## [1,] -0.2 ## [2,] 1.6 ## [3,] 0.0 So \\([ \\mathsf{v}]_{\\mathcal{B}} = [-1/5, 8/5, 0]^{\\top}\\). \\(-\\frac{1}{5} \\cdot 6^{2021} \\cdot \\begin{bmatrix} -5 \\\\ -2 \\\\ 1 \\end{bmatrix} + \\frac{8}{5} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}\\) 24.4.5 This system scales by \\(\\sqrt{1+1} = \\sqrt{2}\\) and it rotates by \\(\\tan^{-1} (1/1) = \\pi/4\\). 24.4.6 We have \\[ \\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix} = \\begin{bmatrix} .97 &amp; -.71\\\\ .71 &amp; .97 \\end{bmatrix} \\] Let’s turn to RStudio a = .97 b = .71 (scale = sqrt(a^2+b^2)) ## [1] 1.202082 (angle = atan (b/a)) ## [1] 0.6318544 angle/(2*pi)*360 ## [1] 36.20259 2 * pi / angle ## [1] 9.94404 It takes 10 iterations to rotate past the \\(x\\)-axis. We are further from the origin because \\(| \\lambda| \\approx 1.2 &gt; 1\\). 24.4.7 The matrix \\(A\\) is diagonalizable because it has 3 distinct eigenvalues We must see whether \\(\\lambda=4\\) has geometric multiplicty 2 (to match its algebraic multiplicity). rref( cbind(c(-1,-1,2), c(-1,-1,2), c(2,2,-4))) ## [,1] [,2] [,3] ## [1,] 1 1 -2 ## [2,] 0 0 0 ## [3,] 0 0 0 We see that \\(B - 4I\\) has two free columns, so \\(\\dim ( \\mbox{Nul}(B-4I))=2\\). This means that \\(\\lambda=4\\) has geometric multiplicity 2. Therefore \\(B\\) is diagonalizable. 24.4.8 We set \\(P = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix}\\). So \\[ P^{-1} = - \\frac{1}{5} \\begin{bmatrix} -1 &amp; -1 \\\\ -3 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix} \\] Or we can find this inverse using RStudio. A = cbind(c(2,3),c(1,-1)) solve(A) ## [,1] [,2] ## [1,] 0.2 0.2 ## [2,] 0.6 -0.4 Therefore \\[ A = \\begin{bmatrix} 0.7 &amp; 0.2 \\\\ 0.3 &amp; 0.8 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0.5 \\end{bmatrix} \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix} \\] We compute this as follows: \\[ A^n = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0.5 \\end{bmatrix}^n \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix}= \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 1^n &amp; 0 \\\\ 0 &amp; 0.5^n \\end{bmatrix} \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix} \\] so \\[ \\lim_{n\\to \\infty} A^n = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix} =\\begin{bmatrix} 0.4 &amp; 0.4 \\\\ 0.6 &amp; 0.6 \\end{bmatrix} \\] We need to find the coefficients for \\(x_0 = [25, 0]^{\\top}\\). P = cbind(c(2,3), c(1,-1)) v = c(25,0) solve(P,v) ## [1] 5 15 So the formula is \\[ 5 \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} + 15 \\left( \\frac{1}{2} \\right)^n \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\] This converges to \\(5 \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\). 24.4.9 We have \\[ A = \\left[ \\begin{array}{cc} \\frac{1}{2} &amp; \\frac{1}{5} \\\\ -\\frac{2}{5} &amp; \\frac{9}{10} \\\\ \\end{array} \\right] = \\begin{bmatrix} -1/2 &amp; 1/2 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 0.7 &amp; -0.2 \\\\ 0.2 &amp; 0.7 \\end{bmatrix} \\begin{bmatrix} -2 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} \\] Here are some R calculations to check the answer for (a) and to find the values for (b) and (c). A = cbind(c(1/2,-2/5), c(1/5,9/10)) A ## [,1] [,2] ## [1,] 0.5 0.2 ## [2,] -0.4 0.9 eigen(A) ## eigen() decomposition ## $values ## [1] 0.7+0.2i 0.7-0.2i ## ## $vectors ## [,1] [,2] ## [1,] 0.4082483-0.4082483i 0.4082483+0.4082483i ## [2,] 0.8164966+0.0000000i 0.8164966+0.0000000i P = cbind(c(-1/2,0),c(1/2,1)) C = cbind(c(.7,.2),c(-.2,.7)) Pinv = solve(P) Pinv ## [,1] [,2] ## [1,] -2 1 ## [2,] 0 1 P %*% C %*% Pinv ## [,1] [,2] ## [1,] 0.5 0.2 ## [2,] -0.4 0.9 atan(.2/.7) ## [1] 0.2782997 sqrt(.7^2 + .2^2) ## [1] 0.728011 The angle of rotation is \\(\\tan^{-1} (.2/.7) = 0.278\\) radians The dilation factor is \\(\\sqrt{0.49 + 0.04} = \\sqrt{0.53} = 0.728\\). 24.4.10 Let’s use RStudio. A = cbind(c(0.662, 0.338),c(0.25, 0.75)) A %*% A %*% c(0,1) ## [,1] ## [1,] 0.353 ## [2,] 0.647 v1 = c(-0.595, -0.804 ) v1/sum(v1) ## [1] 0.4253038 0.5746962 If Monday is dry, then the probability of a wet Wednesday is \\(0.353\\). The easiest way to calculate this \\(A^2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\\) In the long run, \\(42.5\\%\\) of days are wet and \\(57.5\\%\\) of days are dry. 24.4.11 Here is the Leslie matrix, as well as some eigensystem computations. L = cbind(c(0,.8,0),c(0,0,.9),c(.4,0,.8)) L ## [,1] [,2] [,3] ## [1,] 0.0 0.0 0.4 ## [2,] 0.8 0.0 0.0 ## [3,] 0.0 0.9 0.8 (vals = eigen(L)$values) ## [1] 1.0575217+0.0000000i -0.1287609+0.5057227i -0.1287609-0.5057227i Mod(vals) ## [1] 1.0575217 0.5218571 0.5218571 vecs = eigen(L)$vectors v = vecs[,1] Re(v/sum(v)) # get it to sum to 1 AND remove the 0 imaginary part ## [1] 0.2272578 0.1719172 0.6008250 The eigenvalues are \\(1.058, -0.129 \\pm 0.506 i\\). The complex eigenvalues have length 0.52, so they shrink away pretty quickly. If we start outside of the span of the dominant eigenvalue, then the trajectory oscillate slightly until it settles into the direction of the dominant eigevector, with an overall growth trend of \\(1.058\\), or \\(5.8\\%\\). The size of the herd grows. The proportions are \\([0.227, 0.172, 0.601]\\). 24.4.12 \\((A + B) v = A v + B v = \\lambda v + \\mu v = (\\lambda + \\mu) v\\), so yes, \\(v\\) is an eigenvector of \\(A+B\\) of eigenvalue \\(\\lambda + \\mu\\). \\(A B v = A (B v) = A (\\mu v) = \\mu (A v) = \\mu \\lambda v\\), so yes, \\(v\\) is an eigenvector of \\(AB\\) of eigenvalue \\(\\lambda\\mu\\). 24.4.13 We are given \\(A v = \\lambda v\\). Thus, \\[ \\begin{array}{cccl} A v &amp; = &amp; \\lambda v &amp; \\text{given} \\\\ A^{-1} A v &amp; = &amp; \\lambda A^{-1} v &amp; \\text{multiply on the left by $A^{-1}$} \\\\ v &amp; = &amp; \\lambda A^{-1} v \\\\ \\frac{1}{\\lambda} v &amp; = &amp; A^{-1} v \\\\ \\end{array} \\] This shows that \\(A^{-1} v = \\frac{1}{\\lambda} v\\) so \\(v\\) is an eigenvector of \\(A^{-1}\\) with eigenvalue \\(\\frac{1}{\\lambda}\\) (method 1) If \\(A\\) is diagonal, then there is a basis \\(\\{v_1, v_2, \\ldots, v_n\\}\\) of eigenvectors of \\(A\\) with eigenvalues \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\). By the previous part, \\(\\{v_1, v_2, \\ldots, v_n\\}\\) are eigenvectors of \\(A^{-1}\\) with eigenvalues \\(1/\\lambda_1, 1/\\lambda_2, \\ldots, 1/\\lambda_n\\). Thus \\(A^{-1}\\) has the same eigenbasis, and the diagonalization of \\(A^{-1}\\) is \\[ A^{-1} = \\underbrace{ \\begin{bmatrix} \\vert &amp;\\vert &amp;&amp;\\vert \\\\ v_1 &amp; v_2 &amp; \\cdots &amp; v_n \\\\ \\vert &amp;\\vert &amp;&amp;\\vert \\\\ \\end{bmatrix} }_P \\begin{bmatrix} 1/\\lambda_1 &amp; &amp; &amp; \\\\ &amp; 1/\\lambda_2 &amp; &amp; \\\\ &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; 1/\\lambda_n \\\\ \\end{bmatrix} P^{-1} \\] (method 2) If \\(A = P D P^{-1}\\) then by the fact that the order reverses when computing inverses (the shoes-and-socks property), we have \\(A^{-1} = (P D P^{-1})^{-1} = (P^{-1})^{-1} D^{-1} P^{-1} = P D^{-1} P^{-1}.\\) Furthermore \\(D^{-1}\\) is a diagonal matrix such that \\[ \\text{if} \\qquad D = \\begin{bmatrix} \\lambda_1 &amp; &amp; &amp; \\\\ &amp; \\lambda_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\lambda_n \\\\ \\end{bmatrix} \\qquad\\text{then}\\qquad D^{-1} = \\begin{bmatrix} 1/\\lambda_1 &amp; &amp; &amp; \\\\ &amp; 1/\\lambda_2 &amp; &amp; \\\\ &amp; &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; &amp; 1/\\lambda_n \\\\ \\end{bmatrix} \\] Note that \\(A\\) is invertible, 0 is not an eigenvalue, so each \\(1/\\lambda_i\\) does not cause division by 0. 24.4.14 We are given that \\(A w = 5 w\\) and \\(A v = -3 v\\). \\(A (v + w) = A v + A w = -3 v + 5 w \\not = \\lambda(v + w)\\) for any \\(\\lambda\\), so \\(v + w\\) is not an eigenvector of \\(A\\). Note: it would be if they had the same eigenvalue. \\(A (2021 v) = 2021 A v = 2021 (-3) v = (-3) (2021 v)\\) so \\(2021 v\\) is an eigenvector also of eigenvalue \\(-3\\). \\(A^2 w = A (A w) = A (5 w) = 5 (A w) = 5 (5 w) = 25 w\\), so \\(w\\) is an eigenvector of \\(A^2\\) of eigenvalue 25. \\((A - 2021I_n)v = A v - 2021 I_n v = -3 v - 2021 v = -2024 v\\), so \\(v\\) is an eigenvector of \\((A - 2021I_n)\\) of eigenvalue \\(-2024\\). 24.4.15 Will discuss in class on Friday 11/18. 24.4.16 Will discuss in class on Friday 11/18. 24.4.17 Put the vectors in the rows of a matrix and row reduce. Then \\(W\\) is the row space of this matrix. \\[ \\begin{bmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; 1 \\\\ -1 &amp; 3 &amp; -1 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; 3 &amp; 1 \\\\ 0 &amp; 2 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix}\\rightarrow \\begin{bmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 3 &amp; 1 \\\\ 0 &amp; 2 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix}\\rightarrow \\begin{bmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 3 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; -2 &amp; 4 \\end{bmatrix} \\] \\[ \\rightarrow \\begin{bmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 3 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 \\end{bmatrix}\\rightarrow \\begin{bmatrix} 1 &amp; -2 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 7 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 \\end{bmatrix}\\rightarrow \\begin{bmatrix} 1 &amp; -2 &amp; 0 &amp; 0 &amp; -6 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 7 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 \\end{bmatrix} \\] \\[ \\rightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; -2 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 7 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 \\end{bmatrix} \\] Let’s check our work in R: ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 -2 ## [2,] 0 1 0 0 2 ## [3,] 0 0 1 0 7 ## [4,] 0 0 0 1 -2 Thus, the dimension of \\(W\\) is 4 and the dimension of \\(W^\\perp\\) is 1 and it is spanned by the vector below \\[ W^\\perp = span\\left\\{ \\begin{bmatrix} 2 \\\\ -2 \\\\ -7 \\\\ 2 \\\\ 1 \\end{bmatrix}\\right\\}. \\] 24.4.18 Will discuss in class on Friday 11/18. 24.4.19 Will discuss in class on Friday 11/18. 24.4.20 The vector \\(v\\) is a basis for the line \\(L\\) that is spanned by \\(v\\). Make \\(v\\) be the row of a matrix \\(A\\) (a 1x3 matrix) and find its null space. 24.4.21 The product \\(A^T A\\) being a diagonal matrix tells us that the vectors in the columns of \\(A\\) are orthogonal (but not orthonormal). This means that you can use the nice dot product formula to find the coefficients of \\(v\\) with respects to the basis in the columns of \\(A\\). "],["final-exam-review.html", "Section 25 Final Exam Review 25.1 Overview 25.2 Vocabulary, Concepts and Skills 25.3 Practice Problems 25.4 Solutions to Practice Problems", " Section 25 Final Exam Review 25.1 Overview The Final Exam has two goals It will cover the new material from Sections 6.1-6.5 (Orthogonality) and 7.1 (Symmetric Matrices). It will have some comprehensive material. The comprehensive material will not be tricky. My goal is to be sure that you understand the fundamentals of the course. Here are some important guidelines about the final: Date and Time: Thursday, December 16, 4:00-6:00 PM. Location: OLRI 100 and OLRI 150 Length: 2 hours, but I do not plan to make the exam twice as long as a midterm. Maybe 1.5 as long but give you twice as much time. Cheatsheet: you can bring a full sheet of paper (8.5\" x 11\") with notes on both sides. As in the past, I will focus on the ideas more than the computations. But there will be some computations. To prepare: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Think about how the ideas fit together: typically, the hardest part is to figure what needs to be done. Look at all of the problems we discussed in class. Practice doing many of them. Look over the Edfinity homework assignments 25.2 Vocabulary, Concepts and Skills 25.2.1 Orthogonality I should be able to do the following tasks: Find the length of a vector Find the distance between two vectors Normalize a vector Find the cosine of the angle between two vectors Find the orthogonal projection of one vector onto another Find the orthogonal projection of one vector onto a subspace (using an orthogonal basis) Find the orthogonal complement of a subspace Find the least squares approximation for an inconsistent system Formulate a least-squares fitting problem as an inconsistent linear system \\(A \\mathsf{x} = \\mathsf{b}\\) Orthogonally diagonalize a symmetric matrix as \\(A=PDP^{\\top}\\). You will not have to compute an SVD. 25.2.2 Vocabulary I should know and be able to use and explain the following terms or properties. dot product of two vectors \\(\\mathsf{v} \\cdot \\mathsf{w} = \\mathsf{v}^{\\top} \\mathsf{w}\\) (aka scalar product, inner product) length (magnitude) of a vector angle between vectors normalize unit vector orthogonal vectors orthogonal complement of a subspace orthogonal projection orthogonal basis orthonormal basis normal equations for a least squares approximation least squares solution residual vector symmetric matrix orthogonally diagonalizable outer product of two vectors \\(\\mathsf{v} \\, \\mathsf{w}^{\\top}\\) spectral decomposition of a symmetric matrix 25.2.3 Conceptual Thinking I should understand and be able to explain the following concepts: The dot product gives an algebraic encoding of the geometry (lengths and angles) of \\(\\mathbb{R}^n\\) If two vectors are orthogonal, then they are perpendicular, or one of them is the zero vector An orthogonal projection is a linear transformation The row space of a matrix is orthogonal to its nullspace The inverse of orthogonal matrix \\(A\\) is the transpose \\(A^{\\top}\\) Cosine similarity is a useful way to compare vectors, especially in high-dimensional vector spaces. The residual vector measures the quality of fit of a least squares solution The outer product \\(\\mathsf{v}\\, \\mathsf{w}^{\\top}\\) is a square matrix with rank 1 The least-squares project onto a subspace: normal equations, predicted, residual. Decomposition of a vector \\(v = w + z\\) with \\(w \\in W\\) and \\(z in W^\\perp\\). 25.2.4 Comprehensive Review Here are some terms and ideas that you should know: Pivot position. Elementary row operations. Ways to compute and think about \\(A v\\) in both words and symbols. Linear combination. \\(Span(v_1, . . . , v_k).\\) In particular, you should be able to visualize \\(Span(v)\\), \\(Span(u,v)\\) and give geometric interpretations of these sets in \\(\\mathbb{R}^2\\) or \\(\\mathbb{R}^3\\). Linearly independent and linearly dependent. Linear transformation. Domain, codomain, image, range, onto, and one-to-one. The transpose of a matrix, the inverse of a matrix, invertible matrix. Subspace. Null space, column space, and row space of a matrix. Kernel and range of a linear transformation. Basis and dimension. Rank. Eigenvalue, eigenvector, eigenspace. Characteristic polynomial and characteristic equation. Diagonalizable matrix. Dot product, length of a vector, angle between vectors, cosine similarity Orthogonal vectors, orthogonal spaces. Orthogonal complement of a subspace. Orthogonal basis, orthonormal basis, orthogonal matrix. Orthogonal projection, least squares solutions. 25.2.5 Skills Form an augmented matrix and reduce a matrix or augmented matrix into row echelon or reduced row echelon form. Determine whether a given matrix is in either of those forms. Determine whether a particular form of a matrix is a possible row echelon or reduced echelon form. Determine whether a system is consistent and if it has a unique solution. Write the general solution in parametric vector form. Describe the set of solutions geometrically. Interpret a system of equations as (i) a vector equation (ii) a matrix equation. Determine when a vector is in a subset spanned by specified vectors. Exhibit a vector as a linear combination of specified vectors. Determine whether a specified vector is in the range of a linear transformation. Determine whether the columns of an \\(m \\times n\\) matrix span \\(\\mathbb{R}^m\\). Determine whether the columns are linearly independent. Compute a matrix-vector product, and interpret it as a linear combination of the columns of \\(A\\). Use linearity of matrix multiplication to compute \\(A(u + v)\\) or \\(A(c u)\\). Find the matrix of a linear transformation. Determine whether a transformation is linear. Determine whether a linear transformation \\(T( x)=A x\\) is one-to-one or onto, using the properties of the matrix \\(A\\). Determine whether a subset of vectors is a subspace. Determine whether a set of vectors is linearly independent and whether it is a basis for some subspace or vector space. Find a basis for a vector space, or for the null space or column space of a matrix, or for the kernel or range of a linear transformation. Find the dimension of a vector space or subspace. Find the dimension of the null space (number of free variables) and column space (number of pivot columns) of a matrix. Find and interpret the rank of a matrix. Calculate the characteristic equation, eigenvalues, and eigenvectors of a square matrix. Find eigenvectors for a specific eigenvalue. Check if a vector is an eigenvector of a given matrix. Determine whether a square matrix is diagonalizable. Factor a diagonalizable matrix into \\(A=PDP^{-1}\\), where \\(D\\) is a diagonal matrix. Use eigenvalues and eigenvectors to analyze the long-term behavior of discrete dynamical systems. Find the length of a vector, the distance between two vectors, or the angle between two vectors. Determine whether a set of vectors are orthogonal. Determine whether a vector is orthogonal to a subspace or whether two subspaces are orthogonal, by checking whether their basis vectors are orthogonal. Find the orthogonal projection of (i) one vector onto another vector, or (ii) one vector onto a subspace (using an orthogonal basis for the subspace). Find the distance between a vector and a space (by computing the residual). Set up the matrix equation to find the ``best-fitting’’ function to a set of data using least squares. Interpret the normal equations \\(A^{\\top}A x = A^{\\top}b\\). Find the least squares approximation by solving the normal equations \\(\\hat{x}=(A^{\\top}A)^{-1}A^{\\top}b\\). 25.3 Practice Problems 25.3.1 Let \\(\\mathsf{v} = \\begin{bmatrix}1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathsf{w}= \\begin{bmatrix}5 \\\\ 2 \\\\ 3 \\end{bmatrix}\\). Find \\(\\| \\mathsf{v} \\|\\) and \\(\\| \\mathsf{w} \\|\\). Find the distance between \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\). Find the cosine of the angle between \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\). Find \\(\\mbox{proj}_{\\mathsf{v}} \\mathsf{w}\\). Let \\(W=\\mbox{span} (\\mathsf{v}, \\mathsf{w})\\). Use the residual from the previous projection to create an orthonormal basis \\(\\mathsf{u}_1, \\mathsf{u}_2\\) for \\(W\\) such that \\(\\mathsf{u}_1\\) is a vector in the same direction as \\(\\mathsf{v}\\). 25.3.2 For a subspace \\(W \\subseteq \\mathbb{R}^n\\), define the function \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) by \\(T(\\mathsf{x}) = \\mbox{proj}_{\\mathsf{u}} \\mathsf{x}\\). Recall that the null space of \\(T\\) is the subspace \\(\\mbox{ker}(T) = \\{ \\mathsf{x} \\in \\mathbb{R}^n \\mid T(x) = \\mathbf{0} \\}\\). Describe \\(\\mbox{ker}(T)\\) as explicitly as you can. 25.3.3 The vectors \\(\\mathsf{u}_1, \\mathsf{u}_2\\) form an orthonormal basis of a subspace \\(W\\) of \\(\\mathbb{R}^4\\). Find the projection of \\(\\mathsf{v}\\) onto \\(W\\) and determine how close \\(\\mathsf{v}\\) is to \\(W\\). \\[ \\mathsf{u}_1 = \\frac{1}{2}\\begin{bmatrix} 1\\\\ -1\\\\ -1\\\\ 1 \\end{bmatrix}, \\quad \\mathsf{u}_2 = \\frac{1}{2}\\begin{bmatrix} 1\\\\ -1\\\\ 1\\\\ -1 \\end{bmatrix}, \\quad \\mathsf{v} = \\begin{bmatrix} 2\\\\ 2\\\\ 4\\\\ 2 \\end{bmatrix} \\] 25.3.4 Consider vectors \\(\\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\-1 \\end{bmatrix}\\) and \\(\\mathsf{v}_2= \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\) in \\(\\mathbb{R}^3\\). Let \\(W=\\mbox{span}(\\mathsf{v}_1, \\mathsf{v}_2)\\). Show that \\(\\mathsf{v}_1\\) and \\(\\mathsf{v}_2\\) are orthogonal. Find a basis for \\(W^{\\perp}\\). Use orthogonal projections to find the representation of \\(\\mathsf{y} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 2 \\end{bmatrix}\\) as \\(\\mathsf{y} = \\hat{\\mathsf{y}} + \\mathsf{z}\\) where \\(\\hat{\\mathsf{y}} \\in W\\) and \\(\\mathsf{z} \\in W^{\\perp}\\). 25.3.5 Let \\(W\\) be the span of the vectors \\[ \\begin{bmatrix} 1 \\\\ -2 \\\\ 1 \\\\ 0 \\\\1 \\end{bmatrix}, \\quad \\begin{bmatrix} -1 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 3 \\\\1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\\\4 \\end{bmatrix} \\] Find a basis for \\(W\\). What is the dimension of this subspace? Find a basis for \\(W^{\\perp}\\) 25.3.6 Consider the system \\(A \\mathsf{x} = \\mathsf{b}\\) given by \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; -1 \\\\ 1 &amp; 1 &amp; -1 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 4\\\\ 1 \\\\ -2 \\\\ -1 \\end{bmatrix}. \\] Show that this system is inconsistent. Find the projected value \\(\\hat{\\mathsf{b}}\\), and the residual \\(\\mathsf{z}\\). How close is your approximate solution to the desired target vector? 25.3.7 Here is an inconsistent system of equations: \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 2 \\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 6\\\\ 4 \\\\ -4 \\end{bmatrix} \\] State the normal equations for this problem (be sure to do all of the necessary matrix multiplications). Find the least squares solution to the problem. How close is your approximate solution to the desired target vector? 25.3.8 One-to-one and Onto Here is a matrix \\(A\\) and its reduced row echelon form \\(B\\) \\[ A = \\begin{bmatrix} 1 &amp; 3 &amp; -3 &amp; 1 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 &amp; 6 &amp; 5 \\\\ 3 &amp; 3 &amp; -3 &amp; 6 &amp; 3 \\\\ -1 &amp; 4 &amp; -3 &amp; -3 &amp; -1 \\end{bmatrix} \\qquad \\longrightarrow \\qquad B = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 2.5 &amp; 1.5 \\\\ 0 &amp; 1 &amp; 0 &amp; 1.0 &amp; 2.0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1.5 &amp; 2.5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0.0 &amp; 0.0 \\end{bmatrix}. \\] Find a basis for \\(\\mbox{Nul}(A)\\) and \\(\\mbox{Col}(A)\\). Is the linear transformation \\(T(\\mathsf{x}) = A \\mathsf{x}\\) one-to-one? Onto? 25.3.9 A 4x5 Matrix \\(\\mathsf{A}\\) is a \\(4 \\times 5\\) matrix and \\(b \\in \\mathbb{R}^4\\). The augmented matrix \\([\\,\\mathsf{A}\\mid b\\,]\\) row reduces as shown here. \\[ [\\,\\mathsf{A}\\mid b\\,] = \\left[ \\begin{array}{ccccc|c} \\vert &amp; \\vert &amp; \\vert &amp; \\vert &amp; \\vert &amp;\\vert\\\\ v_1 &amp; v_2 &amp; v_3 &amp; v_4 &amp; v_5 &amp; b\\\\ \\vert &amp; \\vert &amp; \\vert &amp; \\vert &amp; \\vert &amp;\\vert\\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{rrrrr|r} 1 &amp; 0 &amp; 2 &amp; 0 &amp; -1 &amp;1\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -1 &amp;1\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp;-2\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right] \\] a. Give all of the solutions to o \\(\\mathsf{A} x = b\\) in parametric form. Give a dependence relation among the columns of \\(\\mathsf{A}\\). These true-false questions refer to the coefficient matrix \\(\\mathsf{A}\\) above. Decide if the statement is T = True or F = False. No justification necessary. \\(\\mathsf{A} x = b\\) has a solution for all \\(b \\in \\mathbb{R}^4\\). The columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^4\\). If \\(\\mathsf{A} x = b\\) has a solution, then it has infinitely many solutions. The linear transformation \\(x \\mapsto \\mathsf{A} x\\) is one-to-one The linear transformation \\(x \\mapsto \\mathsf{A} x\\) is onto 25.3.10 A x = b Suppose that \\({\\mathsf{A}}\\) is an \\(n \\times n\\) matrix and that \\(\\vec{\\mathsf{x}}_1\\) and \\(\\vec{\\mathsf{x}}_2\\) are two solutions to \\({\\mathsf{A}}x = \\mathsf{b}\\) with \\(\\mathsf{b}\\not= \\vec{\\mathbf{0}}\\) and \\(\\vec{\\mathsf{x}}_1 \\not= \\vec{\\mathsf{x}}_2\\). Give a nonzero solution to \\({\\mathsf{A}}\\vec{\\mathsf{x}}= \\vec{\\mathbf{0}}\\). Give a solution to \\({\\mathsf{A}}\\vec{\\mathsf{x}}= \\mathsf{b}\\) that no one else in the class has. Decide if the statement is T = True or F = False, or I = there is not enough information to know. The equation \\({\\mathsf{A}}x = \\vec{\\mathsf{y}}\\) has a solution for all \\(\\vec{\\mathsf{y}}\\in \\mathbb{R}^n\\) \\(\\lambda = 0\\) is an eigenvalue of \\({\\mathsf{A}}\\) \\({\\mathsf{A}}\\) is invertible \\({\\mathsf{A}}\\) is diagonalizable 25.3.11 Watch this! The answer to at least one question on the final exam is contained in this video. 25.4 Solutions to Practice Problems 25.4.1 \\[\\begin{align} \\| \\mathsf{v} \\| &amp;= \\sqrt{ \\mathsf{v} \\cdot \\mathsf{v}} = \\sqrt{1+1+1} = \\sqrt{3} \\\\ \\| \\mathsf{w} \\| &amp;= \\sqrt{ \\mathsf{w} \\cdot \\mathsf{vw}} = \\sqrt{25+4+9} = \\sqrt{38} \\\\ \\end{align}\\] We have \\(\\mathsf{v} - \\mathsf{w} = \\begin{bmatrix} -4 \\\\ -3 \\\\ -2 \\end{bmatrix}\\) and so \\[ \\| \\mathsf{v} - \\mathsf{w}\\| = \\sqrt{16+9+4} = \\sqrt{29} \\] \\[ \\cos \\theta = \\frac{\\mathsf{v} \\cdot \\mathsf{w}}{\\| \\mathsf{v} \\| \\, \\|\\mathsf{w} \\| } = \\frac{5-2+3}{\\sqrt{3} \\, \\sqrt{38} } = \\frac{2\\sqrt{3}}{\\sqrt{38} } \\] \\[ \\hat{\\mathsf{w}} = \\mbox{proj}_{\\mathsf{v}} \\mathsf{w} = \\frac{\\mathsf{v} \\cdot \\mathsf{w}}{ \\mathsf{v} \\cdot \\mathsf{v} } \\, \\mathsf{v} = \\frac{5-2+3}{1+1+1} \\mathsf{v} = 2 \\mathsf{v} = \\begin{bmatrix} 2 \\\\ -2 \\\\ 2 \\end{bmatrix} \\] Using \\(\\hat{\\mathsf{w}}\\) from the previous problem, we know that \\[ \\mathsf{z} = \\mathsf{w} - \\hat{\\mathsf{w}} = \\begin{bmatrix} 5 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\] is orthogonal to \\(\\mathsf{v}\\).So an orthonormal basis is \\[ \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} \\quad \\mbox{and} \\quad \\frac{1}{\\sqrt{26}} \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\] 25.4.2 Here are a few ways to describe \\(\\mbox{ker}(T)\\). \\(\\mbox{ker}(T) = \\{ \\mathsf{x} \\in \\mathbb{R}^n \\mid \\mathsf{x} \\cdot \\mathsf{u} = 0 \\}\\). \\(\\mbox{ker}(T)\\) is the set of vectors that are orthogonal to \\(\\mathsf{u}\\). Let \\(A\\) be the \\(1 \\times n\\) matrix \\(\\mathsf{u}^{\\top}\\). Then \\(\\mbox{ker}(T)= \\mbox{Nul}(A)\\). 25.4.3 We have \\(\\mathsf{u}_1 \\cdot \\mathsf{v} = 2-2-4+2=-2\\) and \\(\\mathsf{u}_1 \\cdot \\mathsf{v} = 2-2+4-2=2\\) so \\[ \\hat{\\mathsf{v}} = \\mbox{proj}_W \\mathsf{v} = - \\mathsf{u}_1 + \\mathsf{u}_2 = -\\frac{1}{2} \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{bmatrix} \\] with residual vector \\[ \\mathsf{z} = \\mathsf{v} - \\hat{\\mathsf{v}} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 4 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 3 \\\\ 3 \\end{bmatrix} \\] and the distance is \\(\\| \\mathsf{z} \\| = \\sqrt{4+4+9+9} = \\sqrt{26}\\). 25.4.4 \\(\\mathsf{v}_1 \\cdot \\mathsf{v}_2 = 1 +2 - 3 =0\\). We must find \\(\\mbox{Nul}(A)\\) where \\(A = \\begin{bmatrix} \\mathsf{v}_1^{\\top} \\\\ \\mathsf{v}_2^{\\top}\\end{bmatrix}\\). \\[ \\begin{bmatrix} 1 &amp; 1 &amp; -1 \\\\ 1 &amp; 2 &amp; 3 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 1 &amp; -1 \\\\ 0 &amp; 1 &amp; 4 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; -5 \\\\ 0 &amp; 1 &amp; 4 \\end{bmatrix} \\] so the vector \\(\\begin{bmatrix} 5 \\\\ -4 \\\\ 1 \\end{bmatrix}\\) is a basis for \\(W^{\\perp}\\) We have \\[\\begin{align} \\hat{\\mathsf{y}} &amp;= \\frac{\\mathsf{y} \\cdot \\mathsf{v_1}}{\\mathsf{v_1} \\cdot \\mathsf{v_1}} \\, \\mathsf{v_1} + \\frac{\\mathsf{y} \\cdot \\mathsf{v_2}}{\\mathsf{v_2} \\cdot \\mathsf{v_2}} \\, \\mathsf{v_2} = \\frac{8-2}{1+1+1} \\mathsf{v_1} + \\frac{8+6}{1+4+9} \\mathsf{v_2} \\\\ &amp;= 2\\mathsf{v_1} +\\mathsf{v_2} = \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\end{align}\\] and so \\[ \\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ -4 \\\\ 1 \\end{bmatrix}. \\] 25.4.5 a .We will answer this one using RStudio. A = cbind(c(1,-2,1,0,1), c(-1,3,-1,1,-1), c(0,0,1,3,1), c(0,2,0,0,4)) rref(A) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## [5,] 0 0 0 0 So we need all four vectors to span the column space. We obtain a basis for \\(W^{\\perp}\\) by finding \\(\\mbox{Nul(A^{\\top})}\\) So let’s row reduce \\(A^{\\top}\\) rref(t(A)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 -2 ## [2,] 0 1 0 0 2 ## [3,] 0 0 1 0 7 ## [4,] 0 0 0 1 -2 The vector \\(\\begin{bmatrix} 2 \\\\ -2 \\\\ -7 \\\\ 2 \\\\ 1\\end{bmatrix}\\) spans \\(W^{\\perp}\\) 25.4.6 A = cbind(c(1,1,1,1), c(1,2,1,2),c(1,-1,-1,1)) b = c(4,1,-2,-1) rref(cbind(A,b)) ## b ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 There is a pivot in the last column of this augmented matrix, so this system is inconsistent. Here is the least squares calculation. #solve the normal equation (xhat = solve(t(A) %*% A, t(A) %*% b)) ## [,1] ## [1,] 2 ## [2,] -1 ## [3,] 1 # find the projection (bhat = A %*% xhat) ## [,1] ## [1,] 2.000000e+00 ## [2,] -1.000000e+00 ## [3,] 6.661338e-16 ## [4,] 1.000000e+00 # find the residual vector (z = b - bhat) ## [,1] ## [1,] 2 ## [2,] 2 ## [3,] -2 ## [4,] -2 # check that z is orthogonal to Col(A) t(A) %*% z ## [,1] ## [1,] -8.881784e-16 ## [2,] 0.000000e+00 ## [3,] 0.000000e+00 # measure the distance between bhat and b sqrt( t(z) %*% z) ## [,1] ## [1,] 4 The projection is \\(\\hat{\\mathsf{b}} = [2,-1,0,1]^{\\top}\\). The residual is \\(\\mathsf{z} = [2,2,-2,-2]^{\\top}\\) The distance of between \\(\\mathsf{b}\\) and \\(\\hat{\\mathsf{b}}\\) is \\[ \\| = \\| \\mathsf{z} \\| = \\sqrt{4+4+4+4} = \\sqrt{16} = 4. \\] 25.4.7 Can be done by hand or by R: ## [,1] [,2] ## [1,] 3 3 ## [2,] 3 9 ## [,1] ## [1,] 6 ## [2,] 24 The normal equations are \\[ \\begin{bmatrix} 3 &amp; 3 \\\\ 3 &amp; 9 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 24 \\end{bmatrix} \\] The least squares solution is ## [,1] ## [1,] -1 ## [2,] 3 How close? ## [,1] ## [1,] 2 ## [,1] ## [1,] 1.414214 25.4.8 4x5 matrix: solution coming 25.4.9 Ax = b: solution coming 25.4.10 I hope you enjoyed the video. "]]
